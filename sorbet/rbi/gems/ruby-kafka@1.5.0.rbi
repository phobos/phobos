# typed: true

# DO NOT EDIT MANUALLY
# This is an autogenerated file for types exported from the `ruby-kafka` gem.
# Please instead update this file by running `bin/tapioca gem ruby-kafka`.

# Represents a broker in a Kafka cluster.
#
# source://ruby-kafka//lib/kafka/version.rb#3
module Kafka
  class << self
    # Initializes a new Kafka client.
    #
    # @return [Client]
    # @see Client#initialize
    #
    # source://ruby-kafka//lib/kafka.rb#362
    def new(seed_brokers = T.unsafe(nil), **options); end
  end
end

# A Kafka producer that does all its work in the background so as to not block
# the calling thread. Calls to {#deliver_messages} are asynchronous and return
# immediately.
#
# In addition to this property it's possible to define automatic delivery
# policies. These allow placing an upper bound on the number of buffered
# messages and the time between message deliveries.
#
# * If `delivery_threshold` is set to a value _n_ higher than zero, the producer
#   will automatically deliver its messages once its buffer size reaches _n_.
# * If `delivery_interval` is set to a value _n_ higher than zero, the producer
#   will automatically deliver its messages every _n_ seconds.
#
# By default, automatic delivery is disabled and you'll have to call
# {#deliver_messages} manually.
#
# ## Buffer Overflow and Backpressure
#
# The calling thread communicates with the background thread doing the actual
# work using a thread safe queue. While the background thread is busy delivering
# messages, new messages will be buffered in the queue. In order to avoid
# the queue growing uncontrollably in cases where the background thread gets
# stuck or can't follow the pace of the calling thread, there's a maximum
# number of messages that is allowed to be buffered. You can configure this
# value by setting `max_queue_size`.
#
# If you produce messages faster than the background producer thread can
# deliver them to Kafka you will eventually fill the producer's buffer. Once
# this happens, the background thread will stop popping messages off the
# queue until it can successfully deliver the buffered messages. The queue
# will therefore grow in size, potentially hitting the `max_queue_size` limit.
# Once this happens, calls to {#produce} will raise a {BufferOverflow} error.
#
# Depending on your use case you may want to slow down the rate of messages
# being produced or perhaps halt your application completely until the
# producer can deliver the buffered messages and clear the message queue.
#
# ## Example
#
#     producer = kafka.async_producer(
#       # Keep at most 1.000 messages in the buffer before delivering:
#       delivery_threshold: 1000,
#
#       # Deliver messages every 30 seconds:
#       delivery_interval: 30,
#     )
#
#     # There's no need to manually call #deliver_messages, it will happen
#     # automatically in the background.
#     producer.produce("hello", topic: "greetings")
#
#     # Remember to shut down the producer when you're done with it.
#     producer.shutdown
#
# source://ruby-kafka//lib/kafka/async_producer.rb#61
class Kafka::AsyncProducer
  # Initializes a new AsyncProducer.
  #
  # @param sync_producer [Kafka::Producer] the synchronous producer that should
  #   be used in the background.
  # @param max_queue_size [Integer] the maximum number of messages allowed in
  #   the queue.
  # @param delivery_threshold [Integer] if greater than zero, the number of
  #   buffered messages that will automatically trigger a delivery.
  # @param delivery_interval [Integer] if greater than zero, the number of
  #   seconds between automatic message deliveries.
  # @raise [ArgumentError]
  # @return [AsyncProducer] a new instance of AsyncProducer
  #
  # source://ruby-kafka//lib/kafka/async_producer.rb#73
  def initialize(sync_producer:, instrumenter:, logger:, max_queue_size: T.unsafe(nil), delivery_threshold: T.unsafe(nil), delivery_interval: T.unsafe(nil), max_retries: T.unsafe(nil), retry_backoff: T.unsafe(nil)); end

  # Asynchronously delivers the buffered messages. This method will return
  # immediately and the actual work will be done in the background.
  #
  # @return [nil]
  # @see Kafka::Producer#deliver_messages
  #
  # source://ruby-kafka//lib/kafka/async_producer.rb#133
  def deliver_messages; end

  # Produces a message to the specified topic.
  #
  # @param value [String] the message data.
  # @param key [String] the message key.
  # @param headers [Hash<String, String>] the headers for the message.
  # @param topic [String] the topic that the message should be written to.
  # @param partition [Integer] the partition that the message should be written to.
  # @param partition_key [String] the key that should be used to assign a partition.
  # @param create_time [Time] the timestamp that should be set on the message.
  # @raise [BufferOverflow] if the message queue is full.
  # @return [nil]
  # @see Kafka::Producer#produce
  #
  # source://ruby-kafka//lib/kafka/async_producer.rb#105
  def produce(value, topic:, **options); end

  # Shuts down the producer, releasing the network resources used. This
  # method will block until the buffered messages have been delivered.
  #
  # @return [nil]
  # @see Kafka::Producer#shutdown
  #
  # source://ruby-kafka//lib/kafka/async_producer.rb#146
  def shutdown; end

  private

  # @raise [BufferOverflow]
  #
  # source://ruby-kafka//lib/kafka/async_producer.rb#175
  def buffer_overflow(topic, message); end

  # source://ruby-kafka//lib/kafka/async_producer.rb#158
  def ensure_threads_running!; end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/async_producer.rb#171
  def timer_thread_alive?; end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/async_producer.rb#167
  def worker_thread_alive?; end
end

# source://ruby-kafka//lib/kafka/async_producer.rb#183
class Kafka::AsyncProducer::Timer
  # @return [Timer] a new instance of Timer
  #
  # source://ruby-kafka//lib/kafka/async_producer.rb#184
  def initialize(interval:, queue:); end

  # source://ruby-kafka//lib/kafka/async_producer.rb#189
  def run; end
end

# source://ruby-kafka//lib/kafka/async_producer.rb#200
class Kafka::AsyncProducer::Worker
  # @return [Worker] a new instance of Worker
  #
  # source://ruby-kafka//lib/kafka/async_producer.rb#201
  def initialize(queue:, producer:, delivery_threshold:, instrumenter:, logger:, max_retries: T.unsafe(nil), retry_backoff: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/async_producer.rb#211
  def run; end

  private

  # source://ruby-kafka//lib/kafka/async_producer.rb#283
  def deliver_messages; end

  # source://ruby-kafka//lib/kafka/async_producer.rb#226
  def do_loop; end

  # source://ruby-kafka//lib/kafka/async_producer.rb#264
  def produce(value, **kwargs); end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/async_producer.rb#291
  def threshold_reached?; end
end

# source://ruby-kafka//lib/kafka/broker.rb#8
class Kafka::Broker
  # @return [Broker] a new instance of Broker
  #
  # source://ruby-kafka//lib/kafka/broker.rb#9
  def initialize(connection_builder:, host:, port:, logger:, node_id: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/broker.rb#185
  def add_offsets_to_txn(**options); end

  # source://ruby-kafka//lib/kafka/broker.rb#173
  def add_partitions_to_txn(**options); end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/broker.rb#18
  def address_match?(host, port); end

  # source://ruby-kafka//lib/kafka/broker.rb#137
  def alter_configs(**options); end

  # source://ruby-kafka//lib/kafka/broker.rb#155
  def api_versions; end

  # source://ruby-kafka//lib/kafka/broker.rb#83
  def commit_offsets(**options); end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/broker.rb#33
  def connected?; end

  # source://ruby-kafka//lib/kafka/broker.rb#143
  def create_partitions(**options); end

  # source://ruby-kafka//lib/kafka/broker.rb#119
  def create_topics(**options); end

  # source://ruby-kafka//lib/kafka/broker.rb#125
  def delete_topics(**options); end

  # source://ruby-kafka//lib/kafka/broker.rb#131
  def describe_configs(**options); end

  # source://ruby-kafka//lib/kafka/broker.rb#161
  def describe_groups(**options); end

  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/broker.rb#28
  def disconnect; end

  # source://ruby-kafka//lib/kafka/broker.rb#179
  def end_txn(**options); end

  # Fetches messages from a specified topic and partition.
  #
  # @param max_wait_time [Integer]
  # @param min_bytes [Integer]
  # @param topics [Hash]
  # @return [Kafka::Protocol::FetchResponse]
  #
  # source://ruby-kafka//lib/kafka/broker.rb#51
  def fetch_messages(**options); end

  # Fetches cluster metadata from the broker.
  #
  # @param topics [Array<String>]
  # @return [Kafka::Protocol::MetadataResponse]
  #
  # source://ruby-kafka//lib/kafka/broker.rb#41
  def fetch_metadata(**options); end

  # source://ruby-kafka//lib/kafka/broker.rb#77
  def fetch_offsets(**options); end

  # source://ruby-kafka//lib/kafka/broker.rb#107
  def find_coordinator(**options); end

  # source://ruby-kafka//lib/kafka/broker.rb#113
  def heartbeat(**options); end

  # source://ruby-kafka//lib/kafka/broker.rb#167
  def init_producer_id(**options); end

  # source://ruby-kafka//lib/kafka/broker.rb#89
  def join_group(**options); end

  # source://ruby-kafka//lib/kafka/broker.rb#101
  def leave_group(**options); end

  # source://ruby-kafka//lib/kafka/broker.rb#149
  def list_groups; end

  # Lists the offset of the specified topics and partitions.
  #
  # @param topics [Hash]
  # @return [Kafka::Protocol::ListOffsetResponse]
  #
  # source://ruby-kafka//lib/kafka/broker.rb#61
  def list_offsets(**options); end

  # Produces a set of messages to the broker.
  #
  # @param required_acks [Integer]
  # @param timeout [Integer]
  # @param messages_for_topics [Hash]
  # @return [Kafka::Protocol::ProduceResponse]
  #
  # source://ruby-kafka//lib/kafka/broker.rb#71
  def produce(**options); end

  # source://ruby-kafka//lib/kafka/broker.rb#95
  def sync_group(**options); end

  # @return [String]
  #
  # source://ruby-kafka//lib/kafka/broker.rb#23
  def to_s; end

  # source://ruby-kafka//lib/kafka/broker.rb#191
  def txn_offset_commit(**options); end

  private

  # source://ruby-kafka//lib/kafka/broker.rb#213
  def connection; end

  # source://ruby-kafka//lib/kafka/broker.rb#199
  def send_request(request); end
end

# source://ruby-kafka//lib/kafka/broker_info.rb#3
class Kafka::BrokerInfo
  # @return [BrokerInfo] a new instance of BrokerInfo
  #
  # source://ruby-kafka//lib/kafka/broker_info.rb#6
  def initialize(node_id:, host:, port:); end

  # Returns the value of attribute host.
  #
  # source://ruby-kafka//lib/kafka/broker_info.rb#4
  def host; end

  # Returns the value of attribute node_id.
  #
  # source://ruby-kafka//lib/kafka/broker_info.rb#4
  def node_id; end

  # Returns the value of attribute port.
  #
  # source://ruby-kafka//lib/kafka/broker_info.rb#4
  def port; end

  # source://ruby-kafka//lib/kafka/broker_info.rb#12
  def to_s; end
end

# 8
# The broker is not available.
#
# source://ruby-kafka//lib/kafka.rb#76
class Kafka::BrokerNotAvailable < ::Kafka::ProtocolError; end

# source://ruby-kafka//lib/kafka/broker_pool.rb#6
class Kafka::BrokerPool
  # @return [BrokerPool] a new instance of BrokerPool
  #
  # source://ruby-kafka//lib/kafka/broker_pool.rb#7
  def initialize(connection_builder:, logger:); end

  # source://ruby-kafka//lib/kafka/broker_pool.rb#34
  def close; end

  # source://ruby-kafka//lib/kafka/broker_pool.rb#13
  def connect(host, port, node_id: T.unsafe(nil)); end
end

# source://ruby-kafka//lib/kafka/broker_uri.rb#6
module Kafka::BrokerUri
  class << self
    # Parses a Kafka broker URI string.
    #
    # Examples of valid strings:
    # * `kafka1.something`
    # * `kafka1.something:1234`
    # * `kafka://kafka1.something:1234`
    # * `kafka+ssl://kafka1.something:1234`
    # * `plaintext://kafka1.something:1234`
    #
    # @param str [String] a Kafka broker URI string.
    # @return [URI]
    #
    # source://ruby-kafka//lib/kafka/broker_uri.rb#21
    def parse(str); end
  end
end

# source://ruby-kafka//lib/kafka/broker_uri.rb#7
Kafka::BrokerUri::DEFAULT_PORT = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/broker_uri.rb#8
Kafka::BrokerUri::URI_SCHEMES = T.let(T.unsafe(nil), Array)

# Raised when a producer buffer has reached its maximum size.
#
# source://ruby-kafka//lib/kafka.rb#325
class Kafka::BufferOverflow < ::Kafka::Error; end

# source://ruby-kafka//lib/kafka/client.rb#21
class Kafka::Client
  # Initializes a new Kafka client.
  #
  # @param seed_brokers [Array<String>, String] the list of brokers used to initialize
  #   the client. Either an Array of connections, or a comma separated string of connections.
  #   A connection can either be a string of "host:port" or a full URI with a scheme.
  #   If there's a scheme it's ignored and only host/port are used.
  # @param client_id [String] the identifier for this application.
  # @param logger [Logger] the logger that should be used by the client.
  # @param connect_timeout [Integer, nil] the timeout setting for connecting
  #   to brokers. See {BrokerPool#initialize}.
  # @param socket_timeout [Integer, nil] the timeout setting for socket
  #   connections. See {BrokerPool#initialize}.
  # @param ssl_ca_cert [String, Array<String>, nil] a PEM encoded CA cert, or an Array of
  #   PEM encoded CA certs, to use with an SSL connection.
  # @param ssl_ca_cert_file_path [String, Array<String>, nil] a path on the filesystem, or an
  #   Array of paths, to PEM encoded CA cert(s) to use with an SSL connection.
  # @param ssl_client_cert [String, nil] a PEM encoded client cert to use with an
  #   SSL connection. Must be used in combination with ssl_client_cert_key.
  # @param ssl_client_cert_key [String, nil] a PEM encoded client cert key to use with an
  #   SSL connection. Must be used in combination with ssl_client_cert.
  # @param ssl_client_cert_key_password [String, nil] the password required to read the
  #   ssl_client_cert_key. Must be used in combination with ssl_client_cert_key.
  # @param sasl_gssapi_principal [String, nil] a KRB5 principal
  # @param sasl_gssapi_keytab [String, nil] a KRB5 keytab filepath
  # @param sasl_scram_username [String, nil] SCRAM username
  # @param sasl_scram_password [String, nil] SCRAM password
  # @param sasl_scram_mechanism [String, nil] Scram mechanism, either "sha256" or "sha512"
  # @param sasl_over_ssl [Boolean] whether to enforce SSL with SASL
  # @param ssl_ca_certs_from_system [Boolean] whether to use the CA certs from the
  #   system's default certificate store.
  # @param partitioner [Partitioner, nil] the partitioner that should be used by the client.
  # @param sasl_oauth_token_provider [Object, nil] OAuthBearer Token Provider instance that
  #   implements method token. See {Sasl::OAuth#initialize}
  # @param ssl_verify_hostname [Boolean, true] whether to verify that the host serving
  #   the SSL certificate and the signing chain of the certificate have the correct domains
  #   based on the CA certificate
  # @param resolve_seed_brokers [Boolean] whether to resolve each hostname of the seed brokers.
  #   If a broker is resolved to multiple IP addresses, the client tries to connect to each
  #   of the addresses until it can connect.
  # @return [Client]
  #
  # source://ruby-kafka//lib/kafka/client.rb#83
  def initialize(seed_brokers:, client_id: T.unsafe(nil), logger: T.unsafe(nil), connect_timeout: T.unsafe(nil), socket_timeout: T.unsafe(nil), ssl_ca_cert_file_path: T.unsafe(nil), ssl_ca_cert: T.unsafe(nil), ssl_client_cert: T.unsafe(nil), ssl_client_cert_key: T.unsafe(nil), ssl_client_cert_key_password: T.unsafe(nil), ssl_client_cert_chain: T.unsafe(nil), sasl_gssapi_principal: T.unsafe(nil), sasl_gssapi_keytab: T.unsafe(nil), sasl_plain_authzid: T.unsafe(nil), sasl_plain_username: T.unsafe(nil), sasl_plain_password: T.unsafe(nil), sasl_scram_username: T.unsafe(nil), sasl_scram_password: T.unsafe(nil), sasl_scram_mechanism: T.unsafe(nil), sasl_aws_msk_iam_access_key_id: T.unsafe(nil), sasl_aws_msk_iam_secret_key_id: T.unsafe(nil), sasl_aws_msk_iam_aws_region: T.unsafe(nil), sasl_aws_msk_iam_session_token: T.unsafe(nil), sasl_over_ssl: T.unsafe(nil), ssl_ca_certs_from_system: T.unsafe(nil), partitioner: T.unsafe(nil), sasl_oauth_token_provider: T.unsafe(nil), ssl_verify_hostname: T.unsafe(nil), resolve_seed_brokers: T.unsafe(nil)); end

  # Alter broker configs
  #
  # @param broker_id [int] the id of the broker
  # @param configs [Array] array of config strings.
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/client.rb#598
  def alter_configs(broker_id, configs = T.unsafe(nil)); end

  # Alter the configuration of a topic.
  #
  # Configuration keys must match
  # [Kafka's topic-level configs](https://kafka.apache.org/documentation/#topicconfigs).
  #
  # @example Describing the cleanup policy config of a topic
  #   kafka = Kafka.new(["kafka1:9092"])
  #   kafka.alter_topic("my-topic", "cleanup.policy" => "delete", "max.message.byte" => "100000")
  # @note This is an alpha level API and is subject to change.
  # @param name [String] the name of the topic.
  # @param configs [Hash<String, String>] hash of desired config keys and values.
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/client.rb#675
  def alter_topic(name, configs = T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/client.rb#792
  def apis; end

  # Creates a new AsyncProducer instance.
  #
  # All parameters allowed by {#producer} can be passed. In addition to this,
  # a few extra parameters can be passed when creating an async producer.
  #
  # @param max_queue_size [Integer] the maximum number of messages allowed in
  #   the queue.
  # @param delivery_threshold [Integer] if greater than zero, the number of
  #   buffered messages that will automatically trigger a delivery.
  # @param delivery_interval [Integer] if greater than zero, the number of
  #   seconds between automatic message deliveries.
  # @return [AsyncProducer]
  # @see AsyncProducer
  #
  # source://ruby-kafka//lib/kafka/client.rb#340
  def async_producer(delivery_interval: T.unsafe(nil), delivery_threshold: T.unsafe(nil), max_queue_size: T.unsafe(nil), max_retries: T.unsafe(nil), retry_backoff: T.unsafe(nil), **options); end

  # List all brokers in the cluster.
  #
  # @return [Array<Kafka::BrokerInfo>] the list of brokers.
  #
  # source://ruby-kafka//lib/kafka/client.rb#799
  def brokers; end

  # Closes all connections to the Kafka brokers and frees up used resources.
  #
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/client.rb#813
  def close; end

  # Creates a new Kafka consumer.
  #
  # @param group_id [String] the id of the group that the consumer should join.
  # @param session_timeout [Integer] the number of seconds after which, if a client
  #   hasn't contacted the Kafka cluster, it will be kicked out of the group.
  # @param offset_commit_interval [Integer] the interval between offset commits,
  #   in seconds.
  # @param offset_commit_threshold [Integer] the number of messages that can be
  #   processed before their offsets are committed. If zero, offset commits are
  #   not triggered by message processing.
  # @param heartbeat_interval [Integer] the interval between heartbeats; must be less
  #   than the session window.
  # @param offset_retention_time [Integer] the time period that committed
  #   offsets will be retained, in seconds. Defaults to the broker setting.
  # @param fetcher_max_queue_size [Integer] max number of items in the fetch queue that
  #   are stored for further processing. Note, that each item in the queue represents a
  #   response from a single broker.
  # @param refresh_topic_interval [Integer] interval of refreshing the topic list.
  #   If it is 0, the topic list won't be refreshed (default)
  #   If it is n (n > 0), the topic list will be refreshed every n seconds
  # @param interceptors [Array<Object>] a list of consumer interceptors that implement
  #   `call(Kafka::FetchedBatch)`.
  # @param assignment_strategy [Object] a partition assignment strategy that
  #   implements `protocol_type()`, `user_data()`, and `assign(members:, partitions:)`
  # @return [Consumer]
  #
  # source://ruby-kafka//lib/kafka/client.rb#380
  def consumer(group_id:, session_timeout: T.unsafe(nil), rebalance_timeout: T.unsafe(nil), offset_commit_interval: T.unsafe(nil), offset_commit_threshold: T.unsafe(nil), heartbeat_interval: T.unsafe(nil), offset_retention_time: T.unsafe(nil), fetcher_max_queue_size: T.unsafe(nil), refresh_topic_interval: T.unsafe(nil), interceptors: T.unsafe(nil), assignment_strategy: T.unsafe(nil)); end

  # The current controller broker in the cluster.
  #
  # @return [Kafka::BrokerInfo] information on the controller broker.
  #
  # source://ruby-kafka//lib/kafka/client.rb#806
  def controller_broker; end

  # Create partitions for a topic.
  #
  # the topic
  # partitions to be added.
  #
  # @param name [String] the name of the topic.
  # @param num_partitions [Integer] the number of desired partitions for
  # @param timeout [Integer] a duration of time to wait for the new
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/client.rb#703
  def create_partitions_for(name, num_partitions: T.unsafe(nil), timeout: T.unsafe(nil)); end

  # Creates a topic in the cluster.
  #
  # @example Creating a topic with log compaction
  #   # Enable log compaction:
  #   config = { "cleanup.policy" => "compact" }
  #
  #   # Create the topic:
  #   kafka.create_topic("dns-mappings", config: config)
  # @param name [String] the name of the topic.
  # @param num_partitions [Integer] the number of partitions that should be created
  #   in the topic.
  # @param replication_factor [Integer] the replication factor of the topic.
  # @param config [Hash] topic configuration entries. See
  #   [the Kafka documentation](https://kafka.apache.org/documentation/#topicconfigs)
  #   for more information.
  # @param timeout [Integer] a duration of time to wait for the topic to be
  #   completely created.
  # @raise [Kafka::TopicAlreadyExists] if the topic already exists.
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/client.rb#622
  def create_topic(name, num_partitions: T.unsafe(nil), replication_factor: T.unsafe(nil), timeout: T.unsafe(nil), config: T.unsafe(nil)); end

  # Delete a topic in the cluster.
  #
  # @param name [String] the name of the topic.
  # @param timeout [Integer] a duration of time to wait for the topic to be
  #   completely marked deleted.
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/client.rb#638
  def delete_topic(name, timeout: T.unsafe(nil)); end

  # Delivers a single message to the Kafka cluster.
  #
  # **Note:** Only use this API for low-throughput scenarios. If you want to deliver
  # many messages at a high rate, or if you want to configure the way messages are
  # sent, use the {#producer} or {#async_producer} APIs instead.
  #
  # @param value [String, nil] the message value.
  # @param key [String, nil] the message key.
  # @param headers [Hash<String, String>] the headers for the message.
  # @param topic [String] the topic that the message should be written to.
  # @param partition [Integer, nil] the partition that the message should be written
  #   to, or `nil` if either `partition_key` is passed or the partition should be
  #   chosen at random.
  # @param partition_key [String] a value used to deterministically choose a
  #   partition to write to.
  # @param retries [Integer] the number of times to retry the delivery before giving
  #   up.
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/client.rb#163
  def deliver_message(value, topic:, key: T.unsafe(nil), headers: T.unsafe(nil), partition: T.unsafe(nil), partition_key: T.unsafe(nil), retries: T.unsafe(nil)); end

  # Describe broker configs
  #
  # @param broker_id [int] the id of the broker
  # @param configs [Array] array of config keys.
  # @return [Array<Kafka::Protocol::DescribeConfigsResponse::ConfigEntry>]
  #
  # source://ruby-kafka//lib/kafka/client.rb#589
  def describe_configs(broker_id, configs = T.unsafe(nil)); end

  # Describe a consumer group
  #
  # @param group_id [String] the id of the consumer group
  # @return [Kafka::Protocol::DescribeGroupsResponse::Group]
  #
  # source://ruby-kafka//lib/kafka/client.rb#683
  def describe_group(group_id); end

  # Describe the configuration of a topic.
  #
  # Retrieves the topic configuration from the Kafka brokers. Configuration names
  # refer to [Kafka's topic-level configs](https://kafka.apache.org/documentation/#topicconfigs).
  #
  # @example Describing the cleanup policy config of a topic
  #   kafka = Kafka.new(["kafka1:9092"])
  #   kafka.describe_topic("my-topic", ["cleanup.policy"])
  #   #=> { "cleanup.policy" => "delete" }
  # @note This is an alpha level API and is subject to change.
  # @param name [String] the name of the topic.
  # @param configs [Array<String>] array of desired config names.
  # @return [Hash<String, String>]
  #
  # source://ruby-kafka//lib/kafka/client.rb#657
  def describe_topic(name, configs = T.unsafe(nil)); end

  # Enumerate all messages in a topic.
  #
  # @param topic [String] the topic to consume messages from.
  # @param start_from_beginning [Boolean] whether to start from the beginning
  #   of the topic or just subscribe to new messages being produced.
  # @param max_wait_time [Integer] the maximum amount of time to wait before
  #   the server responds, in seconds.
  # @param min_bytes [Integer] the minimum number of bytes to wait for. If set to
  #   zero, the broker will respond immediately, but the response may be empty.
  #   The default is 1 byte, which means that the broker will respond as soon as
  #   a message is written to the partition.
  # @param max_bytes [Integer] the maximum number of bytes to include in the
  #   response message set. Default is 1 MB. You need to set this higher if you
  #   expect messages to be larger than this.
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/client.rb#558
  def each_message(topic:, start_from_beginning: T.unsafe(nil), max_wait_time: T.unsafe(nil), min_bytes: T.unsafe(nil), max_bytes: T.unsafe(nil), &block); end

  # Fetch all committed offsets for a consumer group
  #
  # @param group_id [String] the id of the consumer group
  # @return [Hash<String, Hash<Integer, Kafka::Protocol::OffsetFetchResponse::PartitionOffsetInfo>>]
  #
  # source://ruby-kafka//lib/kafka/client.rb#691
  def fetch_group_offsets(group_id); end

  # Fetches a batch of messages from a single partition. Note that it's possible
  # to get back empty batches.
  #
  # The starting point for the fetch can be configured with the `:offset` argument.
  # If you pass a number, the fetch will start at that offset. However, there are
  # two special Symbol values that can be passed instead:
  #
  # * `:earliest` — the first offset in the partition.
  # * `:latest` — the next offset that will be written to, effectively making the
  #   call block until there is a new message in the partition.
  #
  # The Kafka protocol specifies the numeric values of these two options: -2 and -1,
  # respectively. You can also pass in these numbers directly.
  #
  # ## Example
  #
  # When enumerating the messages in a partition, you typically fetch batches
  # sequentially.
  #
  #     offset = :earliest
  #
  #     loop do
  #       messages = kafka.fetch_messages(
  #         topic: "my-topic",
  #         partition: 42,
  #         offset: offset,
  #       )
  #
  #       messages.each do |message|
  #         puts message.offset, message.key, message.value
  #
  #         # Set the next offset that should be read to be the subsequent
  #         # offset.
  #         offset = message.offset + 1
  #       end
  #     end
  #
  # See a working example in `examples/simple-consumer.rb`.
  #
  # @param topic [String] the topic that messages should be fetched from.
  # @param partition [Integer] the partition that messages should be fetched from.
  # @param offset [Integer, Symbol] the offset to start reading from. Default is
  #   the latest offset.
  # @param max_wait_time [Integer] the maximum amount of time to wait before
  #   the server responds, in seconds.
  # @param min_bytes [Integer] the minimum number of bytes to wait for. If set to
  #   zero, the broker will respond immediately, but the response may be empty.
  #   The default is 1 byte, which means that the broker will respond as soon as
  #   a message is written to the partition.
  # @param max_bytes [Integer] the maximum number of bytes to include in the
  #   response message set. Default is 1 MB. You need to set this higher if you
  #   expect messages to be larger than this.
  # @return [Array<Kafka::FetchedMessage>] the messages returned from the broker.
  #
  # source://ruby-kafka//lib/kafka/client.rb#510
  def fetch_messages(topic:, partition:, offset: T.unsafe(nil), max_wait_time: T.unsafe(nil), min_bytes: T.unsafe(nil), max_bytes: T.unsafe(nil), retries: T.unsafe(nil)); end

  # Lists all consumer groups in the cluster
  #
  # @return [Array<String>] the list of group ids
  #
  # source://ruby-kafka//lib/kafka/client.rb#725
  def groups; end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/client.rb#729
  def has_topic?(topic); end

  # Retrieve the offset of the last message in a partition. If there are no
  # messages in the partition -1 is returned.
  #
  # @param topic [String]
  # @param partition [Integer]
  # @return [Integer] the offset of the last message in the partition, or -1 if
  #   there are no messages in the partition.
  #
  # source://ruby-kafka//lib/kafka/client.rb#758
  def last_offset_for(topic, partition); end

  # Retrieve the offset of the last message in each partition of the specified topics.
  #
  # @example
  #   last_offsets_for('topic-1', 'topic-2') # =>
  #   # {
  #   #   'topic-1' => { 0 => 100, 1 => 100 },
  #   #   'topic-2' => { 0 => 100, 1 => 100 }
  #   # }
  # @param topics [Array<String>] topic names.
  # @return [Hash<String, Hash<Integer, Integer>>]
  #
  # source://ruby-kafka//lib/kafka/client.rb#774
  def last_offsets_for(*topics); end

  # Counts the number of partitions in a topic.
  #
  # @param topic [String]
  # @return [Integer] the number of partitions in the topic.
  #
  # source://ruby-kafka//lib/kafka/client.rb#739
  def partitions_for(topic); end

  # Initializes a new Kafka producer.
  #
  # @param ack_timeout [Integer] The number of seconds a broker can wait for
  #   replicas to acknowledge a write before responding with a timeout.
  # @param required_acks [Integer, Symbol] The number of replicas that must acknowledge
  #   a write, or `:all` if all in-sync replicas must acknowledge.
  # @param max_retries [Integer] the number of retries that should be attempted
  #   before giving up sending messages to the cluster. Does not include the
  #   original attempt.
  # @param retry_backoff [Integer] the number of seconds to wait between retries.
  # @param max_buffer_size [Integer] the number of messages allowed in the buffer
  #   before new writes will raise {BufferOverflow} exceptions.
  # @param max_buffer_bytesize [Integer] the maximum size of the buffer in bytes.
  #   attempting to produce messages when the buffer reaches this size will
  #   result in {BufferOverflow} being raised.
  # @param compression_codec [Symbol, nil] the name of the compression codec to
  #   use, or nil if no compression should be performed. Valid codecs: `:snappy`,
  #   `:gzip`, `:lz4`, `:zstd`
  # @param compression_threshold [Integer] the number of messages that needs to
  #   be in a message set before it should be compressed. Note that message sets
  #   are per-partition rather than per-topic or per-producer.
  # @param interceptors [Array<Object>] a list of producer interceptors the implement
  #   `call(Kafka::PendingMessage)`.
  # @return [Kafka::Producer] the Kafka producer.
  #
  # source://ruby-kafka//lib/kafka/client.rb#278
  def producer(compression_codec: T.unsafe(nil), compression_threshold: T.unsafe(nil), ack_timeout: T.unsafe(nil), required_acks: T.unsafe(nil), max_retries: T.unsafe(nil), retry_backoff: T.unsafe(nil), max_buffer_size: T.unsafe(nil), max_buffer_bytesize: T.unsafe(nil), idempotent: T.unsafe(nil), transactional: T.unsafe(nil), transactional_id: T.unsafe(nil), transactional_timeout: T.unsafe(nil), interceptors: T.unsafe(nil)); end

  # Counts the number of replicas for a topic's partition
  #
  # @param topic [String]
  # @return [Integer] the number of replica nodes for the topic's partition
  #
  # source://ruby-kafka//lib/kafka/client.rb#747
  def replica_count_for(topic); end

  # Check whether current cluster supports a specific version or not
  #
  # @param api_key [Integer] API key.
  # @param version [Integer] API version.
  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/client.rb#788
  def supports_api?(api_key, version = T.unsafe(nil)); end

  # Lists all topics in the cluster.
  #
  # @return [Array<String>] the list of topic names.
  #
  # source://ruby-kafka//lib/kafka/client.rb#710
  def topics; end

  private

  # source://ruby-kafka//lib/kafka/client.rb#819
  def initialize_cluster; end

  # source://ruby-kafka//lib/kafka/client.rb#833
  def normalize_seed_brokers(seed_brokers); end
end

# A cluster represents the state of a Kafka cluster. It needs to be initialized
# with a non-empty list of seed brokers. The first seed broker that the cluster can connect
# to will be asked for the cluster metadata, which allows the cluster to map topic
# partitions to the current leader for those partitions.
#
# source://ruby-kafka//lib/kafka/cluster.rb#13
class Kafka::Cluster
  # Initializes a Cluster with a set of seed brokers.
  #
  # The cluster will try to fetch cluster metadata from one of the brokers.
  #
  # @param seed_brokers [Array<URI>]
  # @param broker_pool [Kafka::BrokerPool]
  # @param logger [Logger]
  # @param resolve_seed_brokers [Boolean] See {Kafka::Client#initialize}
  # @return [Cluster] a new instance of Cluster
  #
  # source://ruby-kafka//lib/kafka/cluster.rb#23
  def initialize(seed_brokers:, broker_pool:, logger:, resolve_seed_brokers: T.unsafe(nil)); end

  # Adds a list of topics to the target list. Only the topics on this list will
  # be queried for metadata.
  #
  # @param topics [Array<String>]
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/cluster.rb#45
  def add_target_topics(topics); end

  # source://ruby-kafka//lib/kafka/cluster.rb#166
  def alter_configs(broker_id, configs = T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/cluster.rb#271
  def alter_topic(name, configs = T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/cluster.rb#64
  def api_info(api_key); end

  # source://ruby-kafka//lib/kafka/cluster.rb#79
  def apis; end

  # Clears the list of target topics.
  #
  # @return [nil]
  # @see #add_target_topics
  #
  # source://ruby-kafka//lib/kafka/cluster.rb#94
  def clear_target_topics; end

  # source://ruby-kafka//lib/kafka/cluster.rb#404
  def cluster_info; end

  # source://ruby-kafka//lib/kafka/cluster.rb#310
  def create_partitions_for(name, num_partitions:, timeout:); end

  # source://ruby-kafka//lib/kafka/cluster.rb#192
  def create_topic(name, num_partitions:, replication_factor:, timeout:, config:); end

  # source://ruby-kafka//lib/kafka/cluster.rb#233
  def delete_topic(name, timeout:); end

  # source://ruby-kafka//lib/kafka/cluster.rb#149
  def describe_configs(broker_id, configs = T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/cluster.rb#289
  def describe_group(group_id); end

  # source://ruby-kafka//lib/kafka/cluster.rb#252
  def describe_topic(name, configs = T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/cluster.rb#400
  def disconnect; end

  # source://ruby-kafka//lib/kafka/cluster.rb#296
  def fetch_group_offsets(group_id); end

  # Finds the broker acting as the coordinator of the given group.
  #
  # @param group_id [String]
  # @return [Broker] the broker that's currently coordinator.
  #
  # source://ruby-kafka//lib/kafka/cluster.rb#125
  def get_group_coordinator(group_id:); end

  # Finds the broker acting as the leader of the given topic and partition.
  #
  # @param topic [String]
  # @param partition [Integer]
  # @return [Broker] the broker that's currently leader.
  #
  # source://ruby-kafka//lib/kafka/cluster.rb#117
  def get_leader(topic, partition); end

  # Finds the broker acting as the coordinator of the given transaction.
  #
  # @param transactional_id [String]
  # @return [Broker] the broker that's currently coordinator.
  #
  # source://ruby-kafka//lib/kafka/cluster.rb#135
  def get_transaction_coordinator(transactional_id:); end

  # source://ruby-kafka//lib/kafka/cluster.rb#391
  def list_groups; end

  # Lists all topics in the cluster.
  #
  # source://ruby-kafka//lib/kafka/cluster.rb#384
  def list_topics; end

  # source://ruby-kafka//lib/kafka/cluster.rb#99
  def mark_as_stale!; end

  # source://ruby-kafka//lib/kafka/cluster.rb#183
  def partitions_for(topic); end

  # source://ruby-kafka//lib/kafka/cluster.rb#103
  def refresh_metadata!; end

  # source://ruby-kafka//lib/kafka/cluster.rb#108
  def refresh_metadata_if_necessary!; end

  # source://ruby-kafka//lib/kafka/cluster.rb#372
  def resolve_offset(topic, partition, offset); end

  # source://ruby-kafka//lib/kafka/cluster.rb#330
  def resolve_offsets(topic, partitions, offset); end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/cluster.rb#68
  def supports_api?(api_key, version = T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/cluster.rb#376
  def topics; end

  private

  # source://ruby-kafka//lib/kafka/cluster.rb#463
  def connect_to_broker(broker_id); end

  # source://ruby-kafka//lib/kafka/cluster.rb#469
  def controller_broker; end

  # Fetches the cluster metadata.
  #
  # This is used to update the partition leadership information, among other things.
  # The methods will go through each node listed in `seed_brokers`, connecting to the
  # first one that is available. This node will be queried for the cluster metadata.
  #
  # @raise [ConnectionError] if none of the nodes in `seed_brokers` are available.
  # @return [Protocol::MetadataResponse] the cluster metadata.
  #
  # source://ruby-kafka//lib/kafka/cluster.rb#422
  def fetch_cluster_info; end

  # @raise [Kafka::Error]
  #
  # source://ruby-kafka//lib/kafka/cluster.rb#473
  def get_coordinator(coordinator_type, coordinator_key); end

  # source://ruby-kafka//lib/kafka/cluster.rb#410
  def get_leader_id(topic, partition); end

  # source://ruby-kafka//lib/kafka/cluster.rb#457
  def random_broker; end
end

# 31
#
# source://ruby-kafka//lib/kafka.rb#196
class Kafka::ClusterAuthorizationFailed < ::Kafka::ProtocolError; end

# source://ruby-kafka//lib/kafka/compression.rb#9
module Kafka::Compression
  class << self
    # source://ruby-kafka//lib/kafka/compression.rb#21
    def codecs; end

    # source://ruby-kafka//lib/kafka/compression.rb#25
    def find_codec(name); end

    # source://ruby-kafka//lib/kafka/compression.rb#35
    def find_codec_by_id(codec_id); end
  end
end

# source://ruby-kafka//lib/kafka/compression.rb#17
Kafka::Compression::CODECS_BY_ID = T.let(T.unsafe(nil), Hash)

# source://ruby-kafka//lib/kafka/compression.rb#10
Kafka::Compression::CODECS_BY_NAME = T.let(T.unsafe(nil), Hash)

# Compresses message sets using a specified codec.
#
# A message set is only compressed if its size meets the defined threshold.
#
# ## Instrumentation
#
# Whenever a message set is compressed, the notification
# `compress.compressor.kafka` will be emitted with the following payload:
#
# * `message_count` – the number of messages in the message set.
# * `uncompressed_bytesize` – the byte size of the original data.
# * `compressed_bytesize` – the byte size of the compressed data.
#
# source://ruby-kafka//lib/kafka/compressor.rb#20
class Kafka::Compressor
  # @param codec_name [Symbol, nil]
  # @param threshold [Integer] the minimum number of messages in a message set
  #   that will trigger compression.
  # @return [Compressor] a new instance of Compressor
  #
  # source://ruby-kafka//lib/kafka/compressor.rb#26
  def initialize(instrumenter:, codec_name: T.unsafe(nil), threshold: T.unsafe(nil)); end

  # Returns the value of attribute codec.
  #
  # source://ruby-kafka//lib/kafka/compressor.rb#21
  def codec; end

  # @param record_batch [Protocol::RecordBatch]
  # @param offset [Integer] used to simulate broker behaviour in tests
  # @return [Protocol::RecordBatch]
  #
  # source://ruby-kafka//lib/kafka/compressor.rb#37
  def compress(record_batch, offset: T.unsafe(nil)); end

  private

  # source://ruby-kafka//lib/kafka/compressor.rb#48
  def compress_message_set(message_set, offset); end

  # source://ruby-kafka//lib/kafka/compressor.rb#69
  def compress_record_batch(record_batch); end
end

# 51
# The producer attempted to update a transaction while another concurrent operation on the same transaction was ongoing
#
# source://ruby-kafka//lib/kafka.rb#290
class Kafka::ConcurrentTransactionError < ::Kafka::Error; end

# A connection to a single Kafka broker.
#
# Usually you'll need a separate connection to each broker in a cluster, since most
# requests must be directed specifically to the broker that is currently leader for
# the set of topic partitions you want to produce to or consume from.
#
# ## Instrumentation
#
# Connections emit a `request.connection.kafka` notification on each request. The following
# keys will be found in the payload:
#
# * `:api` — the name of the API being invoked.
# * `:request_size` — the number of bytes in the request.
# * `:response_size` — the number of bytes in the response.
#
# The notification also includes the duration of the request.
#
# source://ruby-kafka//lib/kafka/connection.rb#29
class Kafka::Connection
  # Opens a connection to a Kafka broker.
  #
  # @param host [String] the hostname of the broker.
  # @param port [Integer] the port of the broker.
  # @param client_id [String] the client id is a user-specified string sent in each
  #   request to help trace calls and should logically identify the application
  #   making the request.
  # @param logger [Logger] the logger used to log trace messages.
  # @param connect_timeout [Integer] the socket timeout for connecting to the broker.
  #   Default is 10 seconds.
  # @param socket_timeout [Integer] the socket timeout for reading and writing to the
  #   broker. Default is 10 seconds.
  # @return [Connection] a new connection.
  #
  # source://ruby-kafka//lib/kafka/connection.rb#53
  def initialize(host:, port:, client_id:, logger:, instrumenter:, connect_timeout: T.unsafe(nil), socket_timeout: T.unsafe(nil), ssl_context: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/connection.rb#74
  def close; end

  # Returns the value of attribute decoder.
  #
  # source://ruby-kafka//lib/kafka/connection.rb#37
  def decoder; end

  # Returns the value of attribute encoder.
  #
  # source://ruby-kafka//lib/kafka/connection.rb#36
  def encoder; end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/connection.rb#70
  def open?; end

  # Sends a request over the connection.
  #
  # @param request [#encode, #response_class] the request that should be
  #   encoded and written.
  # @return [Object] the response.
  #
  # source://ruby-kafka//lib/kafka/connection.rb#86
  def send_request(request); end

  # source://ruby-kafka//lib/kafka/connection.rb#66
  def to_s; end

  private

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/connection.rb#150
  def idle?; end

  # source://ruby-kafka//lib/kafka/connection.rb#126
  def open; end

  # Reads a response from the connection.
  #
  # @param response_class [#decode] an object that can decode the response from
  #   a given Decoder.
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/connection.rb#185
  def read_response(response_class, notification); end

  # source://ruby-kafka//lib/kafka/connection.rb#205
  def wait_for_response(response_class, notification); end

  # Writes a request over the connection.
  #
  # @param request [#encode] the request that should be encoded and written.
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/connection.rb#159
  def write_request(request, notification); end
end

# source://ruby-kafka//lib/kafka/connection.rb#31
Kafka::Connection::CONNECT_TIMEOUT = T.let(T.unsafe(nil), Integer)

# Time after which an idle connection will be reopened.
#
# source://ruby-kafka//lib/kafka/connection.rb#34
Kafka::Connection::IDLE_TIMEOUT = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/connection.rb#30
Kafka::Connection::SOCKET_TIMEOUT = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/connection_builder.rb#4
class Kafka::ConnectionBuilder
  # @return [ConnectionBuilder] a new instance of ConnectionBuilder
  #
  # source://ruby-kafka//lib/kafka/connection_builder.rb#5
  def initialize(client_id:, logger:, instrumenter:, connect_timeout:, socket_timeout:, ssl_context:, sasl_authenticator:); end

  # source://ruby-kafka//lib/kafka/connection_builder.rb#15
  def build_connection(host, port); end
end

# Raised when there's a network connection error.
#
# source://ruby-kafka//lib/kafka.rb#318
class Kafka::ConnectionError < ::Kafka::Error; end

# A client that consumes messages from a Kafka cluster in coordination with
# other clients.
#
# A Consumer subscribes to one or more Kafka topics; all consumers with the
# same *group id* then agree on who should read from the individual topic
# partitions. When group members join or leave, the group synchronizes,
# making sure that all partitions are assigned to a single member, and that
# all members have some partitions to read from.
#
# ## Example
#
# A simple producer that simply writes the messages it consumes to the
# console.
#
#     require "kafka"
#
#     kafka = Kafka.new(["kafka1:9092", "kafka2:9092"])
#
#     # Create a new Consumer instance in the group `my-group`:
#     consumer = kafka.consumer(group_id: "my-group")
#
#     # Subscribe to a Kafka topic:
#     consumer.subscribe("messages")
#
#     # Loop forever, reading in messages from all topics that have been
#     # subscribed to.
#     consumer.each_message do |message|
#       puts message.topic
#       puts message.partition
#       puts message.key
#       puts message.headers
#       puts message.value
#       puts message.offset
#     end
#
# source://ruby-kafka//lib/kafka/consumer.rb#46
class Kafka::Consumer
  # @return [Consumer] a new instance of Consumer
  #
  # source://ruby-kafka//lib/kafka/consumer.rb#48
  def initialize(cluster:, logger:, instrumenter:, group:, fetcher:, offset_manager:, session_timeout:, heartbeat:, refresh_topic_interval: T.unsafe(nil), interceptors: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/consumer.rb#387
  def commit_offsets; end

  # Fetches and enumerates the messages in the topics that the consumer group
  # subscribes to.
  #
  # Each batch of messages is yielded to the provided block. If the block returns
  # without raising an exception, the batch will be considered successfully
  # processed. At regular intervals the offset of the most recent successfully
  # processed message batch in each partition will be committed to the Kafka
  # offset store. If the consumer crashes or leaves the group, the group member
  # that is tasked with taking over processing of these partitions will resume
  # at the last committed offsets.
  #
  # @param min_bytes [Integer] the minimum number of bytes to read before
  #   returning messages from each broker; if `max_wait_time` is reached, this
  #   is ignored.
  # @param max_bytes [Integer] the maximum number of bytes to read before
  #   returning messages from each broker.
  # @param max_wait_time [Integer, Float] the maximum duration of time to wait before
  #   returning messages from each broker, in seconds.
  # @param automatically_mark_as_processed [Boolean] whether to automatically
  #   mark a batch's messages as successfully processed when the block returns
  #   without an exception. Once marked successful, the offsets of processed
  #   messages can be committed to Kafka.
  # @raise [Kafka::ProcessingError] if there was an error processing a batch.
  #   The original exception will be returned by calling `#cause` on the
  #   {Kafka::ProcessingError} instance.
  # @return [nil]
  # @yieldparam batch [Kafka::FetchedBatch] a message batch fetched from Kafka.
  #
  # source://ruby-kafka//lib/kafka/consumer.rb#304
  def each_batch(min_bytes: T.unsafe(nil), max_bytes: T.unsafe(nil), max_wait_time: T.unsafe(nil), automatically_mark_as_processed: T.unsafe(nil)); end

  # Fetches and enumerates the messages in the topics that the consumer group
  # subscribes to.
  #
  # Each message is yielded to the provided block. If the block returns
  # without raising an exception, the message will be considered successfully
  # processed. At regular intervals the offset of the most recent successfully
  # processed message in each partition will be committed to the Kafka
  # offset store. If the consumer crashes or leaves the group, the group member
  # that is tasked with taking over processing of these partitions will resume
  # at the last committed offsets.
  #
  # @param min_bytes [Integer] the minimum number of bytes to read before
  #   returning messages from each broker; if `max_wait_time` is reached, this
  #   is ignored.
  # @param max_bytes [Integer] the maximum number of bytes to read before
  #   returning messages from each broker.
  # @param max_wait_time [Integer, Float] the maximum duration of time to wait before
  #   returning messages from each broker, in seconds.
  # @param automatically_mark_as_processed [Boolean] whether to automatically
  #   mark a message as successfully processed when the block returns
  #   without an exception. Once marked successful, the offsets of processed
  #   messages can be committed to Kafka.
  # @raise [Kafka::ProcessingError] if there was an error processing a message.
  #   The original exception will be returned by calling `#cause` on the
  #   {Kafka::ProcessingError} instance.
  # @return [nil]
  # @yieldparam message [Kafka::FetchedMessage] a message fetched from Kafka.
  #
  # source://ruby-kafka//lib/kafka/consumer.rb#215
  def each_message(min_bytes: T.unsafe(nil), max_bytes: T.unsafe(nil), max_wait_time: T.unsafe(nil), automatically_mark_as_processed: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/consumer.rb#391
  def mark_message_as_processed(message); end

  # Pause processing of a specific topic partition.
  #
  # When a specific message causes the processor code to fail, it can be a good
  # idea to simply pause the partition until the error can be resolved, allowing
  # the rest of the partitions to continue being processed.
  #
  # If the `timeout` argument is passed, the partition will automatically be
  # resumed when the timeout expires. If `exponential_backoff` is enabled, each
  # subsequent pause will cause the timeout to double until a message from the
  # partition has been successfully processed.
  #
  # @param topic [String]
  # @param partition [Integer]
  # @param timeout [nil, Integer] the number of seconds to pause the partition for,
  #   or `nil` if the partition should not be automatically resumed.
  # @param max_timeout [nil, Integer] the maximum number of seconds to pause for,
  #   or `nil` if no maximum should be enforced.
  # @param exponential_backoff [Boolean] whether to enable exponential backoff.
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/consumer.rb#152
  def pause(topic, partition, timeout: T.unsafe(nil), max_timeout: T.unsafe(nil), exponential_backoff: T.unsafe(nil)); end

  # Whether the topic partition is currently paused.
  #
  # @param topic [String]
  # @param partition [Integer]
  # @return [Boolean] true if the partition is paused, false otherwise.
  # @see #pause
  #
  # source://ruby-kafka//lib/kafka/consumer.rb#183
  def paused?(topic, partition); end

  # Resume processing of a topic partition.
  #
  # @param topic [String]
  # @param partition [Integer]
  # @return [nil]
  # @see #pause
  #
  # source://ruby-kafka//lib/kafka/consumer.rb#170
  def resume(topic, partition); end

  # Move the consumer's position in a topic partition to the specified offset.
  #
  # Note that this has to be done prior to calling {#each_message} or {#each_batch}
  # and only has an effect if the consumer is assigned the partition. Typically,
  # you will want to do this in every consumer group member in order to make sure
  # that the member that's assigned the partition knows where to start.
  #
  # @param topic [String]
  # @param partition [Integer]
  # @param offset [Integer]
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/consumer.rb#383
  def seek(topic, partition, offset); end

  # source://ruby-kafka//lib/kafka/consumer.rb#399
  def send_heartbeat; end

  # Aliases for the external API compatibility
  #
  # source://ruby-kafka//lib/kafka/consumer.rb#395
  def send_heartbeat_if_necessary; end

  # Stop the consumer.
  #
  # The consumer will finish any in-progress work and shut down.
  #
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/consumer.rb#128
  def stop; end

  # Subscribes the consumer to a topic.
  #
  # Typically you either want to start reading messages from the very
  # beginning of the topic's partitions or you simply want to wait for new
  # messages to be written. In the former case, set `start_from_beginning`
  # to true (the default); in the latter, set it to false.
  #
  # @param topic_or_regex [String, Regexp] subscribe to single topic with a string
  #   or multiple topics matching a regex.
  # @param default_offset [Symbol] whether to start from the beginning or the
  #   end of the topic's partitions. Deprecated.
  # @param start_from_beginning [Boolean] whether to start from the beginning
  #   of the topic or just subscribe to new messages being produced. This
  #   only applies when first consuming a topic partition – once the consumer
  #   has checkpointed its progress, it will always resume from the last
  #   checkpoint.
  # @param max_bytes_per_partition [Integer] the maximum amount of data fetched
  #   from a single partition at a time.
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/consumer.rb#110
  def subscribe(topic_or_regex, default_offset: T.unsafe(nil), start_from_beginning: T.unsafe(nil), max_bytes_per_partition: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/consumer.rb#395
  def trigger_heartbeat; end

  # source://ruby-kafka//lib/kafka/consumer.rb#399
  def trigger_heartbeat!; end

  private

  # source://ruby-kafka//lib/kafka/consumer.rb#592
  def clear_current_offsets(excluding: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/consumer.rb#630
  def cluster_topics; end

  # source://ruby-kafka//lib/kafka/consumer.rb#409
  def consumer_loop; end

  # source://ruby-kafka//lib/kafka/consumer.rb#542
  def fetch_batches; end

  # source://ruby-kafka//lib/kafka/consumer.rb#470
  def join_group; end

  # source://ruby-kafka//lib/kafka/consumer.rb#455
  def make_final_offsets_commit!(attempts = T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/consumer.rb#580
  def pause_for(topic, partition); end

  # source://ruby-kafka//lib/kafka/consumer.rb#534
  def refresh_topic_list_if_enabled; end

  # source://ruby-kafka//lib/kafka/consumer.rb#517
  def resume_paused_partitions!; end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/consumer.rb#584
  def running?; end

  # source://ruby-kafka//lib/kafka/consumer.rb#600
  def scan_for_subscribing; end

  # source://ruby-kafka//lib/kafka/consumer.rb#505
  def seek_to_next(topic, partition); end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/consumer.rb#588
  def shutting_down?; end

  # source://ruby-kafka//lib/kafka/consumer.rb#613
  def subscribe_to_regex(topic_regex, default_offset, start_from_beginning, max_bytes_per_partition); end

  # source://ruby-kafka//lib/kafka/consumer.rb#619
  def subscribe_to_topic(topic, default_offset, start_from_beginning, max_bytes_per_partition); end
end

# source://ruby-kafka//lib/kafka/consumer_group/assignor.rb#6
class Kafka::ConsumerGroup
  # @return [ConsumerGroup] a new instance of ConsumerGroup
  #
  # source://ruby-kafka//lib/kafka/consumer_group.rb#11
  def initialize(cluster:, logger:, group_id:, session_timeout:, rebalance_timeout:, retention_time:, instrumenter:, assignment_strategy:); end

  # Returns the value of attribute assigned_partitions.
  #
  # source://ruby-kafka//lib/kafka/consumer_group.rb#9
  def assigned_partitions; end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/consumer_group.rb#39
  def assigned_to?(topic, partition); end

  # source://ruby-kafka//lib/kafka/consumer_group.rb#86
  def commit_offsets(offsets); end

  # source://ruby-kafka//lib/kafka/consumer_group.rb#79
  def fetch_offsets; end

  # Returns the value of attribute generation_id.
  #
  # source://ruby-kafka//lib/kafka/consumer_group.rb#9
  def generation_id; end

  # Returns the value of attribute group_id.
  #
  # source://ruby-kafka//lib/kafka/consumer_group.rb#9
  def group_id; end

  # source://ruby-kafka//lib/kafka/consumer_group.rb#105
  def heartbeat; end

  # source://ruby-kafka//lib/kafka/consumer_group.rb#47
  def join; end

  # source://ruby-kafka//lib/kafka/consumer_group.rb#67
  def leave; end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/consumer_group.rb#43
  def member?; end

  # source://ruby-kafka//lib/kafka/consumer_group.rb#30
  def subscribe(topic); end

  # source://ruby-kafka//lib/kafka/consumer_group.rb#35
  def subscribed_partitions; end

  # source://ruby-kafka//lib/kafka/consumer_group.rb#133
  def to_s; end

  private

  # source://ruby-kafka//lib/kafka/consumer_group.rb#221
  def coordinator; end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/consumer_group.rb#182
  def group_leader?; end

  # source://ruby-kafka//lib/kafka/consumer_group.rb#144
  def join_group; end

  # source://ruby-kafka//lib/kafka/consumer_group.rb#186
  def synchronize; end
end

# A consumer group partition assignor
#
# source://ruby-kafka//lib/kafka/consumer_group/assignor.rb#9
class Kafka::ConsumerGroup::Assignor
  # @param cluster [Kafka::Cluster]
  # @param strategy [Object] an object that implements #protocol_type,
  #   #user_data, and #assign.
  # @return [Assignor] a new instance of Assignor
  #
  # source://ruby-kafka//lib/kafka/consumer_group/assignor.rb#15
  def initialize(cluster:, strategy:); end

  # Assign the topic partitions to the group members.
  #
  # @param members [Hash<String, Kafka::Protocol::JoinGroupResponse::Metadata>] a hash
  #   mapping member ids to metadata.
  # @param topics [Array<String>] topics
  # @return [Hash<String, Kafka::Protocol::MemberAssignment>] a hash mapping member
  #   ids to assignments.
  #
  # source://ruby-kafka//lib/kafka/consumer_group/assignor.rb#35
  def assign(members:, topics:); end

  # source://ruby-kafka//lib/kafka/consumer_group/assignor.rb#20
  def protocol_name; end

  # source://ruby-kafka//lib/kafka/consumer_group/assignor.rb#24
  def user_data; end
end

# source://ruby-kafka//lib/kafka/consumer_group/assignor.rb#10
class Kafka::ConsumerGroup::Assignor::Partition < ::Struct
  # Returns the value of attribute partition_id
  #
  # @return [Object] the current value of partition_id
  def partition_id; end

  # Sets the attribute partition_id
  #
  # @param value [Object] the value to set the attribute partition_id to.
  # @return [Object] the newly set value
  def partition_id=(_); end

  # Returns the value of attribute topic
  #
  # @return [Object] the current value of topic
  def topic; end

  # Sets the attribute topic
  #
  # @param value [Object] the value to set the attribute topic to.
  # @return [Object] the newly set value
  def topic=(_); end

  class << self
    def [](*_arg0); end
    def inspect; end
    def keyword_init?; end
    def members; end
    def new(*_arg0); end
  end
end

# 14
# The coordinator is loading and hence can't process requests.
#
# source://ruby-kafka//lib/kafka.rb#108
class Kafka::CoordinatorLoadInProgress < ::Kafka::ProtocolError; end

# 15
# The coordinator is not available.
#
# source://ruby-kafka//lib/kafka.rb#113
class Kafka::CoordinatorNotAvailable < ::Kafka::ProtocolError; end

# 2
# This indicates that a message contents does not match its CRC.
#
# source://ruby-kafka//lib/kafka.rb#41
class Kafka::CorruptMessage < ::Kafka::ProtocolError; end

# source://ruby-kafka//lib/kafka/crc32_hash.rb#6
class Kafka::Crc32Hash
  # source://ruby-kafka//lib/kafka/crc32_hash.rb#11
  def hash(value); end

  # crc32 is supported natively
  #
  # source://ruby-kafka//lib/kafka/crc32_hash.rb#9
  def load; end
end

# source://ruby-kafka//lib/kafka/instrumenter.rb#28
class Kafka::DecoratingInstrumenter
  # @return [DecoratingInstrumenter] a new instance of DecoratingInstrumenter
  #
  # source://ruby-kafka//lib/kafka/instrumenter.rb#29
  def initialize(backend, extra_payload = T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/instrumenter.rb#34
  def instrument(event_name, payload = T.unsafe(nil), &block); end
end

# Raised if not all messages could be sent by a producer.
#
# source://ruby-kafka//lib/kafka.rb#329
class Kafka::DeliveryFailed < ::Kafka::Error
  # @return [DeliveryFailed] a new instance of DeliveryFailed
  #
  # source://ruby-kafka//lib/kafka.rb#332
  def initialize(message, failed_messages); end

  # Returns the value of attribute failed_messages.
  #
  # source://ruby-kafka//lib/kafka.rb#330
  def failed_messages; end
end

# source://ruby-kafka//lib/kafka/digest.rb#7
module Kafka::Digest
  class << self
    # source://ruby-kafka//lib/kafka/digest.rb#13
    def find_digest(name); end
  end
end

# source://ruby-kafka//lib/kafka/digest.rb#8
Kafka::Digest::FUNCTIONS_BY_NAME = T.let(T.unsafe(nil), Hash)

# 46
# The broker received a duplicate sequence number
#
# source://ruby-kafka//lib/kafka.rb#265
class Kafka::DuplicateSequenceNumberError < ::Kafka::Error; end

# source://ruby-kafka//lib/kafka.rb#6
class Kafka::Error < ::StandardError; end

# source://ruby-kafka//lib/kafka.rb#351
class Kafka::FailedScramAuthentication < ::Kafka::SaslScramError; end

# source://ruby-kafka//lib/kafka.rb#345
class Kafka::FetchError < ::Kafka::Error; end

# Fetches messages from one or more partitions.
#
#     operation = Kafka::FetchOperation.new(
#       cluster: cluster,
#       logger: logger,
#       min_bytes: 1,
#       max_wait_time: 10,
#     )
#
#     # These calls will schedule fetches from the specified topics/partitions.
#     operation.fetch_from_partition("greetings", 42, offset: :latest, max_bytes: 100000)
#     operation.fetch_from_partition("goodbyes", 13, offset: :latest, max_bytes: 100000)
#
#     operation.execute
#
# source://ruby-kafka//lib/kafka/fetch_operation.rb#23
class Kafka::FetchOperation
  # @return [FetchOperation] a new instance of FetchOperation
  #
  # source://ruby-kafka//lib/kafka/fetch_operation.rb#24
  def initialize(cluster:, logger:, min_bytes: T.unsafe(nil), max_bytes: T.unsafe(nil), max_wait_time: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/fetch_operation.rb#51
  def execute; end

  # source://ruby-kafka//lib/kafka/fetch_operation.rb#37
  def fetch_from_partition(topic, partition, offset: T.unsafe(nil), max_bytes: T.unsafe(nil)); end
end

# An ordered sequence of messages fetched from a Kafka partition.
#
# source://ruby-kafka//lib/kafka/fetched_batch.rb#6
class Kafka::FetchedBatch
  # @return [FetchedBatch] a new instance of FetchedBatch
  #
  # source://ruby-kafka//lib/kafka/fetched_batch.rb#25
  def initialize(topic:, partition:, highwater_mark_offset:, messages:, last_offset: T.unsafe(nil), leader_epoch: T.unsafe(nil)); end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/fetched_batch.rb#34
  def empty?; end

  # source://ruby-kafka//lib/kafka/fetched_batch.rb#42
  def first_offset; end

  # @return [Integer] the offset of the most recent message in the partition.
  #
  # source://ruby-kafka//lib/kafka/fetched_batch.rb#20
  def highwater_mark_offset; end

  # @return [Integer]
  #
  # source://ruby-kafka//lib/kafka/fetched_batch.rb#14
  def last_offset; end

  # @return [Integer]
  #
  # source://ruby-kafka//lib/kafka/fetched_batch.rb#17
  def leader_epoch; end

  # @return [Array<Kafka::FetchedMessage>]
  #
  # source://ruby-kafka//lib/kafka/fetched_batch.rb#23
  def messages; end

  # @return [Array<Kafka::FetchedMessage>]
  #
  # source://ruby-kafka//lib/kafka/fetched_batch.rb#23
  def messages=(_arg0); end

  # source://ruby-kafka//lib/kafka/fetched_batch.rb#50
  def offset_lag; end

  # @return [Integer]
  #
  # source://ruby-kafka//lib/kafka/fetched_batch.rb#11
  def partition; end

  # @return [String]
  #
  # source://ruby-kafka//lib/kafka/fetched_batch.rb#8
  def topic; end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/fetched_batch.rb#38
  def unknown_last_offset?; end
end

# source://ruby-kafka//lib/kafka/fetched_batch_generator.rb#6
class Kafka::FetchedBatchGenerator
  # @return [FetchedBatchGenerator] a new instance of FetchedBatchGenerator
  #
  # source://ruby-kafka//lib/kafka/fetched_batch_generator.rb#10
  def initialize(topic, fetched_partition, offset, logger:); end

  # source://ruby-kafka//lib/kafka/fetched_batch_generator.rb#17
  def generate; end

  private

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/fetched_batch_generator.rb#110
  def abort_marker?(record_batch); end

  # source://ruby-kafka//lib/kafka/fetched_batch_generator.rb#29
  def empty_fetched_batch; end

  # source://ruby-kafka//lib/kafka/fetched_batch_generator.rb#39
  def extract_messages; end

  # source://ruby-kafka//lib/kafka/fetched_batch_generator.rb#62
  def extract_records; end
end

# source://ruby-kafka//lib/kafka/fetched_batch_generator.rb#8
Kafka::FetchedBatchGenerator::ABORTED_TRANSACTION_SIGNAL = T.let(T.unsafe(nil), String)

# source://ruby-kafka//lib/kafka/fetched_batch_generator.rb#7
Kafka::FetchedBatchGenerator::COMMITTED_TRANSACTION_SIGNAL = T.let(T.unsafe(nil), String)

# source://ruby-kafka//lib/kafka/fetched_message.rb#4
class Kafka::FetchedMessage
  # @return [FetchedMessage] a new instance of FetchedMessage
  #
  # source://ruby-kafka//lib/kafka/fetched_message.rb#11
  def initialize(message:, topic:, partition:); end

  # @return [Time] the timestamp of the message.
  #
  # source://ruby-kafka//lib/kafka/fetched_message.rb#33
  def create_time; end

  # @return [Hash<String, String>] the headers of the message.
  #
  # source://ruby-kafka//lib/kafka/fetched_message.rb#38
  def headers; end

  # @return [Boolean] whether this record is a control record
  #
  # source://ruby-kafka//lib/kafka/fetched_message.rb#43
  def is_control_record; end

  # @return [String] the key of the message.
  #
  # source://ruby-kafka//lib/kafka/fetched_message.rb#23
  def key; end

  # @return [Integer] the offset of the message in the partition.
  #
  # source://ruby-kafka//lib/kafka/fetched_message.rb#28
  def offset; end

  # @return [Integer] the partition number that the message was written to.
  #
  # source://ruby-kafka//lib/kafka/fetched_message.rb#9
  def partition; end

  # @return [String] the name of the topic that the message was written to.
  #
  # source://ruby-kafka//lib/kafka/fetched_message.rb#6
  def topic; end

  # @return [String] the value of the message.
  #
  # source://ruby-kafka//lib/kafka/fetched_message.rb#18
  def value; end
end

# source://ruby-kafka//lib/kafka/fetched_offset_resolver.rb#4
class Kafka::FetchedOffsetResolver
  # @return [FetchedOffsetResolver] a new instance of FetchedOffsetResolver
  #
  # source://ruby-kafka//lib/kafka/fetched_offset_resolver.rb#5
  def initialize(logger:); end

  # source://ruby-kafka//lib/kafka/fetched_offset_resolver.rb#9
  def resolve!(broker, topics); end

  private

  # source://ruby-kafka//lib/kafka/fetched_offset_resolver.rb#29
  def filter_pending_topics(topics); end
end

# source://ruby-kafka//lib/kafka/fetcher.rb#6
class Kafka::Fetcher
  # @return [Fetcher] a new instance of Fetcher
  #
  # source://ruby-kafka//lib/kafka/fetcher.rb#9
  def initialize(cluster:, logger:, instrumenter:, max_queue_size:, group:); end

  # source://ruby-kafka//lib/kafka/fetcher.rb#48
  def configure(min_bytes:, max_bytes:, max_wait_time:); end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/fetcher.rb#77
  def data?; end

  # Returns the value of attribute max_wait_time.
  #
  # source://ruby-kafka//lib/kafka/fetcher.rb#7
  def max_wait_time; end

  # source://ruby-kafka//lib/kafka/fetcher.rb#81
  def poll; end

  # Returns the value of attribute queue.
  #
  # source://ruby-kafka//lib/kafka/fetcher.rb#7
  def queue; end

  # source://ruby-kafka//lib/kafka/fetcher.rb#72
  def reset; end

  # source://ruby-kafka//lib/kafka/fetcher.rb#44
  def seek(topic, partition, offset); end

  # source://ruby-kafka//lib/kafka/fetcher.rb#52
  def start; end

  # source://ruby-kafka//lib/kafka/fetcher.rb#66
  def stop; end

  # source://ruby-kafka//lib/kafka/fetcher.rb#40
  def subscribe(topic, max_bytes_per_partition:); end

  private

  # Returns the value of attribute current_reset_counter.
  #
  # source://ruby-kafka//lib/kafka/fetcher.rb#97
  def current_reset_counter; end

  # source://ruby-kafka//lib/kafka/fetcher.rb#184
  def fetch_batches; end

  # source://ruby-kafka//lib/kafka/fetcher.rb#123
  def handle_configure(min_bytes, max_bytes, max_wait_time); end

  # source://ruby-kafka//lib/kafka/fetcher.rb#129
  def handle_reset; end

  # source://ruby-kafka//lib/kafka/fetcher.rb#149
  def handle_seek(topic, partition, offset); end

  # source://ruby-kafka//lib/kafka/fetcher.rb#134
  def handle_stop(*_arg0); end

  # source://ruby-kafka//lib/kafka/fetcher.rb#144
  def handle_subscribe(topic, max_bytes_per_partition); end

  # source://ruby-kafka//lib/kafka/fetcher.rb#99
  def loop; end

  # source://ruby-kafka//lib/kafka/fetcher.rb#159
  def step; end
end

# 30
#
# source://ruby-kafka//lib/kafka.rb#192
class Kafka::GroupAuthorizationFailed < ::Kafka::ProtocolError; end

# source://ruby-kafka//lib/kafka/gzip_codec.rb#4
class Kafka::GzipCodec
  # source://ruby-kafka//lib/kafka/gzip_codec.rb#5
  def codec_id; end

  # source://ruby-kafka//lib/kafka/gzip_codec.rb#17
  def compress(data); end

  # source://ruby-kafka//lib/kafka/gzip_codec.rb#28
  def decompress(data); end

  # source://ruby-kafka//lib/kafka/gzip_codec.rb#13
  def load; end

  # source://ruby-kafka//lib/kafka/gzip_codec.rb#9
  def produce_api_min_version; end
end

# source://ruby-kafka//lib/kafka/heartbeat.rb#4
class Kafka::Heartbeat
  # @return [Heartbeat] a new instance of Heartbeat
  #
  # source://ruby-kafka//lib/kafka/heartbeat.rb#5
  def initialize(group:, interval:, instrumenter:); end

  # source://ruby-kafka//lib/kafka/heartbeat.rb#21
  def trigger; end

  # source://ruby-kafka//lib/kafka/heartbeat.rb#12
  def trigger!; end
end

# source://ruby-kafka//lib/kafka.rb#339
class Kafka::HeartbeatError < ::Kafka::Error; end

# A connection has been unused for too long, we assume the server has killed it.
#
# source://ruby-kafka//lib/kafka.rb#311
class Kafka::IdleConnection < ::Kafka::Error; end

# 22
# Specified group generation id is not valid.
#
# source://ruby-kafka//lib/kafka.rb#154
class Kafka::IllegalGeneration < ::Kafka::ProtocolError; end

# 23
# The group member's supported protocols are incompatible with those of existing members or first group member tried to join with empty protocol type or empty protocol list.
#
# source://ruby-kafka//lib/kafka.rb#159
class Kafka::InconsistentGroupProtocol < ::Kafka::ProtocolError; end

# source://ruby-kafka//lib/kafka/instrumenter.rb#4
class Kafka::Instrumenter
  # @return [Instrumenter] a new instance of Instrumenter
  #
  # source://ruby-kafka//lib/kafka/instrumenter.rb#7
  def initialize(default_payload = T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/instrumenter.rb#17
  def instrument(event_name, payload = T.unsafe(nil), &block); end
end

# source://ruby-kafka//lib/kafka/instrumenter.rb#5
Kafka::Instrumenter::NAMESPACE = T.let(T.unsafe(nil), String)

# When the record array length doesn't match real number of received records
#
# source://ruby-kafka//lib/kafka.rb#315
class Kafka::InsufficientDataMessage < ::Kafka::Error; end

# Holds a list of interceptors that implement `call`
# and wraps calls to a chain of custom interceptors.
#
# source://ruby-kafka//lib/kafka/interceptors.rb#6
class Kafka::Interceptors
  # @return [Interceptors] a new instance of Interceptors
  #
  # source://ruby-kafka//lib/kafka/interceptors.rb#7
  def initialize(interceptors:, logger:); end

  # This method is called when the client produces a message or once the batches are fetched.
  # The message returned from the first call is passed to the second interceptor call, and so on in an
  # interceptor chain. This method does not throw exceptions.
  #
  # @param intercepted [Kafka::PendingMessage || Kafka::FetchedBatch] the produced message or
  #   fetched batch.
  # @return [Kafka::PendingMessage || Kafka::FetchedBatch] the intercepted message or batch
  #   returned by the last interceptor.
  #
  # source://ruby-kafka//lib/kafka/interceptors.rb#21
  def call(intercepted); end
end

# 28
# The committing offset data size is not valid
#
# source://ruby-kafka//lib/kafka.rb#184
class Kafka::InvalidCommitOffsetSize < ::Kafka::ProtocolError; end

# 40
#
# source://ruby-kafka//lib/kafka.rb#236
class Kafka::InvalidConfig < ::Kafka::ProtocolError; end

# 24
# The configured groupId is invalid
#
# source://ruby-kafka//lib/kafka.rb#164
class Kafka::InvalidGroupId < ::Kafka::ProtocolError; end

# 4
# The message has a negative size.
#
# source://ruby-kafka//lib/kafka.rb#51
class Kafka::InvalidMessageSize < ::Kafka::ProtocolError; end

# 37
# Number of partitions is below 1.
#
# source://ruby-kafka//lib/kafka.rb#223
class Kafka::InvalidPartitions < ::Kafka::ProtocolError; end

# 47
# Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer's transaction has been expired by the broker.
#
# source://ruby-kafka//lib/kafka.rb#270
class Kafka::InvalidProducerEpochError < ::Kafka::Error; end

# 49
# The producer attempted to use a producer id which is not currently assigned to its transactional id
#
# source://ruby-kafka//lib/kafka.rb#280
class Kafka::InvalidProducerIDMappingError < ::Kafka::Error; end

# 39
#
# source://ruby-kafka//lib/kafka.rb#232
class Kafka::InvalidReplicaAssignment < ::Kafka::ProtocolError; end

# 38
# Replication factor is below 1 or larger than the number of available brokers.
#
# source://ruby-kafka//lib/kafka.rb#228
class Kafka::InvalidReplicationFactor < ::Kafka::ProtocolError; end

# 42
#
# source://ruby-kafka//lib/kafka.rb#245
class Kafka::InvalidRequest < ::Kafka::ProtocolError; end

# 21
# Returned from a produce request if the requested requiredAcks is invalid
# (anything other than -1, 1, or 0).
#
# source://ruby-kafka//lib/kafka.rb#149
class Kafka::InvalidRequiredAcks < ::Kafka::ProtocolError; end

# 34
#
# source://ruby-kafka//lib/kafka.rb#210
class Kafka::InvalidSaslState < ::Kafka::ProtocolError; end

# 26
# The session timeout is not within the range allowed by the broker
#
# source://ruby-kafka//lib/kafka.rb#174
class Kafka::InvalidSessionTimeout < ::Kafka::ProtocolError; end

# 32
# The timestamp of the message is out of acceptable range.
#
# source://ruby-kafka//lib/kafka.rb#201
class Kafka::InvalidTimestamp < ::Kafka::ProtocolError; end

# 17
# For a request which attempts to access an invalid topic (e.g. one which has
# an illegal name), or if an attempt is made to write to an internal topic
# (such as the consumer offsets topic).
#
# source://ruby-kafka//lib/kafka.rb#125
class Kafka::InvalidTopic < ::Kafka::ProtocolError; end

# 50
# The transaction timeout is larger than the maximum value allowed by the broker (as configured by transaction.max.timeout.ms).
#
# source://ruby-kafka//lib/kafka.rb#285
class Kafka::InvalidTransactionTimeoutError < ::Kafka::Error; end

# 48
# The producer attempted a transactional operation in an invalid state
#
# source://ruby-kafka//lib/kafka.rb#275
class Kafka::InvalidTxnStateError < ::Kafka::Error; end

# source://ruby-kafka//lib/kafka/lz4_codec.rb#4
class Kafka::LZ4Codec
  # source://ruby-kafka//lib/kafka/lz4_codec.rb#5
  def codec_id; end

  # source://ruby-kafka//lib/kafka/lz4_codec.rb#19
  def compress(data); end

  # source://ruby-kafka//lib/kafka/lz4_codec.rb#23
  def decompress(data); end

  # source://ruby-kafka//lib/kafka/lz4_codec.rb#13
  def load; end

  # source://ruby-kafka//lib/kafka/lz4_codec.rb#9
  def produce_api_min_version; end
end

# 5
# This error is thrown if we are in the middle of a leadership election and
# there is currently no leader for this partition and hence it is unavailable
# for writes.
#
# source://ruby-kafka//lib/kafka.rb#58
class Kafka::LeaderNotAvailable < ::Kafka::ProtocolError; end

# Buffers messages for specific topics/partitions.
#
# source://ruby-kafka//lib/kafka/message_buffer.rb#8
class Kafka::MessageBuffer
  include ::Enumerable

  # @return [MessageBuffer] a new instance of MessageBuffer
  #
  # source://ruby-kafka//lib/kafka/message_buffer.rb#13
  def initialize; end

  # Returns the value of attribute bytesize.
  #
  # source://ruby-kafka//lib/kafka/message_buffer.rb#11
  def bytesize; end

  # Clears messages across all topics and partitions.
  #
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/message_buffer.rb#74
  def clear; end

  # Clears buffered messages for the given topic and partition.
  #
  # @param topic [String] the name of the topic.
  # @param partition [Integer] the partition id.
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/message_buffer.rb#57
  def clear_messages(topic:, partition:); end

  # source://ruby-kafka//lib/kafka/message_buffer.rb#28
  def concat(messages, topic:, partition:); end

  # source://ruby-kafka//lib/kafka/message_buffer.rb#43
  def each; end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/message_buffer.rb#39
  def empty?; end

  # source://ruby-kafka//lib/kafka/message_buffer.rb#67
  def messages_for(topic:, partition:); end

  # Returns the value of attribute size.
  #
  # source://ruby-kafka//lib/kafka/message_buffer.rb#11
  def size; end

  # source://ruby-kafka//lib/kafka/message_buffer.rb#35
  def to_h; end

  # source://ruby-kafka//lib/kafka/message_buffer.rb#19
  def write(value:, key:, topic:, partition:, create_time: T.unsafe(nil), headers: T.unsafe(nil)); end

  private

  # source://ruby-kafka//lib/kafka/message_buffer.rb#82
  def buffer_for(topic, partition); end
end

# 10
# The server has a configurable maximum message size to avoid unbounded memory
# allocation. This error is thrown if the client attempt to produce a message
# larger than this maximum.
#
# source://ruby-kafka//lib/kafka.rb#88
class Kafka::MessageSizeTooLarge < ::Kafka::ProtocolError; end

# A message in a partition is larger than the maximum we've asked for.
#
# source://ruby-kafka//lib/kafka.rb#307
class Kafka::MessageTooLargeToRead < ::Kafka::Error; end

# source://ruby-kafka//lib/kafka/murmur2_hash.rb#4
class Kafka::Murmur2Hash
  # source://ruby-kafka//lib/kafka/murmur2_hash.rb#13
  def hash(value); end

  # source://ruby-kafka//lib/kafka/murmur2_hash.rb#7
  def load; end
end

# source://ruby-kafka//lib/kafka/murmur2_hash.rb#5
Kafka::Murmur2Hash::SEED = T.let(T.unsafe(nil), String)

# 13
# The server disconnected before a response was received.
#
# source://ruby-kafka//lib/kafka.rb#103
class Kafka::NetworkException < ::Kafka::ProtocolError; end

# A fetch operation was executed with no partitions specified.
#
# source://ruby-kafka//lib/kafka.rb#303
class Kafka::NoPartitionsToFetchFrom < ::Kafka::Error; end

# source://ruby-kafka//lib/kafka.rb#321
class Kafka::NoSuchBroker < ::Kafka::Error; end

# 41
# This is not the correct controller for this cluster.
#
# source://ruby-kafka//lib/kafka.rb#241
class Kafka::NotController < ::Kafka::ProtocolError; end

# 16
# This is not the correct coordinator.
#
# source://ruby-kafka//lib/kafka.rb#118
class Kafka::NotCoordinatorForGroup < ::Kafka::ProtocolError; end

# 19
# Returned from a produce request when the number of in-sync replicas is
# lower than the configured minimum and requiredAcks is -1.
#
# source://ruby-kafka//lib/kafka.rb#137
class Kafka::NotEnoughReplicas < ::Kafka::ProtocolError; end

# 20
# Returned from a produce request when the message was written to the log,
# but with fewer in-sync replicas than required.
#
# source://ruby-kafka//lib/kafka.rb#143
class Kafka::NotEnoughReplicasAfterAppend < ::Kafka::ProtocolError; end

# 6
# This error is thrown if the client attempts to send messages to a replica
# that is not the leader for some partition. It indicates that the client's
# metadata is out of date.
#
# source://ruby-kafka//lib/kafka.rb#65
class Kafka::NotLeaderForPartition < ::Kafka::ProtocolError; end

# source://ruby-kafka//lib/kafka.rb#342
class Kafka::OffsetCommitError < ::Kafka::Error; end

# Manages a consumer's position in partitions, figures out where to resume processing
# from, etc.
#
# source://ruby-kafka//lib/kafka/offset_manager.rb#7
class Kafka::OffsetManager
  # @return [OffsetManager] a new instance of OffsetManager
  #
  # source://ruby-kafka//lib/kafka/offset_manager.rb#12
  def initialize(cluster:, group:, fetcher:, logger:, commit_interval:, commit_threshold:, offset_retention_time:); end

  # Clear all stored offset information.
  #
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/offset_manager.rb#160
  def clear_offsets; end

  # Clear stored offset information for all partitions except those specified
  # in `excluded`.
  #
  #     offset_manager.clear_offsets_excluding("my-topic" => [1, 2, 3])
  #
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/offset_manager.rb#174
  def clear_offsets_excluding(excluded); end

  # Commit offsets of messages that have been marked as processed.
  #
  # If `recommit` is set to true, we will also commit the existing positions
  # even if no messages have been processed on a partition. This is done
  # in order to avoid the offset information expiring in cases where messages
  # are very rare -- it's essentially a keep-alive.
  #
  # @param recommit [Boolean] whether to recommit offsets that have already been
  #   committed.
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/offset_manager.rb#131
  def commit_offsets(recommit = T.unsafe(nil)); end

  # Commit offsets if necessary, according to the offset commit policy specified
  # when initializing the class.
  #
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/offset_manager.rb#150
  def commit_offsets_if_necessary; end

  # Mark a message as having been processed.
  #
  # When offsets are committed, the message's offset will be stored in Kafka so
  # that we can resume from this point at a later time.
  #
  # @param topic [String] the name of the topic.
  # @param partition [Integer] the partition number.
  # @param offset [Integer] the offset of the message that should be marked as processed.
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/offset_manager.rb#52
  def mark_as_processed(topic, partition, offset); end

  # Return the next offset that should be fetched for the specified partition.
  #
  # @param topic [String] the name of the topic.
  # @param partition [Integer] the partition number.
  # @return [Integer] the next offset that should be fetched.
  #
  # source://ruby-kafka//lib/kafka/offset_manager.rb#106
  def next_offset_for(topic, partition); end

  # Move the consumer's position in the partition to the specified offset.
  #
  # @param topic [String] the name of the topic.
  # @param partition [Integer] the partition number.
  # @param offset [Integer] the offset that the consumer position should be moved to.
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/offset_manager.rb#94
  def seek_to(topic, partition, offset); end

  # Move the consumer's position in the partition back to the configured default
  # offset, either the first or latest in the partition.
  #
  # @param topic [String] the name of the topic.
  # @param partition [Integer] the partition number.
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/offset_manager.rb#79
  def seek_to_default(topic, partition); end

  # Set the default offset for a topic.
  #
  # When the consumer is started for the first time, or in cases where it gets stuck and
  # has to reset its position, it must start either with the earliest messages or with
  # the latest, skipping to the very end of each partition.
  #
  # @param topic [String] the name of the topic.
  # @param default_offset [Symbol] either `:earliest` or `:latest`.
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/offset_manager.rb#39
  def set_default_offset(topic, default_offset); end

  private

  # source://ruby-kafka//lib/kafka/offset_manager.rb#189
  def clear_resolved_offset(topic); end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/offset_manager.rb#249
  def commit_threshold_reached?; end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/offset_manager.rb#245
  def commit_timeout_reached?; end

  # source://ruby-kafka//lib/kafka/offset_manager.rb#217
  def committed_offset_for(topic, partition); end

  # source://ruby-kafka//lib/kafka/offset_manager.rb#213
  def committed_offsets; end

  # source://ruby-kafka//lib/kafka/offset_manager.rb#198
  def fetch_resolved_offsets(topic); end

  # source://ruby-kafka//lib/kafka/offset_manager.rb#221
  def offsets_to_commit(recommit = T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/offset_manager.rb#231
  def offsets_to_recommit; end

  # source://ruby-kafka//lib/kafka/offset_manager.rb#253
  def prettify_offsets(offsets); end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/offset_manager.rb#241
  def recommit_timeout_reached?; end

  # source://ruby-kafka//lib/kafka/offset_manager.rb#193
  def resolve_offset(topic, partition); end

  # source://ruby-kafka//lib/kafka/offset_manager.rb#205
  def seconds_since(time); end

  # source://ruby-kafka//lib/kafka/offset_manager.rb#209
  def seconds_since_last_commit; end
end

# The default broker setting for offsets.retention.minutes is 1440.
#
# source://ruby-kafka//lib/kafka/offset_manager.rb#10
Kafka::OffsetManager::DEFAULT_RETENTION_TIME = T.let(T.unsafe(nil), Integer)

# 12
# If you specify a string larger than configured maximum for offset metadata.
#
# source://ruby-kafka//lib/kafka.rb#98
class Kafka::OffsetMetadataTooLarge < ::Kafka::ProtocolError; end

# 1
# The requested offset is not within the range of offsets maintained by the server.
#
# source://ruby-kafka//lib/kafka.rb#35
class Kafka::OffsetOutOfRange < ::Kafka::ProtocolError
  # Returns the value of attribute offset.
  #
  # source://ruby-kafka//lib/kafka.rb#36
  def offset; end

  # Sets the attribute offset
  #
  # @param value the value to set the attribute offset to.
  #
  # source://ruby-kafka//lib/kafka.rb#36
  def offset=(_arg0); end

  # Returns the value of attribute partition.
  #
  # source://ruby-kafka//lib/kafka.rb#36
  def partition; end

  # Sets the attribute partition
  #
  # @param value the value to set the attribute partition to.
  #
  # source://ruby-kafka//lib/kafka.rb#36
  def partition=(_arg0); end

  # Returns the value of attribute topic.
  #
  # source://ruby-kafka//lib/kafka.rb#36
  def topic; end

  # Sets the attribute topic
  #
  # @param value the value to set the attribute topic to.
  #
  # source://ruby-kafka//lib/kafka.rb#36
  def topic=(_arg0); end
end

# 45
# The broker received an out of order sequence number
#
# source://ruby-kafka//lib/kafka.rb#260
class Kafka::OutOfOrderSequenceNumberError < ::Kafka::Error; end

# Assigns partitions to messages.
#
# source://ruby-kafka//lib/kafka/partitioner.rb#8
class Kafka::Partitioner
  # @param hash_function [Symbol, nil] the algorithm used to compute a messages
  #   destination partition. Default is :crc32
  # @return [Partitioner] a new instance of Partitioner
  #
  # source://ruby-kafka//lib/kafka/partitioner.rb#11
  def initialize(hash_function: T.unsafe(nil)); end

  # Assigns a partition number based on a partition key. If no explicit
  # partition key is provided, the message key will be used instead.
  #
  # If the key is nil, then a random partition is selected. Otherwise, a digest
  # of the key is used to deterministically find a partition. As long as the
  # number of partitions doesn't change, the same key will always be assigned
  # to the same partition.
  #
  # @param partition_count [Integer] the number of partitions in the topic.
  # @param message [Kafka::PendingMessage] the message that should be assigned
  #   a partition.
  # @raise [ArgumentError]
  # @return [Integer] the partition number.
  #
  # source://ruby-kafka//lib/kafka/partitioner.rb#27
  def call(partition_count, message); end
end

# Manages the pause state of a partition.
#
# The processing of messages in a partition can be paused, e.g. if there was
# an exception during processing. This could be caused by a downstream service
# not being available. A typical way of solving such an issue is to back off
# for a little while and then try again. In order to do that, _pause_ the
# partition.
#
# source://ruby-kafka//lib/kafka/pause.rb#11
class Kafka::Pause
  # @return [Pause] a new instance of Pause
  #
  # source://ruby-kafka//lib/kafka/pause.rb#12
  def initialize(clock: T.unsafe(nil)); end

  # Whether the pause has expired.
  #
  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/pause.rb#66
  def expired?; end

  # Mark the partition as paused.
  #
  # If exponential backoff is enabled, each subsequent pause of a partition will
  # cause a doubling of the actual timeout, i.e. for pause number _n_, the actual
  # timeout will be _2^n * timeout_.
  #
  # Only when {#reset!} is called is this state cleared.
  #
  # @param timeout [nil, Integer] if specified, the partition will automatically
  #   resume after this many seconds.
  # @param exponential_backoff [Boolean] whether to enable exponential timeouts.
  #
  # source://ruby-kafka//lib/kafka/pause.rb#32
  def pause!(timeout: T.unsafe(nil), max_timeout: T.unsafe(nil), exponential_backoff: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/pause.rb#57
  def pause_duration; end

  # Whether the partition is currently paused. The pause may have expired, in which
  # case {#expired?} should be checked as well.
  #
  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/pause.rb#52
  def paused?; end

  # Resets the pause state, ensuring that the next pause is not exponential.
  #
  # source://ruby-kafka//lib/kafka/pause.rb#75
  def reset!; end

  # Resumes the partition.
  #
  # The number of pauses is still retained, and if the partition is paused again
  # it may be with an exponential backoff.
  #
  # source://ruby-kafka//lib/kafka/pause.rb#44
  def resume!; end

  private

  # source://ruby-kafka//lib/kafka/pause.rb#81
  def ends_at; end
end

# source://ruby-kafka//lib/kafka/pending_message.rb#4
class Kafka::PendingMessage
  # @return [PendingMessage] a new instance of PendingMessage
  #
  # source://ruby-kafka//lib/kafka/pending_message.rb#7
  def initialize(value:, key:, topic:, partition:, partition_key:, create_time:, headers: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/pending_message.rb#18
  def ==(other); end

  # Returns the value of attribute bytesize.
  #
  # source://ruby-kafka//lib/kafka/pending_message.rb#5
  def bytesize; end

  # Returns the value of attribute create_time.
  #
  # source://ruby-kafka//lib/kafka/pending_message.rb#5
  def create_time; end

  # Returns the value of attribute headers.
  #
  # source://ruby-kafka//lib/kafka/pending_message.rb#5
  def headers; end

  # Returns the value of attribute key.
  #
  # source://ruby-kafka//lib/kafka/pending_message.rb#5
  def key; end

  # Returns the value of attribute partition.
  #
  # source://ruby-kafka//lib/kafka/pending_message.rb#5
  def partition; end

  # Returns the value of attribute partition_key.
  #
  # source://ruby-kafka//lib/kafka/pending_message.rb#5
  def partition_key; end

  # Returns the value of attribute topic.
  #
  # source://ruby-kafka//lib/kafka/pending_message.rb#5
  def topic; end

  # Returns the value of attribute value.
  #
  # source://ruby-kafka//lib/kafka/pending_message.rb#5
  def value; end
end

# source://ruby-kafka//lib/kafka/pending_message_queue.rb#5
class Kafka::PendingMessageQueue
  # @return [PendingMessageQueue] a new instance of PendingMessageQueue
  #
  # source://ruby-kafka//lib/kafka/pending_message_queue.rb#8
  def initialize; end

  # Returns the value of attribute bytesize.
  #
  # source://ruby-kafka//lib/kafka/pending_message_queue.rb#6
  def bytesize; end

  # source://ruby-kafka//lib/kafka/pending_message_queue.rb#22
  def clear; end

  # Yields each message in the queue.
  #
  # @return [nil]
  # @yieldparam message [PendingMessage]
  #
  # source://ruby-kafka//lib/kafka/pending_message_queue.rb#37
  def each(&block); end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/pending_message_queue.rb#18
  def empty?; end

  # source://ruby-kafka//lib/kafka/pending_message_queue.rb#28
  def replace(messages); end

  # Returns the value of attribute size.
  #
  # source://ruby-kafka//lib/kafka/pending_message_queue.rb#6
  def size; end

  # source://ruby-kafka//lib/kafka/pending_message_queue.rb#12
  def write(message); end
end

# 44
# Request parameters do not satisfy the configured policy.
#
# source://ruby-kafka//lib/kafka.rb#255
class Kafka::PolicyViolation < ::Kafka::ProtocolError; end

# There was an error processing a message.
#
# source://ruby-kafka//lib/kafka.rb#10
class Kafka::ProcessingError < ::Kafka::Error
  # @return [ProcessingError] a new instance of ProcessingError
  #
  # source://ruby-kafka//lib/kafka.rb#13
  def initialize(topic, partition, offset); end

  # Returns the value of attribute offset.
  #
  # source://ruby-kafka//lib/kafka.rb#11
  def offset; end

  # Returns the value of attribute partition.
  #
  # source://ruby-kafka//lib/kafka.rb#11
  def partition; end

  # Returns the value of attribute topic.
  #
  # source://ruby-kafka//lib/kafka.rb#11
  def topic; end
end

# A produce operation attempts to send all messages in a buffer to the Kafka cluster.
# Since topics and partitions are spread among all brokers in a cluster, this usually
# involves sending requests to several or all of the brokers.
#
# ## Instrumentation
#
# When executing the operation, an `ack_message.producer.kafka` notification will be
# emitted for each message that was successfully appended to a topic partition.
# The following keys will be found in the payload:
#
# * `:topic` — the topic that was written to.
# * `:partition` — the partition that the message set was appended to.
# * `:offset` — the offset of the message in the partition.
# * `:key` — the message key.
# * `:value` — the message value.
# * `:delay` — the time between the message was produced and when it was acknowledged.
#
# In addition to these notifications, a `send_messages.producer.kafka` notification will
# be emitted after the operation completes, regardless of whether it succeeds. This
# notification will have the following keys:
#
# * `:message_count` – the total number of messages that the operation tried to
#   send. Note that not all messages may get delivered.
# * `:sent_message_count` – the number of messages that were successfully sent.
#
# source://ruby-kafka//lib/kafka/produce_operation.rb#32
class Kafka::ProduceOperation
  # @return [ProduceOperation] a new instance of ProduceOperation
  #
  # source://ruby-kafka//lib/kafka/produce_operation.rb#33
  def initialize(cluster:, transaction_manager:, buffer:, compressor:, required_acks:, ack_timeout:, logger:, instrumenter:); end

  # source://ruby-kafka//lib/kafka/produce_operation.rb#44
  def execute; end

  private

  # source://ruby-kafka//lib/kafka/produce_operation.rb#143
  def handle_response(broker, response, records_for_topics); end

  # source://ruby-kafka//lib/kafka/produce_operation.rb#71
  def send_buffered_messages; end
end

# Allows sending messages to a Kafka cluster.
#
# Typically you won't instantiate this class yourself, but rather have {Kafka::Client}
# do it for you, e.g.
#
#     # Will instantiate Kafka::Client
#     kafka = Kafka.new(["kafka1:9092", "kafka2:9092"])
#
#     # Will instantiate Kafka::Producer
#     producer = kafka.producer
#
# This is done in order to share a logger as well as a pool of broker connections across
# different producers. This also means that you don't need to pass the `cluster` and
# `logger` options to `#producer`. See {#initialize} for the list of other options
# you can pass in.
#
# ## Buffering
#
# The producer buffers pending messages until {#deliver_messages} is called. Note that there is
# a maximum buffer size (default is 1,000 messages) and writing messages after the
# buffer has reached this size will result in a BufferOverflow exception. Make sure
# to periodically call {#deliver_messages} or set `max_buffer_size` to an appropriate value.
#
# Buffering messages and sending them in batches greatly improves performance, so
# try to avoid sending messages after every write. The tradeoff between throughput and
# message delays depends on your use case.
#
# ## Error Handling and Retries
#
# The design of the error handling is based on having a {MessageBuffer} hold messages
# for all topics/partitions. Whenever we want to send messages to the cluster, we
# group the buffered messages by the broker they need to be sent to and fire off a
# request to each broker. A request can be a partial success, so we go through the
# response and inspect the error code for each partition that we wrote to. If the
# write to a given partition was successful, we clear the corresponding messages
# from the buffer -- otherwise, we log the error and keep the messages in the buffer.
#
# After this, we check if the buffer is empty. If it is, we're all done. If it's
# not, we do another round of requests, this time with just the remaining messages.
# We do this for as long as `max_retries` permits.
#
# ## Compression
#
# Depending on what kind of data you produce, enabling compression may yield improved
# bandwidth and space usage. Compression in Kafka is done on entire messages sets
# rather than on individual messages. This improves the compression rate and generally
# means that compressions works better the larger your buffers get, since the message
# sets will be larger by the time they're compressed.
#
# Since many workloads have variations in throughput and distribution across partitions,
# it's possible to configure a threshold for when to enable compression by setting
# `compression_threshold`. Only if the defined number of messages are buffered for a
# partition will the messages be compressed.
#
# Compression is enabled by passing the `compression_codec` parameter with the
# name of one of the algorithms allowed by Kafka:
#
# * `:snappy` for [Snappy](http://google.github.io/snappy/) compression.
# * `:gzip` for [gzip](https://en.wikipedia.org/wiki/Gzip) compression.
# * `:lz4` for [LZ4](https://en.wikipedia.org/wiki/LZ4_(compression_algorithm)) compression.
# * `:zstd` for [zstd](https://facebook.github.io/zstd/) compression.
#
# By default, all message sets will be compressed if you specify a compression
# codec. To increase the compression threshold, set `compression_threshold` to
# an integer value higher than one.
#
# ## Instrumentation
#
# Whenever {#produce} is called, the notification `produce_message.producer.kafka`
# will be emitted with the following payload:
#
# * `value` – the message value.
# * `key` – the message key.
# * `topic` – the topic that was produced to.
# * `buffer_size` – the buffer size after adding the message.
# * `max_buffer_size` – the maximum allowed buffer size for the producer.
#
# After {#deliver_messages} completes, the notification
# `deliver_messages.producer.kafka` will be emitted with the following payload:
#
# * `message_count` – the total number of messages that the producer tried to
#   deliver. Note that not all messages may get delivered.
# * `delivered_message_count` – the number of messages that were successfully
#   delivered.
# * `attempts` – the number of attempts made to deliver the messages.
#
# ## Example
#
# This is an example of an application which reads lines from stdin and writes them
# to Kafka:
#
#     require "kafka"
#
#     logger = Logger.new($stderr)
#     brokers = ENV.fetch("KAFKA_BROKERS").split(",")
#
#     # Make sure to create this topic in your Kafka cluster or configure the
#     # cluster to auto-create topics.
#     topic = "random-messages"
#
#     kafka = Kafka.new(brokers, client_id: "simple-producer", logger: logger)
#     producer = kafka.producer
#
#     begin
#       $stdin.each_with_index do |line, index|
#         producer.produce(line, topic: topic)
#
#         # Send messages for every 10 lines.
#         producer.deliver_messages if index % 10 == 0
#       end
#     ensure
#       # Make sure to send any remaining messages.
#       producer.deliver_messages
#
#       producer.shutdown
#     end
#
# source://ruby-kafka//lib/kafka/producer.rb#130
class Kafka::Producer
  # @return [Producer] a new instance of Producer
  #
  # source://ruby-kafka//lib/kafka/producer.rb#133
  def initialize(cluster:, transaction_manager:, logger:, instrumenter:, compressor:, ack_timeout:, required_acks:, max_retries:, retry_backoff:, max_buffer_size:, max_buffer_bytesize:, partitioner:, interceptors: T.unsafe(nil)); end

  # This method abort the pending transaction, marks all the produced
  # records aborted. All the records will be wiped out by the brokers and the
  # cosumers don't have a chance to consume those messages, except they enable
  # consuming uncommitted option.
  #
  # This method can only be called if and only if the current transaction
  # is at IN_TRANSACTION state.
  #
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/producer.rb#337
  def abort_transaction; end

  # Mark the beginning of a transaction. This method transitions the state
  # of the transaction trantiions to IN_TRANSACTION.
  #
  # All producing operations can only be executed while the transation is
  # in this state. The records are persisted by Kafka brokers, but not visible
  # the consumers until the #commit_transaction method is trigger. After a
  # timeout period without committed, the transaction is timeout and
  # considered as aborted.
  #
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/producer.rb#313
  def begin_transaction; end

  # source://ruby-kafka//lib/kafka/producer.rb#275
  def buffer_bytesize; end

  # Returns the number of messages currently held in the buffer.
  #
  # @return [Integer] buffer size.
  #
  # source://ruby-kafka//lib/kafka/producer.rb#271
  def buffer_size; end

  # Deletes all buffered messages.
  #
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/producer.rb#282
  def clear_buffer; end

  # This method commits the pending transaction, marks all the produced
  # records committed. After that, they are visible to the consumers.
  #
  # This method can only be called if and only if the current transaction
  # is at IN_TRANSACTION state.
  #
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/producer.rb#324
  def commit_transaction; end

  # Sends all buffered messages to the Kafka brokers.
  #
  # Depending on the value of `required_acks` used when initializing the producer,
  # this call may block until the specified number of replicas have acknowledged
  # the writes. The `ack_timeout` setting places an upper bound on the amount of
  # time the call will block before failing.
  #
  # @raise [DeliveryFailed] if not all messages could be successfully sent.
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/producer.rb#250
  def deliver_messages; end

  # Initializes the producer to ready for future transactions. This method
  # should be triggered once, before any tranactions are created.
  #
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/producer.rb#299
  def init_transactions; end

  # Produces a message to the specified topic. Note that messages are buffered in
  # the producer until {#deliver_messages} is called.
  #
  # ## Partitioning
  #
  # There are several options for specifying the partition that the message should
  # be written to.
  #
  # The simplest option is to not specify a message key, partition key, or
  # partition number, in which case the message will be assigned a partition at
  # random.
  #
  # You can also specify the `partition` parameter yourself. This requires you to
  # know which partitions are available, however. Oftentimes the best option is
  # to specify the `partition_key` parameter: messages with the same partition
  # key will always be assigned to the same partition, as long as the number of
  # partitions doesn't change. You can also omit the partition key and specify
  # a message key instead. The message key is part of the message payload, and
  # so can carry semantic value--whether you want to have the message key double
  # as a partition key is up to you.
  #
  # @param value [String] the message data.
  # @param key [String] the message key.
  # @param headers [Hash<String, String>] the headers for the message.
  # @param topic [String] the topic that the message should be written to.
  # @param partition [Integer] the partition that the message should be written to.
  # @param partition_key [String] the key that should be used to assign a partition.
  # @param create_time [Time] the timestamp that should be set on the message.
  # @raise [BufferOverflow] if the maximum buffer size has been reached.
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/producer.rb#195
  def produce(value, topic:, key: T.unsafe(nil), headers: T.unsafe(nil), partition: T.unsafe(nil), partition_key: T.unsafe(nil), create_time: T.unsafe(nil)); end

  # Sends batch last offset to the consumer group coordinator, and also marks
  # this offset as part of the current transaction. This offset will be considered
  # committed only if the transaction is committed successfully.
  #
  # This method should be used when you need to batch consumed and produced messages
  # together, typically in a consume-transform-produce pattern. Thus, the specified
  # group_id should be the same as config parameter group_id of the used
  # consumer.
  #
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/producer.rb#351
  def send_offsets_to_transaction(batch:, group_id:); end

  # Closes all connections to the brokers.
  #
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/producer.rb#290
  def shutdown; end

  # source://ruby-kafka//lib/kafka/producer.rb#160
  def to_s; end

  # Syntactic sugar to enable easier transaction usage. Do the following steps
  #
  # - Start the transaction (with Producer#begin_transaction)
  # - Yield the given block
  # - Commit the transaction (with Producer#commit_transaction)
  #
  # If the block raises exception, the transaction is automatically aborted
  # *before* bubble up the exception.
  #
  # If the block raises Kafka::Producer::AbortTransaction indicator exception,
  # it aborts the transaction silently, without throwing up that exception.
  #
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/producer.rb#368
  def transaction; end

  private

  # source://ruby-kafka//lib/kafka/producer.rb#446
  def assign_partitions!; end

  # source://ruby-kafka//lib/kafka/producer.rb#496
  def buffer_messages; end

  # @raise [BufferOverflow]
  #
  # source://ruby-kafka//lib/kafka/producer.rb#520
  def buffer_overflow(topic, message); end

  # source://ruby-kafka//lib/kafka/producer.rb#382
  def deliver_messages_with_retries(notification); end

  # source://ruby-kafka//lib/kafka/producer.rb#442
  def pretty_partitions; end
end

# source://ruby-kafka//lib/kafka/producer.rb#131
class Kafka::Producer::AbortTransaction < ::StandardError; end

# The protocol layer of the library.
#
# The Kafka protocol (https://kafka.apache.org/protocol) defines a set of API
# requests, each with a well-known numeric API key, as well as a set of error
# codes with specific meanings.
#
# This module, and the classes contained in it, implement the client side of
# the protocol.
#
# source://ruby-kafka//lib/kafka/protocol/request_message.rb#4
module Kafka::Protocol
  class << self
    # Returns the symbolic name for an API key.
    #
    # @param api_key Integer
    # @return [Symbol]
    #
    # source://ruby-kafka//lib/kafka/protocol.rb#170
    def api_name(api_key); end

    # Handles an error code by either doing nothing (if there was no error) or
    # by raising an appropriate exception.
    #
    # @param error_code Integer
    # @raise [ProtocolError]
    # @return [nil]
    #
    # source://ruby-kafka//lib/kafka/protocol.rb#156
    def handle_error(error_code, error_message = T.unsafe(nil)); end
  end
end

# source://ruby-kafka//lib/kafka/protocol.rb#36
Kafka::Protocol::ADD_OFFSETS_TO_TXN_API = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol.rb#35
Kafka::Protocol::ADD_PARTITIONS_TO_TXN_API = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol.rb#40
Kafka::Protocol::ALTER_CONFIGS_API = T.let(T.unsafe(nil), Integer)

# A mapping from numeric API keys to symbolic API names.
#
# source://ruby-kafka//lib/kafka/protocol.rb#44
Kafka::Protocol::APIS = T.let(T.unsafe(nil), Hash)

# source://ruby-kafka//lib/kafka/protocol.rb#31
Kafka::Protocol::API_VERSIONS_API = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol/add_offsets_to_txn_request.rb#5
class Kafka::Protocol::AddOffsetsToTxnRequest
  # @return [AddOffsetsToTxnRequest] a new instance of AddOffsetsToTxnRequest
  #
  # source://ruby-kafka//lib/kafka/protocol/add_offsets_to_txn_request.rb#6
  def initialize(producer_id:, producer_epoch:, group_id:, transactional_id: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/protocol/add_offsets_to_txn_request.rb#13
  def api_key; end

  # source://ruby-kafka//lib/kafka/protocol/add_offsets_to_txn_request.rb#21
  def encode(encoder); end

  # source://ruby-kafka//lib/kafka/protocol/add_offsets_to_txn_request.rb#17
  def response_class; end
end

# source://ruby-kafka//lib/kafka/protocol/add_offsets_to_txn_response.rb#5
class Kafka::Protocol::AddOffsetsToTxnResponse
  # @return [AddOffsetsToTxnResponse] a new instance of AddOffsetsToTxnResponse
  #
  # source://ruby-kafka//lib/kafka/protocol/add_offsets_to_txn_response.rb#9
  def initialize(error_code:); end

  # Returns the value of attribute error_code.
  #
  # source://ruby-kafka//lib/kafka/protocol/add_offsets_to_txn_response.rb#7
  def error_code; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/add_offsets_to_txn_response.rb#13
    def decode(decoder); end
  end
end

# source://ruby-kafka//lib/kafka/protocol/add_partitions_to_txn_request.rb#5
class Kafka::Protocol::AddPartitionsToTxnRequest
  # @return [AddPartitionsToTxnRequest] a new instance of AddPartitionsToTxnRequest
  #
  # source://ruby-kafka//lib/kafka/protocol/add_partitions_to_txn_request.rb#6
  def initialize(producer_id:, producer_epoch:, topics:, transactional_id: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/protocol/add_partitions_to_txn_request.rb#13
  def api_key; end

  # source://ruby-kafka//lib/kafka/protocol/add_partitions_to_txn_request.rb#21
  def encode(encoder); end

  # source://ruby-kafka//lib/kafka/protocol/add_partitions_to_txn_request.rb#17
  def response_class; end
end

# source://ruby-kafka//lib/kafka/protocol/add_partitions_to_txn_response.rb#5
class Kafka::Protocol::AddPartitionsToTxnResponse
  # @return [AddPartitionsToTxnResponse] a new instance of AddPartitionsToTxnResponse
  #
  # source://ruby-kafka//lib/kafka/protocol/add_partitions_to_txn_response.rb#26
  def initialize(errors:); end

  # Returns the value of attribute errors.
  #
  # source://ruby-kafka//lib/kafka/protocol/add_partitions_to_txn_response.rb#24
  def errors; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/add_partitions_to_txn_response.rb#30
    def decode(decoder); end
  end
end

# source://ruby-kafka//lib/kafka/protocol/add_partitions_to_txn_response.rb#6
class Kafka::Protocol::AddPartitionsToTxnResponse::PartitionError
  # @return [PartitionError] a new instance of PartitionError
  #
  # source://ruby-kafka//lib/kafka/protocol/add_partitions_to_txn_response.rb#9
  def initialize(partition:, error_code:); end

  # Returns the value of attribute error_code.
  #
  # source://ruby-kafka//lib/kafka/protocol/add_partitions_to_txn_response.rb#7
  def error_code; end

  # Returns the value of attribute partition.
  #
  # source://ruby-kafka//lib/kafka/protocol/add_partitions_to_txn_response.rb#7
  def partition; end
end

# source://ruby-kafka//lib/kafka/protocol/add_partitions_to_txn_response.rb#15
class Kafka::Protocol::AddPartitionsToTxnResponse::TopicPartitionsError
  # @return [TopicPartitionsError] a new instance of TopicPartitionsError
  #
  # source://ruby-kafka//lib/kafka/protocol/add_partitions_to_txn_response.rb#18
  def initialize(topic:, partitions:); end

  # Returns the value of attribute partitions.
  #
  # source://ruby-kafka//lib/kafka/protocol/add_partitions_to_txn_response.rb#16
  def partitions; end

  # Returns the value of attribute topic.
  #
  # source://ruby-kafka//lib/kafka/protocol/add_partitions_to_txn_response.rb#16
  def topic; end
end

# source://ruby-kafka//lib/kafka/protocol/alter_configs_request.rb#6
class Kafka::Protocol::AlterConfigsRequest
  # @return [AlterConfigsRequest] a new instance of AlterConfigsRequest
  #
  # source://ruby-kafka//lib/kafka/protocol/alter_configs_request.rb#7
  def initialize(resources:); end

  # source://ruby-kafka//lib/kafka/protocol/alter_configs_request.rb#11
  def api_key; end

  # source://ruby-kafka//lib/kafka/protocol/alter_configs_request.rb#15
  def api_version; end

  # source://ruby-kafka//lib/kafka/protocol/alter_configs_request.rb#23
  def encode(encoder); end

  # source://ruby-kafka//lib/kafka/protocol/alter_configs_request.rb#19
  def response_class; end
end

# source://ruby-kafka//lib/kafka/protocol/alter_configs_response.rb#5
class Kafka::Protocol::AlterConfigsResponse
  # @return [AlterConfigsResponse] a new instance of AlterConfigsResponse
  #
  # source://ruby-kafka//lib/kafka/protocol/alter_configs_response.rb#19
  def initialize(throttle_time_ms:, resources:); end

  # Returns the value of attribute resources.
  #
  # source://ruby-kafka//lib/kafka/protocol/alter_configs_response.rb#17
  def resources; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/alter_configs_response.rb#24
    def decode(decoder); end
  end
end

# source://ruby-kafka//lib/kafka/protocol/alter_configs_response.rb#6
class Kafka::Protocol::AlterConfigsResponse::ResourceDescription
  # @return [ResourceDescription] a new instance of ResourceDescription
  #
  # source://ruby-kafka//lib/kafka/protocol/alter_configs_response.rb#9
  def initialize(name:, type:, error_code:, error_message:); end

  # Returns the value of attribute error_code.
  #
  # source://ruby-kafka//lib/kafka/protocol/alter_configs_response.rb#7
  def error_code; end

  # Returns the value of attribute error_message.
  #
  # source://ruby-kafka//lib/kafka/protocol/alter_configs_response.rb#7
  def error_message; end

  # Returns the value of attribute name.
  #
  # source://ruby-kafka//lib/kafka/protocol/alter_configs_response.rb#7
  def name; end

  # Returns the value of attribute type.
  #
  # source://ruby-kafka//lib/kafka/protocol/alter_configs_response.rb#7
  def type; end
end

# source://ruby-kafka//lib/kafka/protocol/api_versions_request.rb#6
class Kafka::Protocol::ApiVersionsRequest
  # source://ruby-kafka//lib/kafka/protocol/api_versions_request.rb#7
  def api_key; end

  # source://ruby-kafka//lib/kafka/protocol/api_versions_request.rb#11
  def encode(encoder); end

  # source://ruby-kafka//lib/kafka/protocol/api_versions_request.rb#15
  def response_class; end
end

# source://ruby-kafka//lib/kafka/protocol/api_versions_response.rb#6
class Kafka::Protocol::ApiVersionsResponse
  # @return [ApiVersionsResponse] a new instance of ApiVersionsResponse
  #
  # source://ruby-kafka//lib/kafka/protocol/api_versions_response.rb#33
  def initialize(error_code:, apis:); end

  # Returns the value of attribute apis.
  #
  # source://ruby-kafka//lib/kafka/protocol/api_versions_response.rb#31
  def apis; end

  # Returns the value of attribute error_code.
  #
  # source://ruby-kafka//lib/kafka/protocol/api_versions_response.rb#31
  def error_code; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/api_versions_response.rb#38
    def decode(decoder); end
  end
end

# source://ruby-kafka//lib/kafka/protocol/api_versions_response.rb#7
class Kafka::Protocol::ApiVersionsResponse::ApiInfo
  # @return [ApiInfo] a new instance of ApiInfo
  #
  # source://ruby-kafka//lib/kafka/protocol/api_versions_response.rb#10
  def initialize(api_key:, min_version:, max_version:); end

  # Returns the value of attribute api_key.
  #
  # source://ruby-kafka//lib/kafka/protocol/api_versions_response.rb#8
  def api_key; end

  # source://ruby-kafka//lib/kafka/protocol/api_versions_response.rb#14
  def api_name; end

  # source://ruby-kafka//lib/kafka/protocol/api_versions_response.rb#26
  def inspect; end

  # Returns the value of attribute max_version.
  #
  # source://ruby-kafka//lib/kafka/protocol/api_versions_response.rb#8
  def max_version; end

  # Returns the value of attribute min_version.
  #
  # source://ruby-kafka//lib/kafka/protocol/api_versions_response.rb#8
  def min_version; end

  # source://ruby-kafka//lib/kafka/protocol/api_versions_response.rb#22
  def to_s; end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/protocol/api_versions_response.rb#18
  def version_supported?(version); end
end

# Coordinator types. Since Kafka 0.11.0, there are types of coordinators:
# Group and Transaction
#
# source://ruby-kafka//lib/kafka/protocol.rb#147
Kafka::Protocol::COORDINATOR_TYPE_GROUP = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol.rb#148
Kafka::Protocol::COORDINATOR_TYPE_TRANSACTION = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol.rb#41
Kafka::Protocol::CREATE_PARTITIONS_API = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol.rb#32
Kafka::Protocol::CREATE_TOPICS_API = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol/consumer_group_protocol.rb#5
class Kafka::Protocol::ConsumerGroupProtocol
  # @return [ConsumerGroupProtocol] a new instance of ConsumerGroupProtocol
  #
  # source://ruby-kafka//lib/kafka/protocol/consumer_group_protocol.rb#6
  def initialize(topics:, version: T.unsafe(nil), user_data: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/protocol/consumer_group_protocol.rb#12
  def encode(encoder); end
end

# source://ruby-kafka//lib/kafka/protocol/create_partitions_request.rb#6
class Kafka::Protocol::CreatePartitionsRequest
  # @return [CreatePartitionsRequest] a new instance of CreatePartitionsRequest
  #
  # source://ruby-kafka//lib/kafka/protocol/create_partitions_request.rb#7
  def initialize(topics:, timeout:); end

  # source://ruby-kafka//lib/kafka/protocol/create_partitions_request.rb#11
  def api_key; end

  # source://ruby-kafka//lib/kafka/protocol/create_partitions_request.rb#15
  def api_version; end

  # source://ruby-kafka//lib/kafka/protocol/create_partitions_request.rb#23
  def encode(encoder); end

  # source://ruby-kafka//lib/kafka/protocol/create_partitions_request.rb#19
  def response_class; end
end

# source://ruby-kafka//lib/kafka/protocol/create_partitions_response.rb#6
class Kafka::Protocol::CreatePartitionsResponse
  # @return [CreatePartitionsResponse] a new instance of CreatePartitionsResponse
  #
  # source://ruby-kafka//lib/kafka/protocol/create_partitions_response.rb#9
  def initialize(throttle_time_ms:, errors:); end

  # Returns the value of attribute errors.
  #
  # source://ruby-kafka//lib/kafka/protocol/create_partitions_response.rb#7
  def errors; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/create_partitions_response.rb#14
    def decode(decoder); end
  end
end

# source://ruby-kafka//lib/kafka/protocol/create_topics_request.rb#6
class Kafka::Protocol::CreateTopicsRequest
  # @return [CreateTopicsRequest] a new instance of CreateTopicsRequest
  #
  # source://ruby-kafka//lib/kafka/protocol/create_topics_request.rb#7
  def initialize(topics:, timeout:); end

  # source://ruby-kafka//lib/kafka/protocol/create_topics_request.rb#11
  def api_key; end

  # source://ruby-kafka//lib/kafka/protocol/create_topics_request.rb#15
  def api_version; end

  # source://ruby-kafka//lib/kafka/protocol/create_topics_request.rb#23
  def encode(encoder); end

  # source://ruby-kafka//lib/kafka/protocol/create_topics_request.rb#19
  def response_class; end
end

# source://ruby-kafka//lib/kafka/protocol/create_topics_response.rb#6
class Kafka::Protocol::CreateTopicsResponse
  # @return [CreateTopicsResponse] a new instance of CreateTopicsResponse
  #
  # source://ruby-kafka//lib/kafka/protocol/create_topics_response.rb#9
  def initialize(errors:); end

  # Returns the value of attribute errors.
  #
  # source://ruby-kafka//lib/kafka/protocol/create_topics_response.rb#7
  def errors; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/create_topics_response.rb#13
    def decode(decoder); end
  end
end

# source://ruby-kafka//lib/kafka/protocol.rb#33
Kafka::Protocol::DELETE_TOPICS_API = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol.rb#39
Kafka::Protocol::DESCRIBE_CONFIGS_API = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol.rb#28
Kafka::Protocol::DESCRIBE_GROUPS_API = T.let(T.unsafe(nil), Integer)

# A decoder wraps an IO object, making it easy to read specific data types
# from it. The Kafka protocol is not self-describing, so a client must call
# these methods in just the right order for things to work.
#
# source://ruby-kafka//lib/kafka/protocol/decoder.rb#8
class Kafka::Protocol::Decoder
  # Initializes a new decoder.
  #
  # @param io [IO] an object that acts as an IO.
  # @return [Decoder] a new instance of Decoder
  #
  # source://ruby-kafka//lib/kafka/protocol/decoder.rb#16
  def initialize(io); end

  # Decodes an array from the IO object.
  #
  # The provided block will be called once for each item in the array. It is
  # the responsibility of the block to decode the proper type in the block,
  # since there's no information that allows the type to be inferred
  # automatically.
  #
  # @return [Array]
  #
  # source://ruby-kafka//lib/kafka/protocol/decoder.rb#77
  def array(&block); end

  # Decodes an 8-bit boolean from the IO object.
  #
  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/protocol/decoder.rb#37
  def boolean; end

  # Decodes a list of bytes from the IO object.
  #
  # @return [String]
  #
  # source://ruby-kafka//lib/kafka/protocol/decoder.rb#135
  def bytes; end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/protocol/decoder.rb#20
  def eof?; end

  # Decodes a 16-bit integer from the IO object.
  #
  # @return [Integer]
  #
  # source://ruby-kafka//lib/kafka/protocol/decoder.rb#51
  def int16; end

  # Decodes a 32-bit integer from the IO object.
  #
  # @return [Integer]
  #
  # source://ruby-kafka//lib/kafka/protocol/decoder.rb#58
  def int32; end

  # Decodes a 64-bit integer from the IO object.
  #
  # @return [Integer]
  #
  # source://ruby-kafka//lib/kafka/protocol/decoder.rb#65
  def int64; end

  # Decodes an 8-bit integer from the IO object.
  #
  # @return [Integer]
  #
  # source://ruby-kafka//lib/kafka/protocol/decoder.rb#44
  def int8; end

  # Get some next bytes without touching the current io offset
  #
  # @return [Integer]
  #
  # source://ruby-kafka//lib/kafka/protocol/decoder.rb#27
  def peek(offset, length); end

  # Reads the specified number of bytes from the IO object, returning them
  # as a String.
  #
  # @raise [EOFError]
  # @return [String]
  #
  # source://ruby-kafka//lib/kafka/protocol/decoder.rb#162
  def read(number_of_bytes); end

  # Decodes a string from the IO object.
  #
  # @return [String]
  #
  # source://ruby-kafka//lib/kafka/protocol/decoder.rb#94
  def string; end

  # Read an integer under varints serializing from the IO object.
  # https://developers.google.com/protocol-buffers/docs/encoding#varints
  #
  # @return [Integer]
  #
  # source://ruby-kafka//lib/kafka/protocol/decoder.rb#121
  def varint; end

  # Decodes an array from the IO object.
  # Just like #array except the size is in varint format
  #
  # @return [Array]
  #
  # source://ruby-kafka//lib/kafka/protocol/decoder.rb#86
  def varint_array(&block); end

  # Decodes a list of bytes from the IO object. The size is in varint format
  #
  # @return [String]
  #
  # source://ruby-kafka//lib/kafka/protocol/decoder.rb#148
  def varint_bytes; end

  # Decodes a string from the IO object, the size is in varint format
  #
  # @return [String]
  #
  # source://ruby-kafka//lib/kafka/protocol/decoder.rb#107
  def varint_string; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/decoder.rb#9
    def from_string(str); end
  end
end

# source://ruby-kafka//lib/kafka/protocol/delete_topics_request.rb#6
class Kafka::Protocol::DeleteTopicsRequest
  # @return [DeleteTopicsRequest] a new instance of DeleteTopicsRequest
  #
  # source://ruby-kafka//lib/kafka/protocol/delete_topics_request.rb#7
  def initialize(topics:, timeout:); end

  # source://ruby-kafka//lib/kafka/protocol/delete_topics_request.rb#11
  def api_key; end

  # source://ruby-kafka//lib/kafka/protocol/delete_topics_request.rb#15
  def api_version; end

  # source://ruby-kafka//lib/kafka/protocol/delete_topics_request.rb#23
  def encode(encoder); end

  # source://ruby-kafka//lib/kafka/protocol/delete_topics_request.rb#19
  def response_class; end
end

# source://ruby-kafka//lib/kafka/protocol/delete_topics_response.rb#6
class Kafka::Protocol::DeleteTopicsResponse
  # @return [DeleteTopicsResponse] a new instance of DeleteTopicsResponse
  #
  # source://ruby-kafka//lib/kafka/protocol/delete_topics_response.rb#9
  def initialize(errors:); end

  # Returns the value of attribute errors.
  #
  # source://ruby-kafka//lib/kafka/protocol/delete_topics_response.rb#7
  def errors; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/delete_topics_response.rb#13
    def decode(decoder); end
  end
end

# source://ruby-kafka//lib/kafka/protocol/describe_configs_request.rb#6
class Kafka::Protocol::DescribeConfigsRequest
  # @return [DescribeConfigsRequest] a new instance of DescribeConfigsRequest
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_configs_request.rb#7
  def initialize(resources:); end

  # source://ruby-kafka//lib/kafka/protocol/describe_configs_request.rb#11
  def api_key; end

  # source://ruby-kafka//lib/kafka/protocol/describe_configs_request.rb#15
  def api_version; end

  # source://ruby-kafka//lib/kafka/protocol/describe_configs_request.rb#23
  def encode(encoder); end

  # source://ruby-kafka//lib/kafka/protocol/describe_configs_request.rb#19
  def response_class; end
end

# source://ruby-kafka//lib/kafka/protocol/describe_configs_response.rb#5
class Kafka::Protocol::DescribeConfigsResponse
  # @return [DescribeConfigsResponse] a new instance of DescribeConfigsResponse
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_configs_response.rb#32
  def initialize(throttle_time_ms:, resources:); end

  # Returns the value of attribute resources.
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_configs_response.rb#30
  def resources; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/describe_configs_response.rb#37
    def decode(decoder); end
  end
end

# source://ruby-kafka//lib/kafka/protocol/describe_configs_response.rb#18
class Kafka::Protocol::DescribeConfigsResponse::ConfigEntry
  # @return [ConfigEntry] a new instance of ConfigEntry
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_configs_response.rb#21
  def initialize(name:, value:, read_only:, is_default:, is_sensitive:); end

  # Returns the value of attribute is_default.
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_configs_response.rb#19
  def is_default; end

  # Returns the value of attribute is_sensitive.
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_configs_response.rb#19
  def is_sensitive; end

  # Returns the value of attribute name.
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_configs_response.rb#19
  def name; end

  # Returns the value of attribute read_only.
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_configs_response.rb#19
  def read_only; end

  # Returns the value of attribute value.
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_configs_response.rb#19
  def value; end
end

# source://ruby-kafka//lib/kafka/protocol/describe_configs_response.rb#6
class Kafka::Protocol::DescribeConfigsResponse::ResourceDescription
  # @return [ResourceDescription] a new instance of ResourceDescription
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_configs_response.rb#9
  def initialize(name:, type:, error_code:, error_message:, configs:); end

  # Returns the value of attribute configs.
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_configs_response.rb#7
  def configs; end

  # Returns the value of attribute error_code.
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_configs_response.rb#7
  def error_code; end

  # Returns the value of attribute error_message.
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_configs_response.rb#7
  def error_message; end

  # Returns the value of attribute name.
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_configs_response.rb#7
  def name; end

  # Returns the value of attribute type.
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_configs_response.rb#7
  def type; end
end

# source://ruby-kafka//lib/kafka/protocol/describe_groups_request.rb#5
class Kafka::Protocol::DescribeGroupsRequest
  # @return [DescribeGroupsRequest] a new instance of DescribeGroupsRequest
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_groups_request.rb#6
  def initialize(group_ids:); end

  # source://ruby-kafka//lib/kafka/protocol/describe_groups_request.rb#10
  def api_key; end

  # source://ruby-kafka//lib/kafka/protocol/describe_groups_request.rb#14
  def api_version; end

  # source://ruby-kafka//lib/kafka/protocol/describe_groups_request.rb#22
  def encode(encoder); end

  # source://ruby-kafka//lib/kafka/protocol/describe_groups_request.rb#18
  def response_class; end
end

# source://ruby-kafka//lib/kafka/protocol/describe_groups_response.rb#5
class Kafka::Protocol::DescribeGroupsResponse
  # @return [DescribeGroupsResponse] a new instance of DescribeGroupsResponse
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_groups_response.rb#32
  def initialize(groups:); end

  # Returns the value of attribute error_code.
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_groups_response.rb#30
  def error_code; end

  # Returns the value of attribute groups.
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_groups_response.rb#30
  def groups; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/describe_groups_response.rb#36
    def decode(decoder); end
  end
end

# source://ruby-kafka//lib/kafka/protocol/describe_groups_response.rb#6
class Kafka::Protocol::DescribeGroupsResponse::Group
  # @return [Group] a new instance of Group
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_groups_response.rb#9
  def initialize(error_code:, group_id:, protocol_type:, protocol:, state:, members:); end

  # Returns the value of attribute error_code.
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_groups_response.rb#7
  def error_code; end

  # Returns the value of attribute group_id.
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_groups_response.rb#7
  def group_id; end

  # Returns the value of attribute members.
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_groups_response.rb#7
  def members; end

  # Returns the value of attribute protocol.
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_groups_response.rb#7
  def protocol; end

  # Returns the value of attribute state.
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_groups_response.rb#7
  def state; end
end

# source://ruby-kafka//lib/kafka/protocol/describe_groups_response.rb#19
class Kafka::Protocol::DescribeGroupsResponse::Member
  # @return [Member] a new instance of Member
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_groups_response.rb#22
  def initialize(member_id:, client_id:, client_host:, member_assignment:); end

  # Returns the value of attribute client_host.
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_groups_response.rb#20
  def client_host; end

  # Returns the value of attribute client_id.
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_groups_response.rb#20
  def client_id; end

  # Returns the value of attribute member_assignment.
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_groups_response.rb#20
  def member_assignment; end

  # Returns the value of attribute member_id.
  #
  # source://ruby-kafka//lib/kafka/protocol/describe_groups_response.rb#20
  def member_id; end
end

# source://ruby-kafka//lib/kafka/protocol.rb#37
Kafka::Protocol::END_TXN_API = T.let(T.unsafe(nil), Integer)

# A mapping from numeric error codes to exception classes.
#
# source://ruby-kafka//lib/kafka/protocol.rb#70
Kafka::Protocol::ERRORS = T.let(T.unsafe(nil), Hash)

# An encoder wraps an IO object, making it easy to write specific data types
# to it.
#
# source://ruby-kafka//lib/kafka/protocol/encoder.rb#9
class Kafka::Protocol::Encoder
  # Initializes a new encoder.
  #
  # @param io [IO] an object that acts as an IO.
  # @return [Encoder] a new instance of Encoder
  #
  # source://ruby-kafka//lib/kafka/protocol/encoder.rb#13
  def initialize(io); end

  # Writes bytes directly to the IO object.
  #
  # @param bytes [String]
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/protocol/encoder.rb#22
  def write(bytes); end

  # Writes an array to the IO object.
  #
  # Each item in the specified array will be yielded to the provided block;
  # it's the responsibility of the block to write those items using the
  # encoder.
  #
  # @param array [Array]
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/protocol/encoder.rb#76
  def write_array(array, &block); end

  # Writes an 8-bit boolean to the IO object.
  #
  # @param boolean [Boolean]
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/protocol/encoder.rb#32
  def write_boolean(boolean); end

  # Writes a byte string to the IO object.
  #
  # @param bytes [String]
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/protocol/encoder.rb#148
  def write_bytes(bytes); end

  # Writes a 16-bit integer to the IO object.
  #
  # @param int [Integer]
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/protocol/encoder.rb#48
  def write_int16(int); end

  # Writes a 32-bit integer to the IO object.
  #
  # @param int [Integer]
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/protocol/encoder.rb#56
  def write_int32(int); end

  # Writes a 64-bit integer to the IO object.
  #
  # @param int [Integer]
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/protocol/encoder.rb#64
  def write_int64(int); end

  # Writes an 8-bit integer to the IO object.
  #
  # @param int [Integer]
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/protocol/encoder.rb#40
  def write_int8(int); end

  # Writes a string to the IO object.
  #
  # @param string [String]
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/protocol/encoder.rb#104
  def write_string(string); end

  # Writes an integer under varints serializing to the IO object.
  # https://developers.google.com/protocol-buffers/docs/encoding#varints
  #
  # @param int [Integer]
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/protocol/encoder.rb#131
  def write_varint(int); end

  # Writes an array to the IO object.
  # Just like #write_array, unless the size is under varint format
  #
  # @param array [Array]
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/protocol/encoder.rb#91
  def write_varint_array(array, &block); end

  # Writes a byte string to the IO object, the size is under varint format
  #
  # @param bytes [String]
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/protocol/encoder.rb#161
  def write_varint_bytes(bytes); end

  # Writes a string to the IO object, the size is under varint format
  #
  # @param string [String]
  # @return [nil]
  #
  # source://ruby-kafka//lib/kafka/protocol/encoder.rb#117
  def write_varint_string(string); end

  class << self
    # Encodes an object into a new buffer.
    #
    # @param object [#encode] the object that will encode itself.
    # @return [String] the encoded data.
    #
    # source://ruby-kafka//lib/kafka/protocol/encoder.rb#174
    def encode_with(object); end
  end
end

# source://ruby-kafka//lib/kafka/protocol/end_txn_request.rb#5
class Kafka::Protocol::EndTxnRequest
  # @return [EndTxnRequest] a new instance of EndTxnRequest
  #
  # source://ruby-kafka//lib/kafka/protocol/end_txn_request.rb#6
  def initialize(transactional_id:, producer_id:, producer_epoch:, transaction_result:); end

  # source://ruby-kafka//lib/kafka/protocol/end_txn_request.rb#13
  def api_key; end

  # source://ruby-kafka//lib/kafka/protocol/end_txn_request.rb#21
  def encode(encoder); end

  # source://ruby-kafka//lib/kafka/protocol/end_txn_request.rb#17
  def response_class; end
end

# source://ruby-kafka//lib/kafka/protocol/end_txn_response.rb#5
class Kafka::Protocol::EndTxnResposne
  # @return [EndTxnResposne] a new instance of EndTxnResposne
  #
  # source://ruby-kafka//lib/kafka/protocol/end_txn_response.rb#8
  def initialize(error_code:); end

  # Returns the value of attribute error_code.
  #
  # source://ruby-kafka//lib/kafka/protocol/end_txn_response.rb#6
  def error_code; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/end_txn_response.rb#12
    def decode(decoder); end
  end
end

# source://ruby-kafka//lib/kafka/protocol.rb#18
Kafka::Protocol::FETCH_API = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol.rb#23
Kafka::Protocol::FIND_COORDINATOR_API = T.let(T.unsafe(nil), Integer)

# A request to fetch messages from a given partition.
#
# ## API Specification
#
#     FetchRequest => ReplicaId MaxWaitTime MinBytes MaxBytes IsolationLevel [TopicName [Partition FetchOffset MaxBytes]]
#       ReplicaId => int32
#       MaxWaitTime => int32
#       MinBytes => int32
#       MaxBytes => int32
#       IsolationLevel => int8
#       TopicName => string
#       Partition => int32
#       FetchOffset => int64
#       MaxBytes => int32
#
# source://ruby-kafka//lib/kafka/protocol/fetch_request.rb#21
class Kafka::Protocol::FetchRequest
  # @param max_wait_time [Integer]
  # @param min_bytes [Integer]
  # @param topics [Hash]
  # @return [FetchRequest] a new instance of FetchRequest
  #
  # source://ruby-kafka//lib/kafka/protocol/fetch_request.rb#28
  def initialize(max_wait_time:, min_bytes:, max_bytes:, topics:); end

  # source://ruby-kafka//lib/kafka/protocol/fetch_request.rb#36
  def api_key; end

  # source://ruby-kafka//lib/kafka/protocol/fetch_request.rb#40
  def api_version; end

  # source://ruby-kafka//lib/kafka/protocol/fetch_request.rb#48
  def encode(encoder); end

  # source://ruby-kafka//lib/kafka/protocol/fetch_request.rb#44
  def response_class; end
end

# source://ruby-kafka//lib/kafka/protocol/fetch_request.rb#23
Kafka::Protocol::FetchRequest::ISOLATION_READ_COMMITTED = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol/fetch_request.rb#22
Kafka::Protocol::FetchRequest::ISOLATION_READ_UNCOMMITTED = T.let(T.unsafe(nil), Integer)

# A response to a fetch request.
#
# ## API Specification
#
#     FetchResponse => ThrottleTimeMS [TopicName [Partition ErrorCode HighwaterMarkOffset LastStableOffset [AbortedTransaction] Records]]
#       ThrottleTimeMS => int32
#       TopicName => string
#       Partition => int32
#       ErrorCode => int16
#       HighwaterMarkOffset => int64
#       LastStableOffset => int64
#       MessageSetSize => int32
#       AbortedTransaction => [
#             ProducerId => int64
#             FirstOffset => int64
#       ]
#
# source://ruby-kafka//lib/kafka/protocol/fetch_response.rb#26
class Kafka::Protocol::FetchResponse
  # @return [FetchResponse] a new instance of FetchResponse
  #
  # source://ruby-kafka//lib/kafka/protocol/fetch_response.rb#64
  def initialize(topics: T.unsafe(nil), throttle_time_ms: T.unsafe(nil)); end

  # Returns the value of attribute topics.
  #
  # source://ruby-kafka//lib/kafka/protocol/fetch_response.rb#62
  def topics; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/fetch_response.rb#69
    def decode(decoder); end
  end
end

# source://ruby-kafka//lib/kafka/protocol/fetch_response.rb#53
class Kafka::Protocol::FetchResponse::AbortedTransaction
  # @return [AbortedTransaction] a new instance of AbortedTransaction
  #
  # source://ruby-kafka//lib/kafka/protocol/fetch_response.rb#56
  def initialize(producer_id:, first_offset:); end

  # Returns the value of attribute first_offset.
  #
  # source://ruby-kafka//lib/kafka/protocol/fetch_response.rb#54
  def first_offset; end

  # Returns the value of attribute producer_id.
  #
  # source://ruby-kafka//lib/kafka/protocol/fetch_response.rb#54
  def producer_id; end
end

# source://ruby-kafka//lib/kafka/protocol/fetch_response.rb#30
class Kafka::Protocol::FetchResponse::FetchedPartition
  # @return [FetchedPartition] a new instance of FetchedPartition
  #
  # source://ruby-kafka//lib/kafka/protocol/fetch_response.rb#34
  def initialize(partition:, error_code:, highwater_mark_offset:, last_stable_offset:, aborted_transactions:, messages:); end

  # Returns the value of attribute aborted_transactions.
  #
  # source://ruby-kafka//lib/kafka/protocol/fetch_response.rb#32
  def aborted_transactions; end

  # Returns the value of attribute error_code.
  #
  # source://ruby-kafka//lib/kafka/protocol/fetch_response.rb#31
  def error_code; end

  # Returns the value of attribute highwater_mark_offset.
  #
  # source://ruby-kafka//lib/kafka/protocol/fetch_response.rb#32
  def highwater_mark_offset; end

  # Returns the value of attribute last_stable_offset.
  #
  # source://ruby-kafka//lib/kafka/protocol/fetch_response.rb#32
  def last_stable_offset; end

  # Returns the value of attribute messages.
  #
  # source://ruby-kafka//lib/kafka/protocol/fetch_response.rb#32
  def messages; end

  # Returns the value of attribute partition.
  #
  # source://ruby-kafka//lib/kafka/protocol/fetch_response.rb#31
  def partition; end
end

# source://ruby-kafka//lib/kafka/protocol/fetch_response.rb#44
class Kafka::Protocol::FetchResponse::FetchedTopic
  # @return [FetchedTopic] a new instance of FetchedTopic
  #
  # source://ruby-kafka//lib/kafka/protocol/fetch_response.rb#47
  def initialize(name:, partitions:); end

  # Returns the value of attribute name.
  #
  # source://ruby-kafka//lib/kafka/protocol/fetch_response.rb#45
  def name; end

  # Returns the value of attribute partitions.
  #
  # source://ruby-kafka//lib/kafka/protocol/fetch_response.rb#45
  def partitions; end
end

# source://ruby-kafka//lib/kafka/protocol/fetch_response.rb#28
Kafka::Protocol::FetchResponse::MAGIC_BYTE_LENGTH = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol/fetch_response.rb#27
Kafka::Protocol::FetchResponse::MAGIC_BYTE_OFFSET = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol/find_coordinator_request.rb#5
class Kafka::Protocol::FindCoordinatorRequest
  # @return [FindCoordinatorRequest] a new instance of FindCoordinatorRequest
  #
  # source://ruby-kafka//lib/kafka/protocol/find_coordinator_request.rb#6
  def initialize(coordinator_key:, coordinator_type:); end

  # source://ruby-kafka//lib/kafka/protocol/find_coordinator_request.rb#11
  def api_key; end

  # source://ruby-kafka//lib/kafka/protocol/find_coordinator_request.rb#15
  def api_version; end

  # source://ruby-kafka//lib/kafka/protocol/find_coordinator_request.rb#19
  def encode(encoder); end

  # source://ruby-kafka//lib/kafka/protocol/find_coordinator_request.rb#24
  def response_class; end
end

# source://ruby-kafka//lib/kafka/protocol/find_coordinator_response.rb#5
class Kafka::Protocol::FindCoordinatorResponse
  # @return [FindCoordinatorResponse] a new instance of FindCoordinatorResponse
  #
  # source://ruby-kafka//lib/kafka/protocol/find_coordinator_response.rb#10
  def initialize(error_code:, error_message:, coordinator_id:, coordinator_host:, coordinator_port:); end

  # Returns the value of attribute coordinator_host.
  #
  # source://ruby-kafka//lib/kafka/protocol/find_coordinator_response.rb#8
  def coordinator_host; end

  # Returns the value of attribute coordinator_id.
  #
  # source://ruby-kafka//lib/kafka/protocol/find_coordinator_response.rb#8
  def coordinator_id; end

  # Returns the value of attribute coordinator_port.
  #
  # source://ruby-kafka//lib/kafka/protocol/find_coordinator_response.rb#8
  def coordinator_port; end

  # Returns the value of attribute error_code.
  #
  # source://ruby-kafka//lib/kafka/protocol/find_coordinator_response.rb#6
  def error_code; end

  # Returns the value of attribute error_message.
  #
  # source://ruby-kafka//lib/kafka/protocol/find_coordinator_response.rb#6
  def error_message; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/find_coordinator_response.rb#17
    def decode(decoder); end
  end
end

# source://ruby-kafka//lib/kafka/protocol.rb#25
Kafka::Protocol::HEARTBEAT_API = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol/heartbeat_request.rb#5
class Kafka::Protocol::HeartbeatRequest
  # @return [HeartbeatRequest] a new instance of HeartbeatRequest
  #
  # source://ruby-kafka//lib/kafka/protocol/heartbeat_request.rb#6
  def initialize(group_id:, generation_id:, member_id:); end

  # source://ruby-kafka//lib/kafka/protocol/heartbeat_request.rb#12
  def api_key; end

  # source://ruby-kafka//lib/kafka/protocol/heartbeat_request.rb#20
  def encode(encoder); end

  # source://ruby-kafka//lib/kafka/protocol/heartbeat_request.rb#16
  def response_class; end
end

# source://ruby-kafka//lib/kafka/protocol/heartbeat_response.rb#5
class Kafka::Protocol::HeartbeatResponse
  # @return [HeartbeatResponse] a new instance of HeartbeatResponse
  #
  # source://ruby-kafka//lib/kafka/protocol/heartbeat_response.rb#8
  def initialize(error_code:); end

  # Returns the value of attribute error_code.
  #
  # source://ruby-kafka//lib/kafka/protocol/heartbeat_response.rb#6
  def error_code; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/heartbeat_response.rb#12
    def decode(decoder); end
  end
end

# source://ruby-kafka//lib/kafka/protocol.rb#34
Kafka::Protocol::INIT_PRODUCER_ID_API = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol/init_producer_id_request.rb#5
class Kafka::Protocol::InitProducerIDRequest
  # @return [InitProducerIDRequest] a new instance of InitProducerIDRequest
  #
  # source://ruby-kafka//lib/kafka/protocol/init_producer_id_request.rb#6
  def initialize(transactional_timeout:, transactional_id: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/protocol/init_producer_id_request.rb#11
  def api_key; end

  # source://ruby-kafka//lib/kafka/protocol/init_producer_id_request.rb#19
  def encode(encoder); end

  # source://ruby-kafka//lib/kafka/protocol/init_producer_id_request.rb#15
  def response_class; end
end

# source://ruby-kafka//lib/kafka/protocol/init_producer_id_response.rb#5
class Kafka::Protocol::InitProducerIDResponse
  # @return [InitProducerIDResponse] a new instance of InitProducerIDResponse
  #
  # source://ruby-kafka//lib/kafka/protocol/init_producer_id_response.rb#8
  def initialize(error_code:, producer_id:, producer_epoch:); end

  # Returns the value of attribute error_code.
  #
  # source://ruby-kafka//lib/kafka/protocol/init_producer_id_response.rb#6
  def error_code; end

  # Returns the value of attribute producer_epoch.
  #
  # source://ruby-kafka//lib/kafka/protocol/init_producer_id_response.rb#6
  def producer_epoch; end

  # Returns the value of attribute producer_id.
  #
  # source://ruby-kafka//lib/kafka/protocol/init_producer_id_response.rb#6
  def producer_id; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/init_producer_id_response.rb#14
    def decode(decoder); end
  end
end

# source://ruby-kafka//lib/kafka/protocol.rb#24
Kafka::Protocol::JOIN_GROUP_API = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol/join_group_request.rb#7
class Kafka::Protocol::JoinGroupRequest
  # @return [JoinGroupRequest] a new instance of JoinGroupRequest
  #
  # source://ruby-kafka//lib/kafka/protocol/join_group_request.rb#10
  def initialize(group_id:, session_timeout:, rebalance_timeout:, member_id:, protocol_name:, topics: T.unsafe(nil), user_data: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/protocol/join_group_request.rb#21
  def api_key; end

  # source://ruby-kafka//lib/kafka/protocol/join_group_request.rb#25
  def api_version; end

  # source://ruby-kafka//lib/kafka/protocol/join_group_request.rb#33
  def encode(encoder); end

  # source://ruby-kafka//lib/kafka/protocol/join_group_request.rb#29
  def response_class; end
end

# source://ruby-kafka//lib/kafka/protocol/join_group_request.rb#8
Kafka::Protocol::JoinGroupRequest::PROTOCOL_TYPE = T.let(T.unsafe(nil), String)

# source://ruby-kafka//lib/kafka/protocol/join_group_response.rb#5
class Kafka::Protocol::JoinGroupResponse
  # @return [JoinGroupResponse] a new instance of JoinGroupResponse
  #
  # source://ruby-kafka//lib/kafka/protocol/join_group_response.rb#14
  def initialize(error_code:, generation_id:, group_protocol:, leader_id:, member_id:, members:); end

  # Returns the value of attribute error_code.
  #
  # source://ruby-kafka//lib/kafka/protocol/join_group_response.rb#8
  def error_code; end

  # Returns the value of attribute generation_id.
  #
  # source://ruby-kafka//lib/kafka/protocol/join_group_response.rb#10
  def generation_id; end

  # Returns the value of attribute group_protocol.
  #
  # source://ruby-kafka//lib/kafka/protocol/join_group_response.rb#10
  def group_protocol; end

  # Returns the value of attribute leader_id.
  #
  # source://ruby-kafka//lib/kafka/protocol/join_group_response.rb#12
  def leader_id; end

  # Returns the value of attribute member_id.
  #
  # source://ruby-kafka//lib/kafka/protocol/join_group_response.rb#12
  def member_id; end

  # Returns the value of attribute members.
  #
  # source://ruby-kafka//lib/kafka/protocol/join_group_response.rb#12
  def members; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/join_group_response.rb#23
    def decode(decoder); end
  end
end

# source://ruby-kafka//lib/kafka/protocol/join_group_response.rb#6
class Kafka::Protocol::JoinGroupResponse::Metadata < ::Struct
  # Returns the value of attribute topics
  #
  # @return [Object] the current value of topics
  def topics; end

  # Sets the attribute topics
  #
  # @param value [Object] the value to set the attribute topics to.
  # @return [Object] the newly set value
  def topics=(_); end

  # Returns the value of attribute user_data
  #
  # @return [Object] the current value of user_data
  def user_data; end

  # Sets the attribute user_data
  #
  # @param value [Object] the value to set the attribute user_data to.
  # @return [Object] the newly set value
  def user_data=(_); end

  # Returns the value of attribute version
  #
  # @return [Object] the current value of version
  def version; end

  # Sets the attribute version
  #
  # @param value [Object] the value to set the attribute version to.
  # @return [Object] the newly set value
  def version=(_); end

  class << self
    def [](*_arg0); end
    def inspect; end
    def keyword_init?; end
    def members; end
    def new(*_arg0); end
  end
end

# source://ruby-kafka//lib/kafka/protocol.rb#26
Kafka::Protocol::LEAVE_GROUP_API = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol.rb#29
Kafka::Protocol::LIST_GROUPS_API = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol.rb#19
Kafka::Protocol::LIST_OFFSET_API = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol/leave_group_request.rb#5
class Kafka::Protocol::LeaveGroupRequest
  # @return [LeaveGroupRequest] a new instance of LeaveGroupRequest
  #
  # source://ruby-kafka//lib/kafka/protocol/leave_group_request.rb#6
  def initialize(group_id:, member_id:); end

  # source://ruby-kafka//lib/kafka/protocol/leave_group_request.rb#11
  def api_key; end

  # source://ruby-kafka//lib/kafka/protocol/leave_group_request.rb#19
  def encode(encoder); end

  # source://ruby-kafka//lib/kafka/protocol/leave_group_request.rb#15
  def response_class; end
end

# source://ruby-kafka//lib/kafka/protocol/leave_group_response.rb#5
class Kafka::Protocol::LeaveGroupResponse
  # @return [LeaveGroupResponse] a new instance of LeaveGroupResponse
  #
  # source://ruby-kafka//lib/kafka/protocol/leave_group_response.rb#8
  def initialize(error_code:); end

  # Returns the value of attribute error_code.
  #
  # source://ruby-kafka//lib/kafka/protocol/leave_group_response.rb#6
  def error_code; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/leave_group_response.rb#12
    def decode(decoder); end
  end
end

# source://ruby-kafka//lib/kafka/protocol/list_groups_request.rb#5
class Kafka::Protocol::ListGroupsRequest
  # source://ruby-kafka//lib/kafka/protocol/list_groups_request.rb#6
  def api_key; end

  # source://ruby-kafka//lib/kafka/protocol/list_groups_request.rb#10
  def api_version; end

  # source://ruby-kafka//lib/kafka/protocol/list_groups_request.rb#18
  def encode(encoder); end

  # source://ruby-kafka//lib/kafka/protocol/list_groups_request.rb#14
  def response_class; end
end

# source://ruby-kafka//lib/kafka/protocol/list_groups_response.rb#5
class Kafka::Protocol::ListGroupsResponse
  # @return [ListGroupsResponse] a new instance of ListGroupsResponse
  #
  # source://ruby-kafka//lib/kafka/protocol/list_groups_response.rb#17
  def initialize(error_code:, groups:); end

  # Returns the value of attribute error_code.
  #
  # source://ruby-kafka//lib/kafka/protocol/list_groups_response.rb#15
  def error_code; end

  # Returns the value of attribute groups.
  #
  # source://ruby-kafka//lib/kafka/protocol/list_groups_response.rb#15
  def groups; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/list_groups_response.rb#22
    def decode(decoder); end
  end
end

# source://ruby-kafka//lib/kafka/protocol/list_groups_response.rb#6
class Kafka::Protocol::ListGroupsResponse::GroupEntry
  # @return [GroupEntry] a new instance of GroupEntry
  #
  # source://ruby-kafka//lib/kafka/protocol/list_groups_response.rb#9
  def initialize(group_id:, protocol_type:); end

  # Returns the value of attribute group_id.
  #
  # source://ruby-kafka//lib/kafka/protocol/list_groups_response.rb#7
  def group_id; end

  # Returns the value of attribute protocol_type.
  #
  # source://ruby-kafka//lib/kafka/protocol/list_groups_response.rb#7
  def protocol_type; end
end

# A request to list the available offsets for a set of topics/partitions.
#
# ## API Specification
#
#     OffsetRequest => ReplicaId [TopicName [Partition Time MaxNumberOfOffsets]]
#       ReplicaId => int32
#       IsolationLevel => int8
#       TopicName => string
#       Partition => int32
#       Time => int64
#
# source://ruby-kafka//lib/kafka/protocol/list_offset_request.rb#16
class Kafka::Protocol::ListOffsetRequest
  # @param topics [Hash]
  # @return [ListOffsetRequest] a new instance of ListOffsetRequest
  #
  # source://ruby-kafka//lib/kafka/protocol/list_offset_request.rb#21
  def initialize(topics:); end

  # source://ruby-kafka//lib/kafka/protocol/list_offset_request.rb#30
  def api_key; end

  # source://ruby-kafka//lib/kafka/protocol/list_offset_request.rb#26
  def api_version; end

  # source://ruby-kafka//lib/kafka/protocol/list_offset_request.rb#38
  def encode(encoder); end

  # source://ruby-kafka//lib/kafka/protocol/list_offset_request.rb#34
  def response_class; end
end

# source://ruby-kafka//lib/kafka/protocol/list_offset_request.rb#18
Kafka::Protocol::ListOffsetRequest::ISOLATION_READ_COMMITTED = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol/list_offset_request.rb#17
Kafka::Protocol::ListOffsetRequest::ISOLATION_READ_UNCOMMITTED = T.let(T.unsafe(nil), Integer)

# A response to a list offset request.
#
# ## API Specification
#
#     OffsetResponse => [TopicName [PartitionOffsets]]
#       ThrottleTimeMS => int32
#       PartitionOffsets => Partition ErrorCode Timestamp Offset
#       Partition => int32
#       ErrorCode => int16
#       Timestamp => int64
#       Offset => int64
#
# source://ruby-kafka//lib/kafka/protocol/list_offset_response.rb#18
class Kafka::Protocol::ListOffsetResponse
  # @return [ListOffsetResponse] a new instance of ListOffsetResponse
  #
  # source://ruby-kafka//lib/kafka/protocol/list_offset_response.rb#41
  def initialize(topics:); end

  # source://ruby-kafka//lib/kafka/protocol/list_offset_response.rb#45
  def offset_for(topic, partition); end

  # Returns the value of attribute topics.
  #
  # source://ruby-kafka//lib/kafka/protocol/list_offset_response.rb#39
  def topics; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/list_offset_response.rb#65
    def decode(decoder); end
  end
end

# source://ruby-kafka//lib/kafka/protocol/list_offset_response.rb#28
class Kafka::Protocol::ListOffsetResponse::PartitionOffsetInfo
  # @return [PartitionOffsetInfo] a new instance of PartitionOffsetInfo
  #
  # source://ruby-kafka//lib/kafka/protocol/list_offset_response.rb#31
  def initialize(partition:, error_code:, timestamp:, offset:); end

  # Returns the value of attribute error_code.
  #
  # source://ruby-kafka//lib/kafka/protocol/list_offset_response.rb#29
  def error_code; end

  # Returns the value of attribute offset.
  #
  # source://ruby-kafka//lib/kafka/protocol/list_offset_response.rb#29
  def offset; end

  # Returns the value of attribute partition.
  #
  # source://ruby-kafka//lib/kafka/protocol/list_offset_response.rb#29
  def partition; end

  # Returns the value of attribute timestamp.
  #
  # source://ruby-kafka//lib/kafka/protocol/list_offset_response.rb#29
  def timestamp; end
end

# source://ruby-kafka//lib/kafka/protocol/list_offset_response.rb#19
class Kafka::Protocol::ListOffsetResponse::TopicOffsetInfo
  # @return [TopicOffsetInfo] a new instance of TopicOffsetInfo
  #
  # source://ruby-kafka//lib/kafka/protocol/list_offset_response.rb#22
  def initialize(name:, partition_offsets:); end

  # Returns the value of attribute name.
  #
  # source://ruby-kafka//lib/kafka/protocol/list_offset_response.rb#20
  def name; end

  # Returns the value of attribute partition_offsets.
  #
  # source://ruby-kafka//lib/kafka/protocol/list_offset_response.rb#20
  def partition_offsets; end
end

# source://ruby-kafka//lib/kafka/protocol/member_assignment.rb#5
class Kafka::Protocol::MemberAssignment
  # @return [MemberAssignment] a new instance of MemberAssignment
  #
  # source://ruby-kafka//lib/kafka/protocol/member_assignment.rb#8
  def initialize(version: T.unsafe(nil), topics: T.unsafe(nil), user_data: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/protocol/member_assignment.rb#14
  def assign(topic, partitions); end

  # source://ruby-kafka//lib/kafka/protocol/member_assignment.rb#19
  def encode(encoder); end

  # Returns the value of attribute topics.
  #
  # source://ruby-kafka//lib/kafka/protocol/member_assignment.rb#6
  def topics; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/member_assignment.rb#33
    def decode(decoder); end
  end
end

# ## API Specification
#
#     Message => Crc MagicByte Attributes Timestamp Key Value
#         Crc => int32
#         MagicByte => int8
#         Attributes => int8
#         Timestamp => int64, in ms
#         Key => bytes
#         Value => bytes
#
# source://ruby-kafka//lib/kafka/protocol/message.rb#19
class Kafka::Protocol::Message
  # @return [Message] a new instance of Message
  #
  # source://ruby-kafka//lib/kafka/protocol/message.rb#26
  def initialize(value:, key: T.unsafe(nil), create_time: T.unsafe(nil), codec_id: T.unsafe(nil), offset: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/protocol/message.rb#43
  def ==(other); end

  # Returns the value of attribute bytesize.
  #
  # source://ruby-kafka//lib/kafka/protocol/message.rb#24
  def bytesize; end

  # Returns the value of attribute codec_id.
  #
  # source://ruby-kafka//lib/kafka/protocol/message.rb#22
  def codec_id; end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/protocol/message.rb#50
  def compressed?; end

  # Returns the value of attribute create_time.
  #
  # source://ruby-kafka//lib/kafka/protocol/message.rb#24
  def create_time; end

  # @return [Array<Kafka::Protocol::Message>]
  #
  # source://ruby-kafka//lib/kafka/protocol/message.rb#55
  def decompress; end

  # source://ruby-kafka//lib/kafka/protocol/message.rb#36
  def encode(encoder); end

  # source://ruby-kafka//lib/kafka/protocol/message.rb#110
  def headers; end

  # Ensure the backward compatibility of Message format from Kafka 0.11.x
  #
  # source://ruby-kafka//lib/kafka/protocol/message.rb#106
  def is_control_record; end

  # Returns the value of attribute key.
  #
  # source://ruby-kafka//lib/kafka/protocol/message.rb#22
  def key; end

  # Returns the value of attribute offset.
  #
  # source://ruby-kafka//lib/kafka/protocol/message.rb#22
  def offset; end

  # Returns the value of attribute value.
  #
  # source://ruby-kafka//lib/kafka/protocol/message.rb#22
  def value; end

  private

  # Offsets may be relative with regards to wrapped message offset, but there are special cases.
  #
  # Cases when client will receive corrected offsets:
  #   - When fetch request is version 0, kafka will correct relative offset on broker side before replying fetch response
  #   - When messages is stored in 0.9 format on disk (broker configured to do so).
  #
  # All other cases, compressed inner messages should have relative offset, with below attributes:
  #   - The container message should have the 'real' offset
  #   - The container message's offset should be the 'real' offset of the last message in the compressed batch
  #
  # source://ruby-kafka//lib/kafka/protocol/message.rb#125
  def correct_offsets(messages); end

  # source://ruby-kafka//lib/kafka/protocol/message.rb#145
  def encode_with_crc; end

  # source://ruby-kafka//lib/kafka/protocol/message.rb#158
  def encode_without_crc; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/message.rb#66
    def decode(decoder); end
  end
end

# source://ruby-kafka//lib/kafka/protocol/message.rb#20
Kafka::Protocol::Message::MAGIC_BYTE = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol/message_set.rb#5
class Kafka::Protocol::MessageSet
  # @return [MessageSet] a new instance of MessageSet
  #
  # source://ruby-kafka//lib/kafka/protocol/message_set.rb#8
  def initialize(messages: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/protocol/message_set.rb#16
  def ==(other); end

  # source://ruby-kafka//lib/kafka/protocol/message_set.rb#20
  def encode(encoder); end

  # Returns the value of attribute messages.
  #
  # source://ruby-kafka//lib/kafka/protocol/message_set.rb#6
  def messages; end

  # source://ruby-kafka//lib/kafka/protocol/message_set.rb#12
  def size; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/message_set.rb#28
    def decode(decoder); end
  end
end

# source://ruby-kafka//lib/kafka/protocol/metadata_request.rb#5
class Kafka::Protocol::MetadataRequest
  # A request for cluster metadata.
  #
  # @param topics [Array<String>]
  # @return [MetadataRequest] a new instance of MetadataRequest
  #
  # source://ruby-kafka//lib/kafka/protocol/metadata_request.rb#10
  def initialize(topics: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/protocol/metadata_request.rb#14
  def api_key; end

  # source://ruby-kafka//lib/kafka/protocol/metadata_request.rb#18
  def api_version; end

  # source://ruby-kafka//lib/kafka/protocol/metadata_request.rb#26
  def encode(encoder); end

  # source://ruby-kafka//lib/kafka/protocol/metadata_request.rb#22
  def response_class; end
end

# A response to a {MetadataRequest}.
#
# The response contains information on the brokers, topics, and partitions in
# the cluster.
#
# * For each broker a node id, host, and port is provided.
# * For each topic partition the node id of the broker acting as partition leader,
#   as well as a list of node ids for the set of replicas, are given. The `isr` list is
#   the subset of replicas that are "in sync", i.e. have fully caught up with the
#   leader.
#
# ## API Specification
#
#     MetadataResponse => [Broker][TopicMetadata]
#       Broker => NodeId Host Port  (any number of brokers may be returned)
#         NodeId => int32
#         Host => string
#         Port => int32
#
#       TopicMetadata => TopicErrorCode TopicName [PartitionMetadata]
#         TopicErrorCode => int16
#
#       PartitionMetadata => PartitionErrorCode PartitionId Leader Replicas Isr
#         PartitionErrorCode => int16
#         PartitionId => int32
#         Leader => int32
#         Replicas => [int32]
#         Isr => [int32]
#
# source://ruby-kafka//lib/kafka/protocol/metadata_response.rb#35
class Kafka::Protocol::MetadataResponse
  # @return [MetadataResponse] a new instance of MetadataResponse
  #
  # source://ruby-kafka//lib/kafka/protocol/metadata_response.rb#75
  def initialize(brokers:, controller_id:, topics:); end

  # @return [Array<Kafka::BrokerInfo>] the list of brokers in the cluster.
  #
  # source://ruby-kafka//lib/kafka/protocol/metadata_response.rb#67
  def brokers; end

  # source://ruby-kafka//lib/kafka/protocol/metadata_response.rb#123
  def controller_broker; end

  # @return [Integer] The broker id of the controller broker.
  #
  # source://ruby-kafka//lib/kafka/protocol/metadata_response.rb#73
  def controller_id; end

  # Finds the broker info for the given node id.
  #
  # @param node_id [Integer] the node id of the broker.
  # @raise [Kafka::NoSuchBroker]
  # @return [Kafka::BrokerInfo] information about the broker.
  #
  # source://ruby-kafka//lib/kafka/protocol/metadata_response.rb#115
  def find_broker(node_id); end

  # Finds the node id of the broker that is acting as leader for the given topic
  # and partition per this metadata.
  #
  # @param topic [String] the name of the topic.
  # @param partition [Integer] the partition number.
  # @return [Integer] the node id of the leader.
  #
  # source://ruby-kafka//lib/kafka/protocol/metadata_response.rb#87
  def find_leader_id(topic, partition); end

  # source://ruby-kafka//lib/kafka/protocol/metadata_response.rb#127
  def partitions_for(topic_name); end

  # @return [Array<TopicMetadata>] the list of topics in the cluster.
  #
  # source://ruby-kafka//lib/kafka/protocol/metadata_response.rb#70
  def topics; end

  class << self
    # Decodes a MetadataResponse from a {Decoder} containing response data.
    #
    # @param decoder [Decoder]
    # @return [MetadataResponse] the metadata response.
    #
    # source://ruby-kafka//lib/kafka/protocol/metadata_response.rb#143
    def decode(decoder); end
  end
end

# source://ruby-kafka//lib/kafka/protocol/metadata_response.rb#36
class Kafka::Protocol::MetadataResponse::PartitionMetadata
  # @return [PartitionMetadata] a new instance of PartitionMetadata
  #
  # source://ruby-kafka//lib/kafka/protocol/metadata_response.rb#41
  def initialize(partition_error_code:, partition_id:, leader:, replicas: T.unsafe(nil), isr: T.unsafe(nil)); end

  # Returns the value of attribute leader.
  #
  # source://ruby-kafka//lib/kafka/protocol/metadata_response.rb#37
  def leader; end

  # Returns the value of attribute partition_error_code.
  #
  # source://ruby-kafka//lib/kafka/protocol/metadata_response.rb#39
  def partition_error_code; end

  # Returns the value of attribute partition_id.
  #
  # source://ruby-kafka//lib/kafka/protocol/metadata_response.rb#37
  def partition_id; end

  # Returns the value of attribute replicas.
  #
  # source://ruby-kafka//lib/kafka/protocol/metadata_response.rb#37
  def replicas; end
end

# source://ruby-kafka//lib/kafka/protocol/metadata_response.rb#50
class Kafka::Protocol::MetadataResponse::TopicMetadata
  # @return [TopicMetadata] a new instance of TopicMetadata
  #
  # source://ruby-kafka//lib/kafka/protocol/metadata_response.rb#59
  def initialize(topic_name:, partitions:, topic_error_code: T.unsafe(nil)); end

  # @return [Array<PartitionMetadata>] the partitions in the topic.
  #
  # source://ruby-kafka//lib/kafka/protocol/metadata_response.rb#55
  def partitions; end

  # Returns the value of attribute topic_error_code.
  #
  # source://ruby-kafka//lib/kafka/protocol/metadata_response.rb#57
  def topic_error_code; end

  # @return [String] the name of the topic
  #
  # source://ruby-kafka//lib/kafka/protocol/metadata_response.rb#52
  def topic_name; end
end

# source://ruby-kafka//lib/kafka/protocol.rb#21
Kafka::Protocol::OFFSET_COMMIT_API = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol.rb#22
Kafka::Protocol::OFFSET_FETCH_API = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol/offset_commit_request.rb#5
class Kafka::Protocol::OffsetCommitRequest
  # @return [OffsetCommitRequest] a new instance of OffsetCommitRequest
  #
  # source://ruby-kafka//lib/kafka/protocol/offset_commit_request.rb#21
  def initialize(group_id:, generation_id:, member_id:, offsets:, retention_time: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/protocol/offset_commit_request.rb#9
  def api_key; end

  # source://ruby-kafka//lib/kafka/protocol/offset_commit_request.rb#13
  def api_version; end

  # source://ruby-kafka//lib/kafka/protocol/offset_commit_request.rb#29
  def encode(encoder); end

  # source://ruby-kafka//lib/kafka/protocol/offset_commit_request.rb#17
  def response_class; end
end

# This value signals to the broker that its default configuration should be used.
#
# source://ruby-kafka//lib/kafka/protocol/offset_commit_request.rb#7
Kafka::Protocol::OffsetCommitRequest::DEFAULT_RETENTION_TIME = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol/offset_commit_response.rb#5
class Kafka::Protocol::OffsetCommitResponse
  # @return [OffsetCommitResponse] a new instance of OffsetCommitResponse
  #
  # source://ruby-kafka//lib/kafka/protocol/offset_commit_response.rb#8
  def initialize(topics:); end

  # Returns the value of attribute topics.
  #
  # source://ruby-kafka//lib/kafka/protocol/offset_commit_response.rb#6
  def topics; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/offset_commit_response.rb#12
    def decode(decoder); end
  end
end

# source://ruby-kafka//lib/kafka/protocol/offset_fetch_request.rb#5
class Kafka::Protocol::OffsetFetchRequest
  # @return [OffsetFetchRequest] a new instance of OffsetFetchRequest
  #
  # source://ruby-kafka//lib/kafka/protocol/offset_fetch_request.rb#6
  def initialize(group_id:, topics:); end

  # source://ruby-kafka//lib/kafka/protocol/offset_fetch_request.rb#11
  def api_key; end

  # setting topics to nil fetches all offsets for a consumer group
  # and that feature is only available in API version 2+
  #
  # source://ruby-kafka//lib/kafka/protocol/offset_fetch_request.rb#17
  def api_version; end

  # source://ruby-kafka//lib/kafka/protocol/offset_fetch_request.rb#25
  def encode(encoder); end

  # source://ruby-kafka//lib/kafka/protocol/offset_fetch_request.rb#21
  def response_class; end
end

# source://ruby-kafka//lib/kafka/protocol/offset_fetch_response.rb#5
class Kafka::Protocol::OffsetFetchResponse
  # @return [OffsetFetchResponse] a new instance of OffsetFetchResponse
  #
  # source://ruby-kafka//lib/kafka/protocol/offset_fetch_response.rb#18
  def initialize(topics:); end

  # source://ruby-kafka//lib/kafka/protocol/offset_fetch_response.rb#22
  def offset_for(topic, partition); end

  # Returns the value of attribute topics.
  #
  # source://ruby-kafka//lib/kafka/protocol/offset_fetch_response.rb#16
  def topics; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/offset_fetch_response.rb#33
    def decode(decoder); end
  end
end

# source://ruby-kafka//lib/kafka/protocol/offset_fetch_response.rb#6
class Kafka::Protocol::OffsetFetchResponse::PartitionOffsetInfo
  # @return [PartitionOffsetInfo] a new instance of PartitionOffsetInfo
  #
  # source://ruby-kafka//lib/kafka/protocol/offset_fetch_response.rb#9
  def initialize(offset:, metadata:, error_code:); end

  # Returns the value of attribute error_code.
  #
  # source://ruby-kafka//lib/kafka/protocol/offset_fetch_response.rb#7
  def error_code; end

  # Returns the value of attribute metadata.
  #
  # source://ruby-kafka//lib/kafka/protocol/offset_fetch_response.rb#7
  def metadata; end

  # Returns the value of attribute offset.
  #
  # source://ruby-kafka//lib/kafka/protocol/offset_fetch_response.rb#7
  def offset; end
end

# source://ruby-kafka//lib/kafka/protocol.rb#17
Kafka::Protocol::PRODUCE_API = T.let(T.unsafe(nil), Integer)

# A produce request sends a message set to the server.
#
# ## API Specification
#
#     ProduceRequest => RequiredAcks Timeout [TopicName [Partition MessageSetSize MessageSet]]
#         RequiredAcks => int16
#         Timeout => int32
#         Partition => int32
#         MessageSetSize => int32
#
#     MessageSet => [Offset MessageSize Message]
#         Offset => int64
#         MessageSize => int32
#
#     Message => Crc MagicByte Attributes Key Value
#         Crc => int32
#         MagicByte => int8
#         Attributes => int8
#         Key => bytes
#         Value => bytes
#
# source://ruby-kafka//lib/kafka/protocol/produce_request.rb#29
class Kafka::Protocol::ProduceRequest
  # @param required_acks [Integer]
  # @param timeout [Integer]
  # @param messages_for_topics [Hash]
  # @return [ProduceRequest] a new instance of ProduceRequest
  #
  # source://ruby-kafka//lib/kafka/protocol/produce_request.rb#37
  def initialize(required_acks:, timeout:, messages_for_topics:, transactional_id: T.unsafe(nil), compressor: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/protocol/produce_request.rb#45
  def api_key; end

  # source://ruby-kafka//lib/kafka/protocol/produce_request.rb#49
  def api_version; end

  # Returns the value of attribute compressor.
  #
  # source://ruby-kafka//lib/kafka/protocol/produce_request.rb#32
  def compressor; end

  # source://ruby-kafka//lib/kafka/protocol/produce_request.rb#65
  def encode(encoder); end

  # Returns the value of attribute messages_for_topics.
  #
  # source://ruby-kafka//lib/kafka/protocol/produce_request.rb#32
  def messages_for_topics; end

  # Returns the value of attribute required_acks.
  #
  # source://ruby-kafka//lib/kafka/protocol/produce_request.rb#32
  def required_acks; end

  # Whether this request requires any acknowledgements at all. If no acknowledgements
  # are required, the server will not send back a response at all.
  #
  # @return [Boolean] true if acknowledgements are required, false otherwise.
  #
  # source://ruby-kafka//lib/kafka/protocol/produce_request.rb#61
  def requires_acks?; end

  # source://ruby-kafka//lib/kafka/protocol/produce_request.rb#53
  def response_class; end

  # Returns the value of attribute timeout.
  #
  # source://ruby-kafka//lib/kafka/protocol/produce_request.rb#32
  def timeout; end

  # Returns the value of attribute transactional_id.
  #
  # source://ruby-kafka//lib/kafka/protocol/produce_request.rb#32
  def transactional_id; end

  private

  # source://ruby-kafka//lib/kafka/protocol/produce_request.rb#85
  def compress(record_batch); end
end

# source://ruby-kafka//lib/kafka/protocol/produce_request.rb#30
Kafka::Protocol::ProduceRequest::API_MIN_VERSION = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol/produce_response.rb#5
class Kafka::Protocol::ProduceResponse
  # @return [ProduceResponse] a new instance of ProduceResponse
  #
  # source://ruby-kafka//lib/kafka/protocol/produce_response.rb#28
  def initialize(topics: T.unsafe(nil), throttle_time_ms: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/protocol/produce_response.rb#33
  def each_partition; end

  # Returns the value of attribute throttle_time_ms.
  #
  # source://ruby-kafka//lib/kafka/protocol/produce_response.rb#26
  def throttle_time_ms; end

  # Returns the value of attribute topics.
  #
  # source://ruby-kafka//lib/kafka/protocol/produce_response.rb#26
  def topics; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/produce_response.rb#41
    def decode(decoder); end
  end
end

# source://ruby-kafka//lib/kafka/protocol/produce_response.rb#15
class Kafka::Protocol::ProduceResponse::PartitionInfo
  # @return [PartitionInfo] a new instance of PartitionInfo
  #
  # source://ruby-kafka//lib/kafka/protocol/produce_response.rb#18
  def initialize(partition:, error_code:, offset:, timestamp:); end

  # Returns the value of attribute error_code.
  #
  # source://ruby-kafka//lib/kafka/protocol/produce_response.rb#16
  def error_code; end

  # Returns the value of attribute offset.
  #
  # source://ruby-kafka//lib/kafka/protocol/produce_response.rb#16
  def offset; end

  # Returns the value of attribute partition.
  #
  # source://ruby-kafka//lib/kafka/protocol/produce_response.rb#16
  def partition; end

  # Returns the value of attribute timestamp.
  #
  # source://ruby-kafka//lib/kafka/protocol/produce_response.rb#16
  def timestamp; end
end

# source://ruby-kafka//lib/kafka/protocol/produce_response.rb#6
class Kafka::Protocol::ProduceResponse::TopicInfo
  # @return [TopicInfo] a new instance of TopicInfo
  #
  # source://ruby-kafka//lib/kafka/protocol/produce_response.rb#9
  def initialize(topic:, partitions:); end

  # Returns the value of attribute partitions.
  #
  # source://ruby-kafka//lib/kafka/protocol/produce_response.rb#7
  def partitions; end

  # Returns the value of attribute topic.
  #
  # source://ruby-kafka//lib/kafka/protocol/produce_response.rb#7
  def topic; end
end

# The replica id of non-brokers is always -1.
#
# source://ruby-kafka//lib/kafka/protocol.rb#15
Kafka::Protocol::REPLICA_ID = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol.rb#135
Kafka::Protocol::RESOURCE_TYPES = T.let(T.unsafe(nil), Hash)

# source://ruby-kafka//lib/kafka/protocol.rb#129
Kafka::Protocol::RESOURCE_TYPE_ANY = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol.rb#132
Kafka::Protocol::RESOURCE_TYPE_CLUSTER = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol.rb#134
Kafka::Protocol::RESOURCE_TYPE_DELEGATION_TOKEN = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol.rb#131
Kafka::Protocol::RESOURCE_TYPE_GROUP = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol.rb#130
Kafka::Protocol::RESOURCE_TYPE_TOPIC = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol.rb#133
Kafka::Protocol::RESOURCE_TYPE_TRANSACTIONAL_ID = T.let(T.unsafe(nil), Integer)

# A mapping from int to corresponding resource type in symbol.
# https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/resource/ResourceType.java
#
# source://ruby-kafka//lib/kafka/protocol.rb#128
Kafka::Protocol::RESOURCE_TYPE_UNKNOWN = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol/record.rb#3
class Kafka::Protocol::Record
  # @return [Record] a new instance of Record
  #
  # source://ruby-kafka//lib/kafka/protocol/record.rb#7
  def initialize(value:, key: T.unsafe(nil), headers: T.unsafe(nil), attributes: T.unsafe(nil), offset_delta: T.unsafe(nil), offset: T.unsafe(nil), timestamp_delta: T.unsafe(nil), create_time: T.unsafe(nil), is_control_record: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/protocol/record.rb#52
  def ==(other); end

  # Returns the value of attribute attributes.
  #
  # source://ruby-kafka//lib/kafka/protocol/record.rb#4
  def attributes; end

  # Returns the value of attribute bytesize.
  #
  # source://ruby-kafka//lib/kafka/protocol/record.rb#4
  def bytesize; end

  # Returns the value of attribute create_time.
  #
  # source://ruby-kafka//lib/kafka/protocol/record.rb#5
  def create_time; end

  # Sets the attribute create_time
  #
  # @param value the value to set the attribute create_time to.
  #
  # source://ruby-kafka//lib/kafka/protocol/record.rb#5
  def create_time=(_arg0); end

  # source://ruby-kafka//lib/kafka/protocol/record.rb#32
  def encode(encoder); end

  # Returns the value of attribute headers.
  #
  # source://ruby-kafka//lib/kafka/protocol/record.rb#4
  def headers; end

  # Returns the value of attribute is_control_record.
  #
  # source://ruby-kafka//lib/kafka/protocol/record.rb#5
  def is_control_record; end

  # Sets the attribute is_control_record
  #
  # @param value the value to set the attribute is_control_record to.
  #
  # source://ruby-kafka//lib/kafka/protocol/record.rb#5
  def is_control_record=(_arg0); end

  # Returns the value of attribute key.
  #
  # source://ruby-kafka//lib/kafka/protocol/record.rb#4
  def key; end

  # Returns the value of attribute offset.
  #
  # source://ruby-kafka//lib/kafka/protocol/record.rb#5
  def offset; end

  # Sets the attribute offset
  #
  # @param value the value to set the attribute offset to.
  #
  # source://ruby-kafka//lib/kafka/protocol/record.rb#5
  def offset=(_arg0); end

  # Returns the value of attribute offset_delta.
  #
  # source://ruby-kafka//lib/kafka/protocol/record.rb#5
  def offset_delta; end

  # Sets the attribute offset_delta
  #
  # @param value the value to set the attribute offset_delta to.
  #
  # source://ruby-kafka//lib/kafka/protocol/record.rb#5
  def offset_delta=(_arg0); end

  # Returns the value of attribute timestamp_delta.
  #
  # source://ruby-kafka//lib/kafka/protocol/record.rb#5
  def timestamp_delta; end

  # Sets the attribute timestamp_delta
  #
  # @param value the value to set the attribute timestamp_delta to.
  #
  # source://ruby-kafka//lib/kafka/protocol/record.rb#5
  def timestamp_delta=(_arg0); end

  # Returns the value of attribute value.
  #
  # source://ruby-kafka//lib/kafka/protocol/record.rb#4
  def value; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/record.rb#59
    def decode(decoder); end
  end
end

# source://ruby-kafka//lib/kafka/protocol/record_batch.rb#7
class Kafka::Protocol::RecordBatch
  # @return [RecordBatch] a new instance of RecordBatch
  #
  # source://ruby-kafka//lib/kafka/protocol/record_batch.rb#21
  def initialize(records: T.unsafe(nil), first_offset: T.unsafe(nil), first_timestamp: T.unsafe(nil), partition_leader_epoch: T.unsafe(nil), codec_id: T.unsafe(nil), in_transaction: T.unsafe(nil), is_control_batch: T.unsafe(nil), last_offset_delta: T.unsafe(nil), producer_id: T.unsafe(nil), producer_epoch: T.unsafe(nil), first_sequence: T.unsafe(nil), max_timestamp: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/protocol/record_batch.rb#140
  def ==(other); end

  # source://ruby-kafka//lib/kafka/protocol/record_batch.rb#64
  def attributes; end

  # Returns the value of attribute codec_id.
  #
  # source://ruby-kafka//lib/kafka/protocol/record_batch.rb#19
  def codec_id; end

  # Sets the attribute codec_id
  #
  # @param value the value to set the attribute codec_id to.
  #
  # source://ruby-kafka//lib/kafka/protocol/record_batch.rb#19
  def codec_id=(_arg0); end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/protocol/record_batch.rb#122
  def compressed?; end

  # source://ruby-kafka//lib/kafka/protocol/record_batch.rb#70
  def encode(encoder); end

  # source://ruby-kafka//lib/kafka/protocol/record_batch.rb#113
  def encode_record_array; end

  # source://ruby-kafka//lib/kafka/protocol/record_batch.rb#88
  def encode_record_batch_body; end

  # Returns the value of attribute first_offset.
  #
  # source://ruby-kafka//lib/kafka/protocol/record_batch.rb#17
  def first_offset; end

  # Returns the value of attribute first_sequence.
  #
  # source://ruby-kafka//lib/kafka/protocol/record_batch.rb#17
  def first_sequence; end

  # Returns the value of attribute first_timestamp.
  #
  # source://ruby-kafka//lib/kafka/protocol/record_batch.rb#17
  def first_timestamp; end

  # source://ruby-kafka//lib/kafka/protocol/record_batch.rb#126
  def fulfill_relative_data; end

  # Returns the value of attribute in_transaction.
  #
  # source://ruby-kafka//lib/kafka/protocol/record_batch.rb#17
  def in_transaction; end

  # Returns the value of attribute is_control_batch.
  #
  # source://ruby-kafka//lib/kafka/protocol/record_batch.rb#17
  def is_control_batch; end

  # source://ruby-kafka//lib/kafka/protocol/record_batch.rb#60
  def last_offset; end

  # Returns the value of attribute last_offset_delta.
  #
  # source://ruby-kafka//lib/kafka/protocol/record_batch.rb#17
  def last_offset_delta; end

  # source://ruby-kafka//lib/kafka/protocol/record_batch.rb#215
  def mark_control_record; end

  # Returns the value of attribute max_timestamp.
  #
  # source://ruby-kafka//lib/kafka/protocol/record_batch.rb#17
  def max_timestamp; end

  # Returns the value of attribute partition_leader_epoch.
  #
  # source://ruby-kafka//lib/kafka/protocol/record_batch.rb#17
  def partition_leader_epoch; end

  # Returns the value of attribute producer_epoch.
  #
  # source://ruby-kafka//lib/kafka/protocol/record_batch.rb#17
  def producer_epoch; end

  # Returns the value of attribute producer_id.
  #
  # source://ruby-kafka//lib/kafka/protocol/record_batch.rb#17
  def producer_id; end

  # Returns the value of attribute records.
  #
  # source://ruby-kafka//lib/kafka/protocol/record_batch.rb#17
  def records; end

  # source://ruby-kafka//lib/kafka/protocol/record_batch.rb#56
  def size; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/record_batch.rb#152
    def decode(decoder); end
  end
end

# Masks to extract information from attributes
#
# source://ruby-kafka//lib/kafka/protocol/record_batch.rb#12
Kafka::Protocol::RecordBatch::CODEC_ID_MASK = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol/record_batch.rb#13
Kafka::Protocol::RecordBatch::IN_TRANSACTION_MASK = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol/record_batch.rb#14
Kafka::Protocol::RecordBatch::IS_CONTROL_BATCH_MASK = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol/record_batch.rb#8
Kafka::Protocol::RecordBatch::MAGIC_BYTE = T.let(T.unsafe(nil), Integer)

# The size of metadata before the real record data
#
# source://ruby-kafka//lib/kafka/protocol/record_batch.rb#10
Kafka::Protocol::RecordBatch::RECORD_BATCH_OVERHEAD = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol/record_batch.rb#15
Kafka::Protocol::RecordBatch::TIMESTAMP_TYPE_MASK = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol/request_message.rb#5
class Kafka::Protocol::RequestMessage
  # @return [RequestMessage] a new instance of RequestMessage
  #
  # source://ruby-kafka//lib/kafka/protocol/request_message.rb#8
  def initialize(api_key:, correlation_id:, client_id:, request:, api_version: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/protocol/request_message.rb#16
  def encode(encoder); end
end

# source://ruby-kafka//lib/kafka/protocol/request_message.rb#6
Kafka::Protocol::RequestMessage::API_VERSION = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol.rb#30
Kafka::Protocol::SASL_HANDSHAKE_API = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol.rb#27
Kafka::Protocol::SYNC_GROUP_API = T.let(T.unsafe(nil), Integer)

# SaslHandshake Request (Version: 0) => mechanism
#  mechanism => string
#
# source://ruby-kafka//lib/kafka/protocol/sasl_handshake_request.rb#9
class Kafka::Protocol::SaslHandshakeRequest
  # @return [SaslHandshakeRequest] a new instance of SaslHandshakeRequest
  #
  # source://ruby-kafka//lib/kafka/protocol/sasl_handshake_request.rb#13
  def initialize(mechanism); end

  # source://ruby-kafka//lib/kafka/protocol/sasl_handshake_request.rb#20
  def api_key; end

  # source://ruby-kafka//lib/kafka/protocol/sasl_handshake_request.rb#28
  def encode(encoder); end

  # source://ruby-kafka//lib/kafka/protocol/sasl_handshake_request.rb#24
  def response_class; end
end

# source://ruby-kafka//lib/kafka/protocol/sasl_handshake_request.rb#11
Kafka::Protocol::SaslHandshakeRequest::SUPPORTED_MECHANISMS = T.let(T.unsafe(nil), Array)

# SaslHandshake Response (Version: 0) => error_code [enabled_mechanisms]
#  error_code => int16
#  enabled_mechanisms => array of strings
#
# source://ruby-kafka//lib/kafka/protocol/sasl_handshake_response.rb#10
class Kafka::Protocol::SaslHandshakeResponse
  # @return [SaslHandshakeResponse] a new instance of SaslHandshakeResponse
  #
  # source://ruby-kafka//lib/kafka/protocol/sasl_handshake_response.rb#15
  def initialize(error_code:, enabled_mechanisms:); end

  # Returns the value of attribute enabled_mechanisms.
  #
  # source://ruby-kafka//lib/kafka/protocol/sasl_handshake_response.rb#13
  def enabled_mechanisms; end

  # Returns the value of attribute error_code.
  #
  # source://ruby-kafka//lib/kafka/protocol/sasl_handshake_response.rb#11
  def error_code; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/sasl_handshake_response.rb#20
    def decode(decoder); end
  end
end

# source://ruby-kafka//lib/kafka/protocol/sync_group_request.rb#5
class Kafka::Protocol::SyncGroupRequest
  # @return [SyncGroupRequest] a new instance of SyncGroupRequest
  #
  # source://ruby-kafka//lib/kafka/protocol/sync_group_request.rb#6
  def initialize(group_id:, generation_id:, member_id:, group_assignment: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/protocol/sync_group_request.rb#13
  def api_key; end

  # source://ruby-kafka//lib/kafka/protocol/sync_group_request.rb#21
  def encode(encoder); end

  # source://ruby-kafka//lib/kafka/protocol/sync_group_request.rb#17
  def response_class; end
end

# source://ruby-kafka//lib/kafka/protocol/sync_group_response.rb#7
class Kafka::Protocol::SyncGroupResponse
  # @return [SyncGroupResponse] a new instance of SyncGroupResponse
  #
  # source://ruby-kafka//lib/kafka/protocol/sync_group_response.rb#10
  def initialize(error_code:, member_assignment:); end

  # Returns the value of attribute error_code.
  #
  # source://ruby-kafka//lib/kafka/protocol/sync_group_response.rb#8
  def error_code; end

  # Returns the value of attribute member_assignment.
  #
  # source://ruby-kafka//lib/kafka/protocol/sync_group_response.rb#8
  def member_assignment; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/sync_group_response.rb#15
    def decode(decoder); end
  end
end

# source://ruby-kafka//lib/kafka/protocol.rb#20
Kafka::Protocol::TOPIC_METADATA_API = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol.rb#38
Kafka::Protocol::TXN_OFFSET_COMMIT_API = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/protocol/txn_offset_commit_request.rb#5
class Kafka::Protocol::TxnOffsetCommitRequest
  # @return [TxnOffsetCommitRequest] a new instance of TxnOffsetCommitRequest
  #
  # source://ruby-kafka//lib/kafka/protocol/txn_offset_commit_request.rb#19
  def initialize(transactional_id:, group_id:, producer_id:, producer_epoch:, offsets:); end

  # source://ruby-kafka//lib/kafka/protocol/txn_offset_commit_request.rb#7
  def api_key; end

  # source://ruby-kafka//lib/kafka/protocol/txn_offset_commit_request.rb#11
  def api_version; end

  # source://ruby-kafka//lib/kafka/protocol/txn_offset_commit_request.rb#27
  def encode(encoder); end

  # source://ruby-kafka//lib/kafka/protocol/txn_offset_commit_request.rb#15
  def response_class; end
end

# source://ruby-kafka//lib/kafka/protocol/txn_offset_commit_response.rb#5
class Kafka::Protocol::TxnOffsetCommitResponse
  # @return [TxnOffsetCommitResponse] a new instance of TxnOffsetCommitResponse
  #
  # source://ruby-kafka//lib/kafka/protocol/txn_offset_commit_response.rb#26
  def initialize(errors:); end

  # Returns the value of attribute errors.
  #
  # source://ruby-kafka//lib/kafka/protocol/txn_offset_commit_response.rb#24
  def errors; end

  class << self
    # source://ruby-kafka//lib/kafka/protocol/txn_offset_commit_response.rb#30
    def decode(decoder); end
  end
end

# source://ruby-kafka//lib/kafka/protocol/txn_offset_commit_response.rb#6
class Kafka::Protocol::TxnOffsetCommitResponse::PartitionError
  # @return [PartitionError] a new instance of PartitionError
  #
  # source://ruby-kafka//lib/kafka/protocol/txn_offset_commit_response.rb#9
  def initialize(partition:, error_code:); end

  # Returns the value of attribute error_code.
  #
  # source://ruby-kafka//lib/kafka/protocol/txn_offset_commit_response.rb#7
  def error_code; end

  # Returns the value of attribute partition.
  #
  # source://ruby-kafka//lib/kafka/protocol/txn_offset_commit_response.rb#7
  def partition; end
end

# source://ruby-kafka//lib/kafka/protocol/txn_offset_commit_response.rb#15
class Kafka::Protocol::TxnOffsetCommitResponse::TopicPartitionsError
  # @return [TopicPartitionsError] a new instance of TopicPartitionsError
  #
  # source://ruby-kafka//lib/kafka/protocol/txn_offset_commit_response.rb#18
  def initialize(topic:, partitions:); end

  # Returns the value of attribute partitions.
  #
  # source://ruby-kafka//lib/kafka/protocol/txn_offset_commit_response.rb#16
  def partitions; end

  # Returns the value of attribute topic.
  #
  # source://ruby-kafka//lib/kafka/protocol/txn_offset_commit_response.rb#16
  def topic; end
end

# Subclasses of this exception class map to an error code described in the
# Kafka protocol specification.
# https://kafka.apache.org/protocol#protocol_error_codes
#
# source://ruby-kafka//lib/kafka.rb#25
class Kafka::ProtocolError < ::Kafka::Error; end

# 27
# The group is rebalancing, so a rejoin is needed.
#
# source://ruby-kafka//lib/kafka.rb#179
class Kafka::RebalanceInProgress < ::Kafka::ProtocolError; end

# 18
# If a message batch in a produce request exceeds the maximum configured
# segment size.
#
# source://ruby-kafka//lib/kafka.rb#131
class Kafka::RecordListTooLarge < ::Kafka::ProtocolError; end

# 9
# Raised if a replica is expected on a broker, but is not. Can be safely ignored.
#
# source://ruby-kafka//lib/kafka.rb#81
class Kafka::ReplicaNotAvailable < ::Kafka::ProtocolError; end

# 7
# This error is thrown if the request exceeds the user-specified time limit
# in the request.
#
# source://ruby-kafka//lib/kafka.rb#71
class Kafka::RequestTimedOut < ::Kafka::ProtocolError; end

# A round robin assignment strategy inpired on the
# original java client round robin assignor. It's capable
# of handling identical as well as different topic subscriptions
# accross the same consumer group.
#
# source://ruby-kafka//lib/kafka/round_robin_assignment_strategy.rb#7
class Kafka::RoundRobinAssignmentStrategy
  # Assign the topic partitions to the group members.
  #
  # @param cluster [Kafka::Cluster]
  # @param members [Hash<String, Kafka::Protocol::JoinGroupResponse::Metadata>] a hash
  #   mapping member ids to metadata
  # @param partitions [Array<Kafka::ConsumerGroup::Assignor::Partition>] a list of
  #   partitions the consumer group processes
  # @return [Hash<String, Array<Kafka::ConsumerGroup::Assignor::Partition>] a hash
  # mapping member ids to partitions.] Hash<String, Array<Kafka::ConsumerGroup::Assignor::Partition>] a hash
  #   mapping member ids to partitions.
  #
  # source://ruby-kafka//lib/kafka/round_robin_assignment_strategy.rb#21
  def call(cluster:, members:, partitions:); end

  # source://ruby-kafka//lib/kafka/round_robin_assignment_strategy.rb#8
  def protocol_name; end

  # source://ruby-kafka//lib/kafka/round_robin_assignment_strategy.rb#42
  def valid_sorted_partitions(members, partitions); end
end

# Opens sockets in a non-blocking fashion, ensuring that we're not stalling
# for long periods of time.
#
# It's possible to set timeouts for connecting to the server, for reading data,
# and for writing data. Whenever a timeout is exceeded, Errno::ETIMEDOUT is
# raised.
#
# source://ruby-kafka//lib/kafka/ssl_socket_with_timeout.rb#14
class Kafka::SSLSocketWithTimeout
  # Opens a socket.
  #
  # @param host [String]
  # @param port [Integer]
  # @param connect_timeout [Integer] the connection timeout, in seconds.
  # @param timeout [Integer] the read and write timeout, in seconds.
  # @param ssl_context [OpenSSL::SSL::SSLContext] which SSLContext the ssl connection should use
  # @raise [Errno::ETIMEDOUT] if the timeout is exceeded.
  # @return [SSLSocketWithTimeout] a new instance of SSLSocketWithTimeout
  #
  # source://ruby-kafka//lib/kafka/ssl_socket_with_timeout.rb#24
  def initialize(host, port, ssl_context:, connect_timeout: T.unsafe(nil), timeout: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/ssl_socket_with_timeout.rb#162
  def close; end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/ssl_socket_with_timeout.rb#167
  def closed?; end

  # Reads bytes from the socket, possible with a timeout.
  #
  # @param num_bytes [Integer] the number of bytes to read.
  # @raise [Errno::ETIMEDOUT] if the timeout is exceeded.
  # @return [String] the data that was read from the socket.
  #
  # source://ruby-kafka//lib/kafka/ssl_socket_with_timeout.rb#93
  def read(num_bytes); end

  # source://ruby-kafka//lib/kafka/ssl_socket_with_timeout.rb#175
  def select_with_timeout(socket, type); end

  # source://ruby-kafka//lib/kafka/ssl_socket_with_timeout.rb#171
  def set_encoding(encoding); end

  # Writes bytes to the socket, possible with a timeout.
  #
  # @param bytes [String] the data that should be written to the socket.
  # @raise [Errno::ETIMEDOUT] if the timeout is exceeded.
  # @return [Integer] the number of bytes written.
  #
  # source://ruby-kafka//lib/kafka/ssl_socket_with_timeout.rb#127
  def write(bytes); end
end

# source://ruby-kafka//lib/kafka/sasl/plain.rb#4
module Kafka::Sasl; end

# source://ruby-kafka//lib/kafka/sasl/awsmskiam.rb#9
class Kafka::Sasl::AwsMskIam
  # @return [AwsMskIam] a new instance of AwsMskIam
  #
  # source://ruby-kafka//lib/kafka/sasl/awsmskiam.rb#12
  def initialize(aws_region:, access_key_id:, secret_key_id:, logger:, session_token: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/sasl/awsmskiam.rb#30
  def authenticate!(host, encoder, decoder); end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/sasl/awsmskiam.rb#26
  def configured?; end

  # source://ruby-kafka//lib/kafka/sasl/awsmskiam.rb#22
  def ident; end

  private

  # source://ruby-kafka//lib/kafka/sasl/awsmskiam.rb#64
  def authentication_payload(host:, time_now:); end

  # source://ruby-kafka//lib/kafka/sasl/awsmskiam.rb#56
  def bin_to_hex(s); end

  # source://ruby-kafka//lib/kafka/sasl/awsmskiam.rb#103
  def canonical_headers(host:); end

  # source://ruby-kafka//lib/kafka/sasl/awsmskiam.rb#89
  def canonical_query_string(time_now:); end

  # source://ruby-kafka//lib/kafka/sasl/awsmskiam.rb#80
  def canonical_request(host:, time_now:); end

  # source://ruby-kafka//lib/kafka/sasl/awsmskiam.rb#60
  def digest; end

  # source://ruby-kafka//lib/kafka/sasl/awsmskiam.rb#111
  def hashed_payload; end

  # source://ruby-kafka//lib/kafka/sasl/awsmskiam.rb#122
  def signature(host:, time_now:); end

  # source://ruby-kafka//lib/kafka/sasl/awsmskiam.rb#107
  def signed_headers; end

  # source://ruby-kafka//lib/kafka/sasl/awsmskiam.rb#115
  def string_to_sign(host:, time_now:); end
end

# source://ruby-kafka//lib/kafka/sasl/awsmskiam.rb#10
Kafka::Sasl::AwsMskIam::AWS_MSK_IAM = T.let(T.unsafe(nil), String)

# source://ruby-kafka//lib/kafka/sasl/gssapi.rb#5
class Kafka::Sasl::Gssapi
  # @return [Gssapi] a new instance of Gssapi
  #
  # source://ruby-kafka//lib/kafka/sasl/gssapi.rb#9
  def initialize(logger:, principal:, keytab:); end

  # source://ruby-kafka//lib/kafka/sasl/gssapi.rb#23
  def authenticate!(host, encoder, decoder); end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/sasl/gssapi.rb#15
  def configured?; end

  # @raise [Kafka::Error]
  #
  # source://ruby-kafka//lib/kafka/sasl/gssapi.rb#45
  def handshake_messages; end

  # source://ruby-kafka//lib/kafka/sasl/gssapi.rb#19
  def ident; end

  # source://ruby-kafka//lib/kafka/sasl/gssapi.rb#68
  def initialize_gssapi_context(host); end

  # source://ruby-kafka//lib/kafka/sasl/gssapi.rb#59
  def load_gssapi; end

  # source://ruby-kafka//lib/kafka/sasl/gssapi.rb#54
  def send_and_receive_sasl_token; end
end

# source://ruby-kafka//lib/kafka/sasl/gssapi.rb#7
Kafka::Sasl::Gssapi::GSSAPI_CONFIDENTIALITY = T.let(T.unsafe(nil), FalseClass)

# source://ruby-kafka//lib/kafka/sasl/gssapi.rb#6
Kafka::Sasl::Gssapi::GSSAPI_IDENT = T.let(T.unsafe(nil), String)

# source://ruby-kafka//lib/kafka/sasl/oauth.rb#5
class Kafka::Sasl::OAuth
  # token_provider: THE FOLLOWING INTERFACE MUST BE FULFILLED:
  #
  # [REQUIRED] TokenProvider#token      - Returns an ID/Access Token to be sent to the Kafka client.
  #   The implementation should ensure token reuse so that multiple calls at connect time do not
  #   create multiple tokens. The implementation should also periodically refresh the token in
  #   order to guarantee that each call returns an unexpired token. A timeout error should
  #   be returned after a short period of inactivity so that the broker can log debugging
  #   info and retry.
  #
  # [OPTIONAL] TokenProvider#extensions - Returns a map of key-value pairs that can be sent with the
  #   SASL/OAUTHBEARER initial client response. If not provided, the values are ignored. This feature
  #   is only available in Kafka >= 2.1.0.
  #
  # @return [OAuth] a new instance of OAuth
  #
  # source://ruby-kafka//lib/kafka/sasl/oauth.rb#21
  def initialize(logger:, token_provider:); end

  # source://ruby-kafka//lib/kafka/sasl/oauth.rb#34
  def authenticate!(host, encoder, decoder); end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/sasl/oauth.rb#30
  def configured?; end

  # source://ruby-kafka//lib/kafka/sasl/oauth.rb#26
  def ident; end

  private

  # @raise [Kafka::TokenMethodNotImplementedError]
  #
  # source://ruby-kafka//lib/kafka/sasl/oauth.rb#53
  def initial_client_response; end

  # source://ruby-kafka//lib/kafka/sasl/oauth.rb#58
  def token_extensions; end
end

# source://ruby-kafka//lib/kafka/sasl/oauth.rb#6
Kafka::Sasl::OAuth::OAUTH_IDENT = T.let(T.unsafe(nil), String)

# source://ruby-kafka//lib/kafka/sasl/plain.rb#5
class Kafka::Sasl::Plain
  # @return [Plain] a new instance of Plain
  #
  # source://ruby-kafka//lib/kafka/sasl/plain.rb#8
  def initialize(logger:, authzid:, username:, password:); end

  # source://ruby-kafka//lib/kafka/sasl/plain.rb#23
  def authenticate!(host, encoder, decoder); end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/sasl/plain.rb#19
  def configured?; end

  # source://ruby-kafka//lib/kafka/sasl/plain.rb#15
  def ident; end
end

# source://ruby-kafka//lib/kafka/sasl/plain.rb#6
Kafka::Sasl::Plain::PLAIN_IDENT = T.let(T.unsafe(nil), String)

# source://ruby-kafka//lib/kafka/sasl/scram.rb#8
class Kafka::Sasl::Scram
  # @return [Scram] a new instance of Scram
  #
  # source://ruby-kafka//lib/kafka/sasl/scram.rb#14
  def initialize(username:, password:, logger:, mechanism: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/sasl/scram.rb#35
  def authenticate!(host, encoder, decoder); end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/sasl/scram.rb#31
  def configured?; end

  # source://ruby-kafka//lib/kafka/sasl/scram.rb#27
  def ident; end

  private

  # source://ruby-kafka//lib/kafka/sasl/scram.rb#98
  def auth_message; end

  # source://ruby-kafka//lib/kafka/sasl/scram.rb#106
  def client_key; end

  # source://ruby-kafka//lib/kafka/sasl/scram.rb#126
  def client_proof; end

  # source://ruby-kafka//lib/kafka/sasl/scram.rb#118
  def client_signature; end

  # source://ruby-kafka//lib/kafka/sasl/scram.rb#164
  def digest; end

  # source://ruby-kafka//lib/kafka/sasl/scram.rb#156
  def encoded_username; end

  # source://ruby-kafka//lib/kafka/sasl/scram.rb#78
  def final_message; end

  # source://ruby-kafka//lib/kafka/sasl/scram.rb#74
  def final_message_without_proof; end

  # source://ruby-kafka//lib/kafka/sasl/scram.rb#66
  def first_message; end

  # source://ruby-kafka//lib/kafka/sasl/scram.rb#70
  def first_message_bare; end

  # source://ruby-kafka//lib/kafka/sasl/scram.rb#130
  def h(str); end

  # source://ruby-kafka//lib/kafka/sasl/scram.rb#134
  def hi(str, salt, iterations); end

  # source://ruby-kafka//lib/kafka/sasl/scram.rb#144
  def hmac(data, key); end

  # source://ruby-kafka//lib/kafka/sasl/scram.rb#94
  def iterations; end

  # source://ruby-kafka//lib/kafka/sasl/scram.rb#160
  def nonce; end

  # source://ruby-kafka//lib/kafka/sasl/scram.rb#152
  def parse_response(data); end

  # source://ruby-kafka//lib/kafka/sasl/scram.rb#86
  def rnonce; end

  # source://ruby-kafka//lib/kafka/sasl/scram.rb#175
  def safe_str(val); end

  # source://ruby-kafka//lib/kafka/sasl/scram.rb#90
  def salt; end

  # source://ruby-kafka//lib/kafka/sasl/scram.rb#102
  def salted_password; end

  # source://ruby-kafka//lib/kafka/sasl/scram.rb#82
  def server_data; end

  # source://ruby-kafka//lib/kafka/sasl/scram.rb#114
  def server_key; end

  # source://ruby-kafka//lib/kafka/sasl/scram.rb#122
  def server_signature; end

  # source://ruby-kafka//lib/kafka/sasl/scram.rb#110
  def stored_key; end

  # source://ruby-kafka//lib/kafka/sasl/scram.rb#148
  def xor(first, second); end
end

# source://ruby-kafka//lib/kafka/sasl/scram.rb#9
Kafka::Sasl::Scram::MECHANISMS = T.let(T.unsafe(nil), Hash)

# source://ruby-kafka//lib/kafka/sasl_authenticator.rb#10
class Kafka::SaslAuthenticator
  # @return [SaslAuthenticator] a new instance of SaslAuthenticator
  #
  # source://ruby-kafka//lib/kafka/sasl_authenticator.rb#11
  def initialize(logger:, sasl_gssapi_principal:, sasl_gssapi_keytab:, sasl_plain_authzid:, sasl_plain_username:, sasl_plain_password:, sasl_scram_username:, sasl_scram_password:, sasl_scram_mechanism:, sasl_oauth_token_provider:, sasl_aws_msk_iam_access_key_id:, sasl_aws_msk_iam_secret_key_id:, sasl_aws_msk_iam_aws_region:, sasl_aws_msk_iam_session_token: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/sasl_authenticator.rb#61
  def authenticate!(connection); end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/sasl_authenticator.rb#57
  def enabled?; end
end

# source://ruby-kafka//lib/kafka.rb#348
class Kafka::SaslScramError < ::Kafka::Error; end

# source://ruby-kafka//lib/kafka/snappy_codec.rb#4
class Kafka::SnappyCodec
  # source://ruby-kafka//lib/kafka/snappy_codec.rb#5
  def codec_id; end

  # source://ruby-kafka//lib/kafka/snappy_codec.rb#20
  def compress(data); end

  # source://ruby-kafka//lib/kafka/snappy_codec.rb#24
  def decompress(data); end

  # source://ruby-kafka//lib/kafka/snappy_codec.rb#13
  def load; end

  # source://ruby-kafka//lib/kafka/snappy_codec.rb#9
  def produce_api_min_version; end
end

# Opens sockets in a non-blocking fashion, ensuring that we're not stalling
# for long periods of time.
#
# It's possible to set timeouts for connecting to the server, for reading data,
# and for writing data. Whenever a timeout is exceeded, Errno::ETIMEDOUT is
# raised.
#
# source://ruby-kafka//lib/kafka/socket_with_timeout.rb#14
class Kafka::SocketWithTimeout
  # Opens a socket.
  #
  # @param host [String]
  # @param port [Integer]
  # @param connect_timeout [Integer] the connection timeout, in seconds.
  # @param timeout [Integer] the read and write timeout, in seconds.
  # @raise [Errno::ETIMEDOUT] if the timeout is exceeded.
  # @return [SocketWithTimeout] a new instance of SocketWithTimeout
  #
  # source://ruby-kafka//lib/kafka/socket_with_timeout.rb#23
  def initialize(host, port, connect_timeout: T.unsafe(nil), timeout: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/socket_with_timeout.rb#84
  def close; end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/socket_with_timeout.rb#88
  def closed?; end

  # Reads bytes from the socket, possible with a timeout.
  #
  # @param num_bytes [Integer] the number of bytes to read.
  # @raise [Errno::ETIMEDOUT] if the timeout is exceeded.
  # @return [String] the data that was read from the socket.
  #
  # source://ruby-kafka//lib/kafka/socket_with_timeout.rb#61
  def read(num_bytes); end

  # source://ruby-kafka//lib/kafka/socket_with_timeout.rb#92
  def set_encoding(encoding); end

  # Writes bytes to the socket, possible with a timeout.
  #
  # @param bytes [String] the data that should be written to the socket.
  # @raise [Errno::ETIMEDOUT] if the timeout is exceeded.
  # @return [Integer] the number of bytes written.
  #
  # source://ruby-kafka//lib/kafka/socket_with_timeout.rb#76
  def write(bytes); end
end

# source://ruby-kafka//lib/kafka/ssl_context.rb#6
module Kafka::SslContext
  class << self
    # source://ruby-kafka//lib/kafka/ssl_context.rb#9
    def build(ca_cert_file_path: T.unsafe(nil), ca_cert: T.unsafe(nil), client_cert: T.unsafe(nil), client_cert_key: T.unsafe(nil), client_cert_key_password: T.unsafe(nil), client_cert_chain: T.unsafe(nil), ca_certs_from_system: T.unsafe(nil), verify_hostname: T.unsafe(nil)); end
  end
end

# source://ruby-kafka//lib/kafka/ssl_context.rb#7
Kafka::SslContext::CLIENT_CERT_DELIMITER = T.let(T.unsafe(nil), String)

# 11
# The controller moved to another broker.
#
# source://ruby-kafka//lib/kafka.rb#93
class Kafka::StaleControllerEpoch < ::Kafka::ProtocolError; end

# source://ruby-kafka//lib/kafka/tagged_logger.rb#8
class Kafka::TaggedLogger < ::SimpleDelegator
  # @return [TaggedLogger] a new instance of TaggedLogger
  #
  # source://ruby-kafka//lib/kafka/tagged_logger.rb#60
  def initialize(logger_or_stream = T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/tagged_logger.rb#37
  def clear_tags!; end

  # source://ruby-kafka//lib/kafka/tagged_logger.rb#41
  def current_tags; end

  # source://ruby-kafka//lib/kafka/tagged_logger.rb#11
  def debug(msg_or_progname, &block); end

  # source://ruby-kafka//lib/kafka/tagged_logger.rb#11
  def error(msg_or_progname, &block); end

  # source://ruby-kafka//lib/kafka/tagged_logger.rb#71
  def flush; end

  # source://ruby-kafka//lib/kafka/tagged_logger.rb#11
  def info(msg_or_progname, &block); end

  # source://ruby-kafka//lib/kafka/tagged_logger.rb#33
  def pop_tags(size = T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/tagged_logger.rb#27
  def push_tags(*tags); end

  # source://ruby-kafka//lib/kafka/tagged_logger.rb#20
  def tagged(*tags); end

  # source://ruby-kafka//lib/kafka/tagged_logger.rb#47
  def tags_text; end

  # source://ruby-kafka//lib/kafka/tagged_logger.rb#11
  def warn(msg_or_progname, &block); end

  class << self
    # source://ruby-kafka//lib/kafka/tagged_logger.rb#54
    def new(logger_or_stream = T.unsafe(nil)); end
  end
end

# The Token Provider object used for SASL OAuthBearer does not implement the method `token`
#
# source://ruby-kafka//lib/kafka.rb#355
class Kafka::TokenMethodNotImplementedError < ::Kafka::Error; end

# 36
#
# source://ruby-kafka//lib/kafka.rb#218
class Kafka::TopicAlreadyExists < ::Kafka::ProtocolError; end

# 29
#
# source://ruby-kafka//lib/kafka.rb#188
class Kafka::TopicAuthorizationFailed < ::Kafka::ProtocolError; end

# 52
# Indicates that the transaction coordinator sending a WriteTxnMarker is no longer the current coordinator for a given producer
#
# source://ruby-kafka//lib/kafka.rb#295
class Kafka::TransactionCoordinatorFencedError < ::Kafka::Error; end

# source://ruby-kafka//lib/kafka/transaction_manager.rb#6
class Kafka::TransactionManager
  # @return [TransactionManager] a new instance of TransactionManager
  #
  # source://ruby-kafka//lib/kafka/transaction_manager.rb#13
  def initialize(cluster:, logger:, idempotent: T.unsafe(nil), transactional: T.unsafe(nil), transactional_id: T.unsafe(nil), transactional_timeout: T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/transaction_manager.rb#186
  def abort_transaction; end

  # source://ruby-kafka//lib/kafka/transaction_manager.rb#94
  def add_partitions_to_transaction(topic_partitions); end

  # source://ruby-kafka//lib/kafka/transaction_manager.rb#139
  def begin_transaction; end

  # source://ruby-kafka//lib/kafka/transaction_manager.rb#267
  def close; end

  # source://ruby-kafka//lib/kafka/transaction_manager.rb#153
  def commit_transaction; end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/transaction_manager.rb#259
  def error?; end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/transaction_manager.rb#39
  def idempotent?; end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/transaction_manager.rb#255
  def in_transaction?; end

  # source://ruby-kafka//lib/kafka/transaction_manager.rb#47
  def init_producer_id(force = T.unsafe(nil)); end

  # source://ruby-kafka//lib/kafka/transaction_manager.rb#76
  def init_transactions; end

  # source://ruby-kafka//lib/kafka/transaction_manager.rb#66
  def next_sequence_for(topic, partition); end

  # Returns the value of attribute producer_epoch.
  #
  # source://ruby-kafka//lib/kafka/transaction_manager.rb#11
  def producer_epoch; end

  # Returns the value of attribute producer_id.
  #
  # source://ruby-kafka//lib/kafka/transaction_manager.rb#11
  def producer_id; end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/transaction_manager.rb#263
  def ready?; end

  # source://ruby-kafka//lib/kafka/transaction_manager.rb#221
  def send_offsets_to_txn(offsets:, group_id:); end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/transaction_manager.rb#43
  def transactional?; end

  # Returns the value of attribute transactional_id.
  #
  # source://ruby-kafka//lib/kafka/transaction_manager.rb#11
  def transactional_id; end

  # source://ruby-kafka//lib/kafka/transaction_manager.rb#71
  def update_sequence_for(topic, partition, sequence); end

  private

  # source://ruby-kafka//lib/kafka/transaction_manager.rb#301
  def complete_transaction; end

  # source://ruby-kafka//lib/kafka/transaction_manager.rb#279
  def force_transactional!; end

  # source://ruby-kafka//lib/kafka/transaction_manager.rb#295
  def group_coordinator(group_id:); end

  # source://ruby-kafka//lib/kafka/transaction_manager.rb#289
  def transaction_coordinator; end
end

# 60 seconds
#
# source://ruby-kafka//lib/kafka/transaction_manager.rb#7
Kafka::TransactionManager::DEFAULT_TRANSACTION_TIMEOUT = T.let(T.unsafe(nil), Integer)

# source://ruby-kafka//lib/kafka/transaction_manager.rb#9
Kafka::TransactionManager::TRANSACTION_RESULT_ABORT = T.let(T.unsafe(nil), FalseClass)

# source://ruby-kafka//lib/kafka/transaction_manager.rb#8
Kafka::TransactionManager::TRANSACTION_RESULT_COMMIT = T.let(T.unsafe(nil), TrueClass)

# source://ruby-kafka//lib/kafka/transaction_state_machine.rb#4
class Kafka::TransactionStateMachine
  # @return [TransactionStateMachine] a new instance of TransactionStateMachine
  #
  # source://ruby-kafka//lib/kafka/transaction_state_machine.rb#27
  def initialize(logger:); end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/transaction_state_machine.rb#58
  def aborting_transaction?; end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/transaction_state_machine.rb#54
  def committing_transaction?; end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/transaction_state_machine.rb#62
  def error?; end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/transaction_state_machine.rb#50
  def in_transaction?; end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/transaction_state_machine.rb#46
  def ready?; end

  # @raise [InvalidStateError]
  #
  # source://ruby-kafka//lib/kafka/transaction_state_machine.rb#33
  def transition_to!(next_state); end

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/transaction_state_machine.rb#42
  def uninitialized?; end

  private

  # @return [Boolean]
  #
  # source://ruby-kafka//lib/kafka/transaction_state_machine.rb#68
  def in_state?(state); end
end

# source://ruby-kafka//lib/kafka/transaction_state_machine.rb#13
Kafka::TransactionStateMachine::ABORTING_TRANSACTION = T.let(T.unsafe(nil), Symbol)

# source://ruby-kafka//lib/kafka/transaction_state_machine.rb#12
Kafka::TransactionStateMachine::COMMITTING_TRANSACTION = T.let(T.unsafe(nil), Symbol)

# source://ruby-kafka//lib/kafka/transaction_state_machine.rb#14
Kafka::TransactionStateMachine::ERROR = T.let(T.unsafe(nil), Symbol)

# source://ruby-kafka//lib/kafka/transaction_state_machine.rb#11
Kafka::TransactionStateMachine::IN_TRANSACTION = T.let(T.unsafe(nil), Symbol)

# source://ruby-kafka//lib/kafka/transaction_state_machine.rb#6
class Kafka::TransactionStateMachine::InvalidStateError < ::StandardError; end

# source://ruby-kafka//lib/kafka/transaction_state_machine.rb#5
class Kafka::TransactionStateMachine::InvalidTransitionError < ::StandardError; end

# source://ruby-kafka//lib/kafka/transaction_state_machine.rb#10
Kafka::TransactionStateMachine::READY = T.let(T.unsafe(nil), Symbol)

# source://ruby-kafka//lib/kafka/transaction_state_machine.rb#8
Kafka::TransactionStateMachine::STATES = T.let(T.unsafe(nil), Array)

# source://ruby-kafka//lib/kafka/transaction_state_machine.rb#17
Kafka::TransactionStateMachine::TRANSITIONS = T.let(T.unsafe(nil), Hash)

# source://ruby-kafka//lib/kafka/transaction_state_machine.rb#9
Kafka::TransactionStateMachine::UNINITIALIZED = T.let(T.unsafe(nil), Symbol)

# -1
# The server experienced an unexpected error when processing the request
#
# source://ruby-kafka//lib/kafka.rb#30
class Kafka::UnknownError < ::Kafka::ProtocolError; end

# 25
# The coordinator is not aware of this member.
#
# source://ruby-kafka//lib/kafka.rb#169
class Kafka::UnknownMemberId < ::Kafka::ProtocolError; end

# 3
# The request is for a topic or partition that does not exist on the broker.
#
# source://ruby-kafka//lib/kafka.rb#46
class Kafka::UnknownTopicOrPartition < ::Kafka::ProtocolError; end

# 43
# The message format version on the broker does not support the request.
#
# source://ruby-kafka//lib/kafka.rb#250
class Kafka::UnsupportedForMessageFormat < ::Kafka::ProtocolError; end

# 33
# The broker does not support the requested SASL mechanism.
#
# source://ruby-kafka//lib/kafka.rb#206
class Kafka::UnsupportedSaslMechanism < ::Kafka::ProtocolError; end

# 35
#
# source://ruby-kafka//lib/kafka.rb#214
class Kafka::UnsupportedVersion < ::Kafka::ProtocolError; end

# source://ruby-kafka//lib/kafka/version.rb#4
Kafka::VERSION = T.let(T.unsafe(nil), String)

# source://ruby-kafka//lib/kafka/zstd_codec.rb#4
class Kafka::ZstdCodec
  # source://ruby-kafka//lib/kafka/zstd_codec.rb#5
  def codec_id; end

  # source://ruby-kafka//lib/kafka/zstd_codec.rb#19
  def compress(data); end

  # source://ruby-kafka//lib/kafka/zstd_codec.rb#23
  def decompress(data); end

  # source://ruby-kafka//lib/kafka/zstd_codec.rb#13
  def load; end

  # source://ruby-kafka//lib/kafka/zstd_codec.rb#9
  def produce_api_min_version; end
end
